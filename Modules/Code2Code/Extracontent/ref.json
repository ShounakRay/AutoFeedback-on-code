{"function_ids": ["ithinksw/philo_philo/models/fields/__init__.py_0", "ithinksw/philo_philo/models/fields/__init__.py_1", "ithinksw/philo_philo/models/fields/__init__.py_2", "ithinksw/philo_philo/models/fields/__init__.py_3", "ithinksw/philo_philo/models/fields/__init__.py_4", "ithinksw/philo_philo/models/fields/__init__.py_5", "ithinksw/philo_philo/models/fields/__init__.py_6", "ithinksw/philo_philo/models/fields/__init__.py_7", "ithinksw/philo_philo/models/fields/__init__.py_8", "ithinksw/philo_philo/models/fields/__init__.py_9", "ithinksw/philo_philo/models/fields/__init__.py_10", "ithinksw/philo_philo/models/fields/__init__.py_11", "ithinksw/philo_philo/models/fields/__init__.py_12", "ithinksw/philo_philo/models/fields/__init__.py_13", "ithinksw/philo_philo/models/fields/__init__.py_14", "nficano/python-lambda_aws_lambda/aws_lambda.py_0", "nficano/python-lambda_aws_lambda/aws_lambda.py_1", "nficano/python-lambda_aws_lambda/aws_lambda.py_2", "nficano/python-lambda_aws_lambda/aws_lambda.py_3", "nficano/python-lambda_aws_lambda/aws_lambda.py_4", "nficano/python-lambda_aws_lambda/aws_lambda.py_5", "nficano/python-lambda_aws_lambda/aws_lambda.py_6", "nficano/python-lambda_aws_lambda/aws_lambda.py_7", "nficano/python-lambda_aws_lambda/aws_lambda.py_8", "nficano/python-lambda_aws_lambda/aws_lambda.py_9", "nficano/python-lambda_aws_lambda/aws_lambda.py_10", "nficano/python-lambda_aws_lambda/aws_lambda.py_11", "nficano/python-lambda_aws_lambda/aws_lambda.py_12", "mfil/getebook_getebook/epub.py_0", "mfil/getebook_getebook/epub.py_1", "mfil/getebook_getebook/epub.py_2", "mfil/getebook_getebook/epub.py_3", "mfil/getebook_getebook/epub.py_4", "mfil/getebook_getebook/epub.py_5", "mfil/getebook_getebook/epub.py_6", "mfil/getebook_getebook/epub.py_7", "mfil/getebook_getebook/epub.py_8", "mfil/getebook_getebook/epub.py_9", "mfil/getebook_getebook/epub.py_10", "mfil/getebook_getebook/epub.py_11", "mfil/getebook_getebook/epub.py_12", "mfil/getebook_getebook/epub.py_13", "mfil/getebook_getebook/epub.py_14", "mfil/getebook_getebook/epub.py_15", "mfil/getebook_getebook/epub.py_16", "mfil/getebook_getebook/epub.py_17", "mfil/getebook_getebook/epub.py_18", "mfil/getebook_getebook/epub.py_19", "mfil/getebook_getebook/epub.py_20", "mfil/getebook_getebook/epub.py_21", "mfil/getebook_getebook/epub.py_22", "mfil/getebook_getebook/epub.py_23", "mfil/getebook_getebook/epub.py_24", "mfil/getebook_getebook/epub.py_25", "mfil/getebook_getebook/epub.py_26", "mfil/getebook_getebook/epub.py_27", "mfil/getebook_getebook/epub.py_28", "mfil/getebook_getebook/epub.py_29", "mfil/getebook_getebook/epub.py_30", "mfil/getebook_getebook/epub.py_31", "mfil/getebook_getebook/epub.py_32", "mfil/getebook_getebook/epub.py_33", "mfil/getebook_getebook/epub.py_34", "mfil/getebook_getebook/epub.py_35", "mfil/getebook_getebook/epub.py_36", "mfil/getebook_getebook/epub.py_37", "banacer/door-wiz_src/identification/Identifier.py_0", "banacer/door-wiz_src/identification/Identifier.py_1", "wanghuafeng/spider_tools_decorator.py_0", "wanghuafeng/spider_tools_decorator.py_1", "wanghuafeng/spider_tools_decorator.py_2", "wanghuafeng/spider_tools_decorator.py_3", "bohdan7/python_koans_python3/koans/about_iteration.py_0", "bohdan7/python_koans_python3/koans/about_iteration.py_1", "bohdan7/python_koans_python3/koans/about_iteration.py_2", "bohdan7/python_koans_python3/koans/about_iteration.py_3", "bohdan7/python_koans_python3/koans/about_iteration.py_4", "bohdan7/python_koans_python3/koans/about_iteration.py_5", "bohdan7/python_koans_python3/koans/about_iteration.py_6", "2Checkout/2checkout-python_twocheckout/sale.py_0", "2Checkout/2checkout-python_twocheckout/sale.py_1", "2Checkout/2checkout-python_twocheckout/sale.py_2", "2Checkout/2checkout-python_twocheckout/sale.py_3", "2Checkout/2checkout-python_twocheckout/sale.py_4", "CenterForOpenScience/scinet_scinet/views.py_0", "CenterForOpenScience/scinet_scinet/views.py_1", "CenterForOpenScience/scinet_scinet/views.py_2", "CenterForOpenScience/scinet_scinet/views.py_3", "CenterForOpenScience/scinet_scinet/views.py_4", "CenterForOpenScience/scinet_scinet/views.py_5", "CenterForOpenScience/scinet_scinet/views.py_6", "CenterForOpenScience/scinet_scinet/views.py_7", "CenterForOpenScience/scinet_scinet/views.py_8", "CenterForOpenScience/scinet_scinet/views.py_9", "TerryHowe/ansible-modules-hashivault_ansible/modules/hashivault/hashivault_approle_role_get.py_0", "TerryHowe/ansible-modules-hashivault_ansible/modules/hashivault/hashivault_approle_role_get.py_1", "HashirZahir/FIFA-Player-Ratings_FIFAscrape/spiders/fifa_spider.py_0", "HashirZahir/FIFA-Player-Ratings_FIFAscrape/spiders/fifa_spider.py_1", "HashirZahir/FIFA-Player-Ratings_FIFAscrape/spiders/fifa_spider.py_2", "HashirZahir/FIFA-Player-Ratings_FIFAscrape/spiders/fifa_spider.py_3", "Azure/azure-sdk-for-python_sdk/sql/azure-mgmt-sqlvirtualmachine/azure/mgmt/sqlvirtualmachine/aio/_sql_virtual_machine_management_client.py_0", "Azure/azure-sdk-for-python_sdk/sql/azure-mgmt-sqlvirtualmachine/azure/mgmt/sqlvirtualmachine/aio/_sql_virtual_machine_management_client.py_1", "TheMasterGhost/CorpBot_Cogs/Time.py_0", "TheMasterGhost/CorpBot_Cogs/Time.py_1", "the-zebulan/CodeWars_tests/beta_tests/test_what_color_is_your_name.py_0", "the-zebulan/CodeWars_tests/beta_tests/test_what_color_is_your_name.py_1", "the-zebulan/CodeWars_tests/beta_tests/test_what_color_is_your_name.py_2", "michaelBenin/sqlalchemy_test/engine/test_execute.py_0", "michaelBenin/sqlalchemy_test/engine/test_execute.py_1", "michaelBenin/sqlalchemy_test/engine/test_execute.py_2", "michaelBenin/sqlalchemy_test/engine/test_execute.py_3", "michaelBenin/sqlalchemy_test/engine/test_execute.py_4", "michaelBenin/sqlalchemy_test/engine/test_execute.py_5", "michaelBenin/sqlalchemy_test/engine/test_execute.py_6", "michaelBenin/sqlalchemy_test/engine/test_execute.py_7", "michaelBenin/sqlalchemy_test/engine/test_execute.py_8", "michaelBenin/sqlalchemy_test/engine/test_execute.py_9", "michaelBenin/sqlalchemy_test/engine/test_execute.py_10", "michaelBenin/sqlalchemy_test/engine/test_execute.py_11", "michaelBenin/sqlalchemy_test/engine/test_execute.py_12", "michaelBenin/sqlalchemy_test/engine/test_execute.py_13", "michaelBenin/sqlalchemy_test/engine/test_execute.py_14", "michaelBenin/sqlalchemy_test/engine/test_execute.py_15", "michaelBenin/sqlalchemy_test/engine/test_execute.py_16", "michaelBenin/sqlalchemy_test/engine/test_execute.py_17", "michaelBenin/sqlalchemy_test/engine/test_execute.py_18", "michaelBenin/sqlalchemy_test/engine/test_execute.py_19", "michaelBenin/sqlalchemy_test/engine/test_execute.py_20", "michaelBenin/sqlalchemy_test/engine/test_execute.py_21", "michaelBenin/sqlalchemy_test/engine/test_execute.py_22", "michaelBenin/sqlalchemy_test/engine/test_execute.py_23", "michaelBenin/sqlalchemy_test/engine/test_execute.py_24", "michaelBenin/sqlalchemy_test/engine/test_execute.py_25", "michaelBenin/sqlalchemy_test/engine/test_execute.py_26", "michaelBenin/sqlalchemy_test/engine/test_execute.py_27", "michaelBenin/sqlalchemy_test/engine/test_execute.py_28", "michaelBenin/sqlalchemy_test/engine/test_execute.py_29", "michaelBenin/sqlalchemy_test/engine/test_execute.py_30", "michaelBenin/sqlalchemy_test/engine/test_execute.py_31", "michaelBenin/sqlalchemy_test/engine/test_execute.py_32", "michaelBenin/sqlalchemy_test/engine/test_execute.py_33", "michaelBenin/sqlalchemy_test/engine/test_execute.py_34", "michaelBenin/sqlalchemy_test/engine/test_execute.py_35", "michaelBenin/sqlalchemy_test/engine/test_execute.py_36", "michaelBenin/sqlalchemy_test/engine/test_execute.py_37", "michaelBenin/sqlalchemy_test/engine/test_execute.py_38", "michaelBenin/sqlalchemy_test/engine/test_execute.py_39", "michaelBenin/sqlalchemy_test/engine/test_execute.py_40", "michaelBenin/sqlalchemy_test/engine/test_execute.py_41", "michaelBenin/sqlalchemy_test/engine/test_execute.py_42", "michaelBenin/sqlalchemy_test/engine/test_execute.py_43", "michaelBenin/sqlalchemy_test/engine/test_execute.py_44", "michaelBenin/sqlalchemy_test/engine/test_execute.py_45", "michaelBenin/sqlalchemy_test/engine/test_execute.py_46", "michaelBenin/sqlalchemy_test/engine/test_execute.py_47", "michaelBenin/sqlalchemy_test/engine/test_execute.py_48", "michaelBenin/sqlalchemy_test/engine/test_execute.py_49", "michaelBenin/sqlalchemy_test/engine/test_execute.py_50", "michaelBenin/sqlalchemy_test/engine/test_execute.py_51", "michaelBenin/sqlalchemy_test/engine/test_execute.py_52", "michaelBenin/sqlalchemy_test/engine/test_execute.py_53", "michaelBenin/sqlalchemy_test/engine/test_execute.py_54", "michaelBenin/sqlalchemy_test/engine/test_execute.py_55", "michaelBenin/sqlalchemy_test/engine/test_execute.py_56", "michaelBenin/sqlalchemy_test/engine/test_execute.py_57", "michaelBenin/sqlalchemy_test/engine/test_execute.py_58", "michaelBenin/sqlalchemy_test/engine/test_execute.py_59", "michaelBenin/sqlalchemy_test/engine/test_execute.py_60", "michaelBenin/sqlalchemy_test/engine/test_execute.py_61", "michaelBenin/sqlalchemy_test/engine/test_execute.py_62", "michaelBenin/sqlalchemy_test/engine/test_execute.py_63", "michaelBenin/sqlalchemy_test/engine/test_execute.py_64", "michaelBenin/sqlalchemy_test/engine/test_execute.py_65", "michaelBenin/sqlalchemy_test/engine/test_execute.py_66", "michaelBenin/sqlalchemy_test/engine/test_execute.py_67", "michaelBenin/sqlalchemy_test/engine/test_execute.py_68", "michaelBenin/sqlalchemy_test/engine/test_execute.py_69", "michaelBenin/sqlalchemy_test/engine/test_execute.py_70", "michaelBenin/sqlalchemy_test/engine/test_execute.py_71", "michaelBenin/sqlalchemy_test/engine/test_execute.py_72", "michaelBenin/sqlalchemy_test/engine/test_execute.py_73", "michaelBenin/sqlalchemy_test/engine/test_execute.py_74", "michaelBenin/sqlalchemy_test/engine/test_execute.py_75", "michaelBenin/sqlalchemy_test/engine/test_execute.py_76", "michaelBenin/sqlalchemy_test/engine/test_execute.py_77", "michaelBenin/sqlalchemy_test/engine/test_execute.py_78", "guori12321/todo_todo/parser.py_0", "guori12321/todo_todo/parser.py_1", "guori12321/todo_todo/parser.py_2", "guori12321/todo_todo/parser.py_3", "guori12321/todo_todo/parser.py_4", "guori12321/todo_todo/parser.py_5", "alisaifee/limits_tests/storage/test_memcached.py_0", "alisaifee/limits_tests/storage/test_memcached.py_1", "alisaifee/limits_tests/storage/test_memcached.py_2", "alisaifee/limits_tests/storage/test_memcached.py_3", "alisaifee/limits_tests/storage/test_memcached.py_4", "ff0000/red-fab-deploy_fab_deploy/joyent/postgres.py_0", "ff0000/red-fab-deploy_fab_deploy/joyent/postgres.py_1", "ff0000/red-fab-deploy_fab_deploy/joyent/postgres.py_2", "ff0000/red-fab-deploy_fab_deploy/joyent/postgres.py_3", "ff0000/red-fab-deploy_fab_deploy/joyent/postgres.py_4", "ff0000/red-fab-deploy_fab_deploy/joyent/postgres.py_5", "chriso/gauged_gauged/drivers/__init__.py_0", "laterpay/rubberjack-cli_tests/test_cli.py_0", "laterpay/rubberjack-cli_tests/test_cli.py_1", "laterpay/rubberjack-cli_tests/test_cli.py_2", "laterpay/rubberjack-cli_tests/test_cli.py_3", "laterpay/rubberjack-cli_tests/test_cli.py_4", "laterpay/rubberjack-cli_tests/test_cli.py_5", "laterpay/rubberjack-cli_tests/test_cli.py_6", "tareqalayan/pytest_testing/test_capture.py_0", "tareqalayan/pytest_testing/test_capture.py_1", "tareqalayan/pytest_testing/test_capture.py_2", "tareqalayan/pytest_testing/test_capture.py_3", "tareqalayan/pytest_testing/test_capture.py_4", "tareqalayan/pytest_testing/test_capture.py_5", "tareqalayan/pytest_testing/test_capture.py_6", "tareqalayan/pytest_testing/test_capture.py_7", "tareqalayan/pytest_testing/test_capture.py_8", "tareqalayan/pytest_testing/test_capture.py_9", "tareqalayan/pytest_testing/test_capture.py_10", "tareqalayan/pytest_testing/test_capture.py_11", "tareqalayan/pytest_testing/test_capture.py_12", "tareqalayan/pytest_testing/test_capture.py_13", "tareqalayan/pytest_testing/test_capture.py_14", "tareqalayan/pytest_testing/test_capture.py_15", "tareqalayan/pytest_testing/test_capture.py_16", "tareqalayan/pytest_testing/test_capture.py_17", "tareqalayan/pytest_testing/test_capture.py_18", "tareqalayan/pytest_testing/test_capture.py_19", "tareqalayan/pytest_testing/test_capture.py_20", "tareqalayan/pytest_testing/test_capture.py_21", "tareqalayan/pytest_testing/test_capture.py_22", "tareqalayan/pytest_testing/test_capture.py_23", "tareqalayan/pytest_testing/test_capture.py_24", "tareqalayan/pytest_testing/test_capture.py_25", "tareqalayan/pytest_testing/test_capture.py_26", "tareqalayan/pytest_testing/test_capture.py_27", "tareqalayan/pytest_testing/test_capture.py_28", "tareqalayan/pytest_testing/test_capture.py_29", "tareqalayan/pytest_testing/test_capture.py_30", "tareqalayan/pytest_testing/test_capture.py_31", "tareqalayan/pytest_testing/test_capture.py_32", "tareqalayan/pytest_testing/test_capture.py_33", "tareqalayan/pytest_testing/test_capture.py_34", "tareqalayan/pytest_testing/test_capture.py_35", "tareqalayan/pytest_testing/test_capture.py_36", "tareqalayan/pytest_testing/test_capture.py_37", "tareqalayan/pytest_testing/test_capture.py_38", "tareqalayan/pytest_testing/test_capture.py_39", "tareqalayan/pytest_testing/test_capture.py_40", "tareqalayan/pytest_testing/test_capture.py_41", "tareqalayan/pytest_testing/test_capture.py_42", "tareqalayan/pytest_testing/test_capture.py_43", "tareqalayan/pytest_testing/test_capture.py_44", "tareqalayan/pytest_testing/test_capture.py_45", "tareqalayan/pytest_testing/test_capture.py_46", "tareqalayan/pytest_testing/test_capture.py_47", "tareqalayan/pytest_testing/test_capture.py_48", "tareqalayan/pytest_testing/test_capture.py_49", "tareqalayan/pytest_testing/test_capture.py_50", "tareqalayan/pytest_testing/test_capture.py_51", "tareqalayan/pytest_testing/test_capture.py_52", "tareqalayan/pytest_testing/test_capture.py_53", "tareqalayan/pytest_testing/test_capture.py_54", "tareqalayan/pytest_testing/test_capture.py_55", "tareqalayan/pytest_testing/test_capture.py_56", "tareqalayan/pytest_testing/test_capture.py_57", "tareqalayan/pytest_testing/test_capture.py_58", "tareqalayan/pytest_testing/test_capture.py_59", "tareqalayan/pytest_testing/test_capture.py_60", "tareqalayan/pytest_testing/test_capture.py_61", "tareqalayan/pytest_testing/test_capture.py_62", "tareqalayan/pytest_testing/test_capture.py_63", "tareqalayan/pytest_testing/test_capture.py_64", "tareqalayan/pytest_testing/test_capture.py_65", "tareqalayan/pytest_testing/test_capture.py_66", "tareqalayan/pytest_testing/test_capture.py_67", "tareqalayan/pytest_testing/test_capture.py_68", "tareqalayan/pytest_testing/test_capture.py_69", "tareqalayan/pytest_testing/test_capture.py_70", "tareqalayan/pytest_testing/test_capture.py_71", "tareqalayan/pytest_testing/test_capture.py_72", "greencoder/hopefullysunny-django_registrations/management/commands/registration_worker.py_0", "dustlab/noisemapper_scripts/nmcollector.py_0", "dcorney/text-generation_core/dialogue.py_0", "dcorney/text-generation_core/dialogue.py_1", "dcorney/text-generation_core/dialogue.py_2", "dcorney/text-generation_core/dialogue.py_3", "dcorney/text-generation_core/dialogue.py_4", "executablebooks/mdformat_src/mdformat/renderer/_context.py_0", "executablebooks/mdformat_src/mdformat/renderer/_context.py_1", "executablebooks/mdformat_src/mdformat/renderer/_context.py_2", "executablebooks/mdformat_src/mdformat/renderer/_context.py_3", "executablebooks/mdformat_src/mdformat/renderer/_context.py_4", "executablebooks/mdformat_src/mdformat/renderer/_context.py_5", "executablebooks/mdformat_src/mdformat/renderer/_context.py_6", "executablebooks/mdformat_src/mdformat/renderer/_context.py_7", "executablebooks/mdformat_src/mdformat/renderer/_context.py_8", "executablebooks/mdformat_src/mdformat/renderer/_context.py_9", "executablebooks/mdformat_src/mdformat/renderer/_context.py_10", "executablebooks/mdformat_src/mdformat/renderer/_context.py_11", "executablebooks/mdformat_src/mdformat/renderer/_context.py_12", "executablebooks/mdformat_src/mdformat/renderer/_context.py_13", "executablebooks/mdformat_src/mdformat/renderer/_context.py_14", "alfateam123/Teca_tests/test_utils.py_0", "DayGitH/Python-Challenges_DailyProgrammer/DP20150713A.py_0", "redsolution/django-generic-ratings_ratings/forms/widgets.py_0", "redsolution/django-generic-ratings_ratings/forms/widgets.py_1", "redsolution/django-generic-ratings_ratings/forms/widgets.py_2", "redsolution/django-generic-ratings_ratings/forms/widgets.py_3", "redsolution/django-generic-ratings_ratings/forms/widgets.py_4", "redsolution/django-generic-ratings_ratings/forms/widgets.py_5", "redsolution/django-generic-ratings_ratings/forms/widgets.py_6", "redsolution/django-generic-ratings_ratings/forms/widgets.py_7", "IQSS/miniverse_dv_apps/metrics/stats_views_dataverses.py_0", "IQSS/miniverse_dv_apps/metrics/stats_views_dataverses.py_1", "IQSS/miniverse_dv_apps/metrics/stats_views_dataverses.py_2", "IQSS/miniverse_dv_apps/metrics/stats_views_dataverses.py_3", "masschallenge/django-accelerator_simpleuser/models.py_0", "masschallenge/django-accelerator_simpleuser/models.py_1", "masschallenge/django-accelerator_simpleuser/models.py_2", "masschallenge/django-accelerator_simpleuser/models.py_3", "masschallenge/django-accelerator_simpleuser/models.py_4", "masschallenge/django-accelerator_simpleuser/models.py_5", "masschallenge/django-accelerator_simpleuser/models.py_6", "masschallenge/django-accelerator_simpleuser/models.py_7", "masschallenge/django-accelerator_simpleuser/models.py_8", "masschallenge/django-accelerator_simpleuser/models.py_9", "masschallenge/django-accelerator_simpleuser/models.py_10", "masschallenge/django-accelerator_simpleuser/models.py_11", "masschallenge/django-accelerator_simpleuser/models.py_12", "masschallenge/django-accelerator_simpleuser/models.py_13", "masschallenge/django-accelerator_simpleuser/models.py_14", "masschallenge/django-accelerator_simpleuser/models.py_15", "keithhamilton/transposer_setup.py_0", "DestructHub/bcs-contest_2016/Main/L/Python/solution_1_wrong.py_0", "jonasrauber/c2s_c2s/experiment.py_0", "jonasrauber/c2s_c2s/experiment.py_1", "jonasrauber/c2s_c2s/experiment.py_2", "jonasrauber/c2s_c2s/experiment.py_3", "jonasrauber/c2s_c2s/experiment.py_4", "jonasrauber/c2s_c2s/experiment.py_5", "jonasrauber/c2s_c2s/experiment.py_6", "jonasrauber/c2s_c2s/experiment.py_7", "jonasrauber/c2s_c2s/experiment.py_8", "jonasrauber/c2s_c2s/experiment.py_9", "mikhtonyuk/rxpython_concurrent/futures/cooperative/ensure_exception_handled.py_0", "mikhtonyuk/rxpython_concurrent/futures/cooperative/ensure_exception_handled.py_1", "KevinJMcGrath/Symphony-Ares_modules/plugins/PABot/logging.py_0", "nicorellius/pdxpixel_pdxpixel/core/mailgun.py_0", "snfactory/cubefit_cubefit/main.py_0", "snfactory/cubefit_cubefit/main.py_1", "snfactory/cubefit_cubefit/main.py_2", "vicenteneto/online-judge-solutions_URI/1-Beginner/1021.py_0", "slava-sh/NewsBlur_apps/reader/views.py_0", "slava-sh/NewsBlur_apps/reader/views.py_1", "slava-sh/NewsBlur_apps/reader/views.py_2", "slava-sh/NewsBlur_apps/reader/views.py_3", "slava-sh/NewsBlur_apps/reader/views.py_4", "slava-sh/NewsBlur_apps/reader/views.py_5", "slava-sh/NewsBlur_apps/reader/views.py_6", "slava-sh/NewsBlur_apps/reader/views.py_7", "slava-sh/NewsBlur_apps/reader/views.py_8", "slava-sh/NewsBlur_apps/reader/views.py_9", "slava-sh/NewsBlur_apps/reader/views.py_10", "slava-sh/NewsBlur_apps/reader/views.py_11", "slava-sh/NewsBlur_apps/reader/views.py_12", "slava-sh/NewsBlur_apps/reader/views.py_13", "slava-sh/NewsBlur_apps/reader/views.py_14", "slava-sh/NewsBlur_apps/reader/views.py_15", "slava-sh/NewsBlur_apps/reader/views.py_16", "slava-sh/NewsBlur_apps/reader/views.py_17", "slava-sh/NewsBlur_apps/reader/views.py_18", "slava-sh/NewsBlur_apps/reader/views.py_19", "slava-sh/NewsBlur_apps/reader/views.py_20", "slava-sh/NewsBlur_apps/reader/views.py_21", "slava-sh/NewsBlur_apps/reader/views.py_22", "slava-sh/NewsBlur_apps/reader/views.py_23", "slava-sh/NewsBlur_apps/reader/views.py_24", "slava-sh/NewsBlur_apps/reader/views.py_25", "slava-sh/NewsBlur_apps/reader/views.py_26", "slava-sh/NewsBlur_apps/reader/views.py_27", "slava-sh/NewsBlur_apps/reader/views.py_28", "slava-sh/NewsBlur_apps/reader/views.py_29", "slava-sh/NewsBlur_apps/reader/views.py_30", "slava-sh/NewsBlur_apps/reader/views.py_31", "slava-sh/NewsBlur_apps/reader/views.py_32", "slava-sh/NewsBlur_apps/reader/views.py_33", "slava-sh/NewsBlur_apps/reader/views.py_34", "slava-sh/NewsBlur_apps/reader/views.py_35", "slava-sh/NewsBlur_apps/reader/views.py_36", "slava-sh/NewsBlur_apps/reader/views.py_37", "slava-sh/NewsBlur_apps/reader/views.py_38", "slava-sh/NewsBlur_apps/reader/views.py_39", "slava-sh/NewsBlur_apps/reader/views.py_40", "slava-sh/NewsBlur_apps/reader/views.py_41", "slava-sh/NewsBlur_apps/reader/views.py_42", "slava-sh/NewsBlur_apps/reader/views.py_43", "slava-sh/NewsBlur_apps/reader/views.py_44", "slava-sh/NewsBlur_apps/reader/views.py_45", "slava-sh/NewsBlur_apps/reader/views.py_46", "slava-sh/NewsBlur_apps/reader/views.py_47", "slava-sh/NewsBlur_apps/reader/views.py_48", "slava-sh/NewsBlur_apps/reader/views.py_49", "slava-sh/NewsBlur_apps/reader/views.py_50", "slava-sh/NewsBlur_apps/reader/views.py_51", "slava-sh/NewsBlur_apps/reader/views.py_52", "slava-sh/NewsBlur_apps/reader/views.py_53", "slava-sh/NewsBlur_apps/reader/views.py_54", "slava-sh/NewsBlur_apps/reader/views.py_55", "slava-sh/NewsBlur_apps/reader/views.py_56", "slava-sh/NewsBlur_apps/reader/views.py_57", "kif/freesas_freesas/average.py_0", "kif/freesas_freesas/average.py_1", "kif/freesas_freesas/average.py_2", "kif/freesas_freesas/average.py_3", "kif/freesas_freesas/average.py_4", "kif/freesas_freesas/average.py_5", "swcarpentry/amy_amy/workshops/management/commands/instructors_activity.py_0", "swcarpentry/amy_amy/workshops/management/commands/instructors_activity.py_1", "swcarpentry/amy_amy/workshops/management/commands/instructors_activity.py_2", "swcarpentry/amy_amy/workshops/management/commands/instructors_activity.py_3", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_0", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_1", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_2", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_3", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_4", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_5", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_6", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_7", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_8", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_9", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_10", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_11", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_12", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_13", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_14", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_15", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_16", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_17", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_18", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_19", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_20", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_21", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_22", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_23", "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_24", "loretoparisi/docker_theano/rsc15/preprocess.py_0", "loretoparisi/docker_theano/rsc15/preprocess.py_1", "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_0", "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_1", "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_2", "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_3", "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_4", "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_5", "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_6", "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_7", "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_8", "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_0", "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_1", "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_2", "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_3", "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_4", "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_5", "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_6", "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_7", "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_8", "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_9", "arnavd96/Cinemiezer_myvenv/lib/python3.4/site-packages/music21/ext/jsonpickle/__init__.py_0", "arnavd96/Cinemiezer_myvenv/lib/python3.4/site-packages/music21/ext/jsonpickle/__init__.py_1", "marcus-nystrom/share-gaze_sync_clocks/test_clock_resolution.py_0", "ValorNaram/isl_inputchangers/002.py_0", "mrmrwat/pylsner_pylsner/gui.py_0", "mrmrwat/pylsner_pylsner/gui.py_1", "mrmrwat/pylsner_pylsner/gui.py_2", "mrwsr/monotone_test/test_monotone.py_0", "mrwsr/monotone_test/test_monotone.py_1", "mrwsr/monotone_test/test_monotone.py_2", "mrwsr/monotone_test/test_monotone.py_3", "mrwsr/monotone_test/test_monotone.py_4", "mrwsr/monotone_test/test_monotone.py_5", "mrwsr/monotone_test/test_monotone.py_6", "mrwsr/monotone_test/test_monotone.py_7", "mrwsr/monotone_test/test_monotone.py_8", "mrwsr/monotone_test/test_monotone.py_9", "mrwsr/monotone_test/test_monotone.py_10", "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_0", "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_1", "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_2", "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_3", "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_4", "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_5", "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_6", "toranb/django-bower-registry_api/migrations/0001_initial.py_0", "smarkets/smk_python_sdk_smarkets/statsd.py_0", "smarkets/smk_python_sdk_smarkets/statsd.py_1", "smarkets/smk_python_sdk_smarkets/statsd.py_2", "smarkets/smk_python_sdk_smarkets/statsd.py_3", "PythonProgramming/Support-Vector-Machines---Basics-and-Fundamental-Investing-Project_p10.py_0", "KeserOner/puissance4_bestaplayer.py_0", "KeserOner/puissance4_bestaplayer.py_1", "KeserOner/puissance4_bestaplayer.py_2", "KeserOner/puissance4_bestaplayer.py_3", "KeserOner/puissance4_bestaplayer.py_4", "KeserOner/puissance4_bestaplayer.py_5", "KeserOner/puissance4_bestaplayer.py_6", "hfaran/slack-export-viewer_slackviewer/user.py_0", "hfaran/slack-export-viewer_slackviewer/user.py_1", "hfaran/slack-export-viewer_slackviewer/user.py_2", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_0", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_1", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_2", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_3", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_4", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_5", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_6", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_7", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_8", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_9", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_10", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_11", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_12", "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_13", "vervacity/ggr-project_scripts/data_qc/summarize_chipseq_qc.py_0", "vervacity/ggr-project_scripts/data_qc/summarize_chipseq_qc.py_1", "ajitghz582/PythonLearning_DAY_1_ASSIGNMENTS/1_name_phone_number.py_0", "ajitghz582/PythonLearning_DAY_1_ASSIGNMENTS/1_name_phone_number.py_1", "ajitghz582/PythonLearning_DAY_1_ASSIGNMENTS/1_name_phone_number.py_2", "stevearc/dynamo3_tests/__init__.py_0", "stevearc/dynamo3_tests/__init__.py_1", "stevearc/dynamo3_tests/__init__.py_2", "stevearc/dynamo3_tests/__init__.py_3", "stevearc/dynamo3_tests/__init__.py_4", "stevearc/dynamo3_tests/__init__.py_5", "stevearc/dynamo3_tests/__init__.py_6", "stevearc/dynamo3_tests/__init__.py_7", "stevearc/dynamo3_tests/__init__.py_8", "stevearc/dynamo3_tests/__init__.py_9", "stevearc/dynamo3_tests/__init__.py_10", "stevearc/dynamo3_tests/__init__.py_11", "stevearc/dynamo3_tests/__init__.py_12", "stevearc/dynamo3_tests/__init__.py_13", "stevearc/dynamo3_tests/__init__.py_14", "stevearc/dynamo3_tests/__init__.py_15", "stevearc/dynamo3_tests/__init__.py_16", "stevearc/dynamo3_tests/__init__.py_17", "stevearc/dynamo3_tests/__init__.py_18", "stevearc/dynamo3_tests/__init__.py_19", "stevearc/dynamo3_tests/__init__.py_20", "stevearc/dynamo3_tests/__init__.py_21", "stevearc/dynamo3_tests/__init__.py_22", "stevearc/dynamo3_tests/__init__.py_23", "stevearc/dynamo3_tests/__init__.py_24", "stevearc/dynamo3_tests/__init__.py_25", "stevearc/dynamo3_tests/__init__.py_26", "stevearc/dynamo3_tests/__init__.py_27", "stevearc/dynamo3_tests/__init__.py_28", "stevearc/dynamo3_tests/__init__.py_29", "stevearc/dynamo3_tests/__init__.py_30", "stevearc/dynamo3_tests/__init__.py_31", "stevearc/dynamo3_tests/__init__.py_32", "zdvresearch/fast15-paper-extras_ecfs_user_sessions/src/analyze_user_sessions.py_0", "zdvresearch/fast15-paper-extras_ecfs_user_sessions/src/analyze_user_sessions.py_1", "zdvresearch/fast15-paper-extras_ecfs_user_sessions/src/analyze_user_sessions.py_2", "zdvresearch/fast15-paper-extras_ecfs_user_sessions/src/analyze_user_sessions.py_3", "ashwyn/eden-message_parser_modules/s3/s3import.py_0", "ashwyn/eden-message_parser_modules/s3/s3import.py_1", "ashwyn/eden-message_parser_modules/s3/s3import.py_2", "ashwyn/eden-message_parser_modules/s3/s3import.py_3", "ashwyn/eden-message_parser_modules/s3/s3import.py_4", "ashwyn/eden-message_parser_modules/s3/s3import.py_5", "ashwyn/eden-message_parser_modules/s3/s3import.py_6", "ashwyn/eden-message_parser_modules/s3/s3import.py_7", "ashwyn/eden-message_parser_modules/s3/s3import.py_8", "ashwyn/eden-message_parser_modules/s3/s3import.py_9", "ashwyn/eden-message_parser_modules/s3/s3import.py_10", "ashwyn/eden-message_parser_modules/s3/s3import.py_11", "ashwyn/eden-message_parser_modules/s3/s3import.py_12", "ashwyn/eden-message_parser_modules/s3/s3import.py_13", "ashwyn/eden-message_parser_modules/s3/s3import.py_14", "ashwyn/eden-message_parser_modules/s3/s3import.py_15", "ashwyn/eden-message_parser_modules/s3/s3import.py_16", "ashwyn/eden-message_parser_modules/s3/s3import.py_17", "ashwyn/eden-message_parser_modules/s3/s3import.py_18", "ashwyn/eden-message_parser_modules/s3/s3import.py_19", "ashwyn/eden-message_parser_modules/s3/s3import.py_20", "ashwyn/eden-message_parser_modules/s3/s3import.py_21", "ashwyn/eden-message_parser_modules/s3/s3import.py_22", "ashwyn/eden-message_parser_modules/s3/s3import.py_23", "ashwyn/eden-message_parser_modules/s3/s3import.py_24", "ashwyn/eden-message_parser_modules/s3/s3import.py_25", "ashwyn/eden-message_parser_modules/s3/s3import.py_26", "ashwyn/eden-message_parser_modules/s3/s3import.py_27", "ashwyn/eden-message_parser_modules/s3/s3import.py_28", "ashwyn/eden-message_parser_modules/s3/s3import.py_29", "ashwyn/eden-message_parser_modules/s3/s3import.py_30", "ashwyn/eden-message_parser_modules/s3/s3import.py_31", "ashwyn/eden-message_parser_modules/s3/s3import.py_32", "ashwyn/eden-message_parser_modules/s3/s3import.py_33", "ashwyn/eden-message_parser_modules/s3/s3import.py_34", "ashwyn/eden-message_parser_modules/s3/s3import.py_35", "ashwyn/eden-message_parser_modules/s3/s3import.py_36", "ashwyn/eden-message_parser_modules/s3/s3import.py_37", "ashwyn/eden-message_parser_modules/s3/s3import.py_38", "ashwyn/eden-message_parser_modules/s3/s3import.py_39", "ashwyn/eden-message_parser_modules/s3/s3import.py_40", "ashwyn/eden-message_parser_modules/s3/s3import.py_41", "ashwyn/eden-message_parser_modules/s3/s3import.py_42", "ashwyn/eden-message_parser_modules/s3/s3import.py_43", "ashwyn/eden-message_parser_modules/s3/s3import.py_44", "ashwyn/eden-message_parser_modules/s3/s3import.py_45", "ashwyn/eden-message_parser_modules/s3/s3import.py_46", "ashwyn/eden-message_parser_modules/s3/s3import.py_47", "ashwyn/eden-message_parser_modules/s3/s3import.py_48", "ashwyn/eden-message_parser_modules/s3/s3import.py_49", "ashwyn/eden-message_parser_modules/s3/s3import.py_50", "ashwyn/eden-message_parser_modules/s3/s3import.py_51", "ashwyn/eden-message_parser_modules/s3/s3import.py_52", "ashwyn/eden-message_parser_modules/s3/s3import.py_53", "VulcanTechnologies/oauth2lib_oauth2lib/utils.py_0", "VulcanTechnologies/oauth2lib_oauth2lib/utils.py_1", "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_0", "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_1", "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_2", "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_3", "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_4", "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_5", "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_6", "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_7", "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_8", "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_9", "isaac-philip/loolu_common/django/contrib/admin/views/decorators.py_0", "isaac-philip/loolu_common/django/contrib/admin/views/decorators.py_1", "coders-creed/botathon_src/info/fetch_info.py_0", "coders-creed/botathon_src/info/fetch_info.py_1", "coders-creed/botathon_src/info/fetch_info.py_2", "pisskidney/leetcode_medium/16.py_0", "pisskidney/leetcode_medium/16.py_1"], "code_reference": {"ithinksw/philo_philo/models/fields/__init__.py_0": {"code": "def __init__(self, allow=None, disallow=None, secure=True, *args, **kwargs):\n\t\tsuper(TemplateField, self).__init__(*args, **kwargs)\n\t\tself.validators.append(TemplateValidator(allow, disallow, secure))", "documentation": "Add TemplateValidator to the list of validators .", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_1": {"code": "def __init__(self, field):\n\t\tself.field = field", "documentation": "Shortcut to set field to field", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_2": {"code": "def __get__(self, instance, owner):\n\t\tif instance is None:\n\t\t\traise AttributeError # ?", "documentation": "Implement the default get method for the abstract method which raises AttributeError #", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_3": {"code": "def __set__(self, instance, value):\n\t\tinstance.__dict__[self.field.name] = value\n\t\tsetattr(instance, self.field.attname, json.dumps(value))", "documentation": "Set the value of the field on the instance", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_4": {"code": "def __delete__(self, instance):\n\t\tdel(instance.__dict__[self.field.name])\n\t\tsetattr(instance, self.field.attname, json.dumps(None))", "documentation": "Deletes the field from the model .", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_5": {"code": "def get_attname(self):\n\t\treturn \"%s_json\" % self.name", "documentation": "Returns the name of the attribute as a string .", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_6": {"code": "def contribute_to_class(self, cls, name):\n\t\tsuper(JSONField, self).contribute_to_class(cls, name)\n\t\tsetattr(cls, name, JSONDescriptor(self))\n\t\tmodels.signals.pre_init.connect(self.fix_init_kwarg, sender=cls)", "documentation": "Add JSONDescriptor to cls", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_7": {"code": "def fix_init_kwarg(self, sender, args, kwargs, **signal_kwargs):\n\t\t# Anything passed in as self.name is assumed to come from a serializer and\n\t\t# will be treated as a json string.\n\t\tif self.name in kwargs:\n\t\t\tvalue = kwargs.pop(self.name)", "documentation": "Remove self . name from kwargs and return value as a string .", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_8": {"code": "def formfield(self, *args, **kwargs):\n\t\tkwargs[\"form_class\"] = JSONFormField\n\t\treturn super(JSONField, self).formfield(*args, **kwargs)", "documentation": "Add JSONFormField to custom formfield", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_9": {"code": "def get_internal_type(self):\n\t\treturn \"TextField\"", "documentation": "\"\"\"", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_10": {"code": "def to_python(self, value):\n\t\tif not value:\n\t\t\treturn []", "documentation": "Returns a Python value as a list if not empty", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_11": {"code": "def get_prep_value(self, value):\n\t\treturn ','.join(value)", "documentation": "Convert list to comma separated string for database storage", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_12": {"code": "def formfield(self, **kwargs):\n\t\t# This is necessary because django hard-codes TypedChoiceField for things with choices.\n\t\tdefaults = {\n\t\t\t'widget': forms.CheckboxSelectMultiple,\n\t\t\t'choices': self.get_choices(include_blank=False),\n\t\t\t'label': capfirst(self.verbose_name),\n\t\t\t'required': not self.blank,\n\t\t\t'help_text': self.help_text\n\t\t}\n\t\tif self.has_default():\n\t\t\tif callable(self.default):\n\t\t\t\tdefaults['initial'] = self.default\n\t\t\t\tdefaults['show_hidden_initial'] = True\n\t\t\telse:\n\t\t\t\tdefaults['initial'] = self.get_default()", "documentation": "Returns the default form field for this field .", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_13": {"code": "def validate(self, value, model_instance):\n\t\tinvalid_values = []\n\t\tfor val in value:\n\t\t\ttry:\n\t\t\t\tvalidate_slug(val)\n\t\t\texcept ValidationError:\n\t\t\t\tinvalid_values.append(val)", "documentation": "Validate slugs are unique", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "ithinksw/philo_philo/models/fields/__init__.py_14": {"code": "def _get_choices(self):\n\t\tif isinstance(self._choices, RegistryIterator):\n\t\t\treturn self._choices.copy()\n\t\telif hasattr(self._choices, 'next'):\n\t\t\tchoices, self._choices = itertools.tee(self._choices)\n\t\t\treturn choices\n\t\telse:\n\t\t\treturn self._choices", "documentation": "Returns a RegistryIterator for the choices for the widget", "reputation": {"num_stars": 50, "num_forks": 12, "num_watchers": 50, "num_open_issues": 3, "created_at": 1274327279.0}}, "nficano/python-lambda_aws_lambda/aws_lambda.py_0": {"code": "def load_source(module_name, module_path):\n    \"\"\"Loads a python module from the path of the corresponding file.\"\"\"\n\n    if sys.version_info[0] == 3 and sys.version_info[1] >= 5:\n        import importlib.util\n        spec = importlib.util.spec_from_file_location(module_name, module_path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n    elif sys.version_info[0] == 3 and sys.version_info[1] < 5:\n        import importlib.machinery\n        loader = importlib.machinery.SourceFileLoader(module_name, module_path)\n        module = loader.load_module()\n    return module", "documentation": "Loads a python module from the path of the corresponding file .", "reputation": {"num_stars": 1426, "num_forks": 229, "num_watchers": 1426, "num_open_issues": 63, "created_at": 1456456866.0}}, "nficano/python-lambda_aws_lambda/aws_lambda.py_1": {"code": "def deploy(\n    src,\n    requirements=None,\n    local_package=None,\n    config_file=\"config.yaml\",\n    profile_name=None,\n    preserve_vpc=False,", "documentation": "Deploy the given SDK .", "reputation": {"num_stars": 1426, "num_forks": 229, "num_watchers": 1426, "num_open_issues": 63, "created_at": 1456456866.0}}, "nficano/python-lambda_aws_lambda/aws_lambda.py_2": {"code": "def deploy_s3(\n    src,\n    requirements=None,\n    local_package=None,\n    config_file=\"config.yaml\",\n    profile_name=None,\n    preserve_vpc=False,", "documentation": "Deploy a S3 package to the current profile .", "reputation": {"num_stars": 1426, "num_forks": 229, "num_watchers": 1426, "num_open_issues": 63, "created_at": 1456456866.0}}, "nficano/python-lambda_aws_lambda/aws_lambda.py_3": {"code": "def upload(\n    src,\n    requirements=None,\n    local_package=None,\n    config_file=\"config.yaml\",\n    profile_name=None,", "documentation": "Uploads a new project to PyPI using the specified requirements and local package name .", "reputation": {"num_stars": 1426, "num_forks": 229, "num_watchers": 1426, "num_open_issues": 63, "created_at": 1456456866.0}}, "nficano/python-lambda_aws_lambda/aws_lambda.py_4": {"code": "def invoke(\n    src,\n    event_file=\"event.json\",\n    config_file=\"config.yaml\",\n    profile_name=None,\n    verbose=False,", "documentation": "Invoke the hook", "reputation": {"num_stars": 1426, "num_forks": 229, "num_watchers": 1426, "num_open_issues": 63, "created_at": 1456456866.0}}, "nficano/python-lambda_aws_lambda/aws_lambda.py_5": {"code": "def init(src, minimal=False):\n    \"\"\"Copies template files to a given directory.\n\n    :param str src:\n        The path to output the template lambda project files.\n    :param bool minimal:\n        Minimal possible template files (excludes event.json).\n    \"\"\"\n\n    templates_path = os.path.join(\n        os.path.dirname(os.path.abspath(__file__)), \"project_templates\",\n    )\n    for filename in os.listdir(templates_path):\n        if (minimal and filename == \"event.json\") or filename.endswith(\".pyc\"):\n            continue\n        dest_path = os.path.join(templates_path, filename)\n\n        if not os.path.isdir(dest_path):\n            copy(dest_path, src)", "documentation": "Copies template files to a given directory .", "reputation": {"num_stars": 1426, "num_forks": 229, "num_watchers": 1426, "num_open_issues": 63, "created_at": 1456456866.0}}, "nficano/python-lambda_aws_lambda/aws_lambda.py_6": {"code": "def get_callable_handler_function(src, handler):\n    \"\"\"Translate a string of the form \"module.function\" into a callable\n    function.\n\n    :param str src:\n      The path to your Lambda project containing a valid handler file.\n    :param str handler:\n      A dot delimited string representing the `<module>.<function name>`.\n    \"\"\"\n\n    # \"cd\" into `src` directory.\n    os.chdir(src)\n\n    module_name, function_name = handler.split(\".\")\n    filename = get_handler_filename(handler)\n\n    path_to_module_file = os.path.join(src, filename)\n    module = load_source(module_name, path_to_module_file)\n    return getattr(module, function_name)", "documentation": "Translate a string of the form module . function . into a callable function .", "reputation": {"num_stars": 1426, "num_forks": 229, "num_watchers": 1426, "num_open_issues": 63, "created_at": 1456456866.0}}, "nficano/python-lambda_aws_lambda/aws_lambda.py_7": {"code": "def _install_packages(path, packages):\n    \"\"\"Install all packages listed to the target directory.\n\n    Ignores any package that includes Python itself and python-lambda as well\n    since its only needed for deploying and not running the code\n\n    :param str path:\n        Path to copy installed pip packages to.\n    :param list packages:\n        A list of packages to be installed via pip.\n    \"\"\"\n\n    def _filter_blacklist(package):\n        blacklist = [\"-i\", \"#\", \"Python==\", \"python-lambda==\"]\n        return all(package.startswith(entry) is False for entry in blacklist)\n\n    filtered_packages = filter(_filter_blacklist, packages)\n    for package in filtered_packages:\n        if package.startswith(\"-e \"):\n            package = package.replace(\"-e \", \"\")\n\n        print(\"Installing {package}\".format(package=package))\n        subprocess.check_call(\n            [\n                sys.executable,\n                \"-m\",\n                \"pip\",\n                \"install\",\n                package,\n                \"-t\",\n                path,\n                \"--ignore-installed\",\n            ]\n        )\n    print(\n        \"Install directory contents are now: {directory}\".format(\n            directory=os.listdir(path)\n        )\n    )", "documentation": "Install all packages listed to the target directory . Ignores any package that includes Python itself and python", "reputation": {"num_stars": 1426, "num_forks": 229, "num_watchers": 1426, "num_open_issues": 63, "created_at": 1456456866.0}}, "nficano/python-lambda_aws_lambda/aws_lambda.py_8": {"code": "def get_role_name(region, account_id, role):\n    \"\"\"Shortcut to insert the `account_id` and `role` into the iam string.\"\"\"\n    prefix = ARN_PREFIXES.get(region, \"aws\")\n    return \"arn:{0}:iam::{1}:role/{2}\".format(prefix, account_id, role)", "documentation": "Get role name for the given account_id and role .", "reputation": {"num_stars": 1426, "num_forks": 229, "num_watchers": 1426, "num_open_issues": 63, "created_at": 1456456866.0}}, "nficano/python-lambda_aws_lambda/aws_lambda.py_9": {"code": "def get_client(\n    client,\n    profile_name,\n    aws_access_key_id,\n    aws_secret_access_key,\n    region=None,", "documentation": "Get boto3 client", "reputation": {"num_stars": 1426, "num_forks": 229, "num_watchers": 1426, "num_open_issues": 63, "created_at": 1456456866.0}}, "nficano/python-lambda_aws_lambda/aws_lambda.py_10": {"code": "def create_function(cfg, path_to_zip_file, use_s3=False, s3_file=None):\n    \"\"\"Register and upload a function to AWS Lambda.\"\"\"\n\n    print(\"Creating your new Lambda function\")\n    byte_stream = read(path_to_zip_file, binary_file=True)\n    profile_name = cfg.get(\"profile\")\n    aws_access_key_id = cfg.get(\"aws_access_key_id\")\n    aws_secret_access_key = cfg.get(\"aws_secret_access_key\")\n\n    account_id = get_account_id(\n        profile_name,\n        aws_access_key_id,\n        aws_secret_access_key,\n        cfg.get(\"region\",),\n    )\n    role = get_role_name(\n        cfg.get(\"region\"),\n        account_id,\n        cfg.get(\"role\", \"lambda_basic_execution\"),\n    )\n\n    client = get_client(\n        \"lambda\",\n        profile_name,\n        aws_access_key_id,\n        aws_secret_access_key,\n        cfg.get(\"region\"),\n    )\n\n    # Do we prefer development variable over config?\n    buck_name = os.environ.get(\"S3_BUCKET_NAME\") or cfg.get(\"bucket_name\")\n    func_name = os.environ.get(\"LAMBDA_FUNCTION_NAME\") or cfg.get(\n        \"function_name\"\n    )\n    print(\"Creating lambda function with name: {}\".format(func_name))\n\n    if use_s3:\n        kwargs = {\n            \"FunctionName\": func_name,\n            \"Runtime\": cfg.get(\"runtime\", \"python2.7\"),\n            \"Role\": role,\n            \"Handler\": cfg.get(\"handler\"),\n            \"Code\": {\n                \"S3Bucket\": \"{}\".format(buck_name),\n                \"S3Key\": \"{}\".format(s3_file),\n            },\n            \"Description\": cfg.get(\"description\", \"\"),\n            \"Timeout\": cfg.get(\"timeout\", 15),\n            \"MemorySize\": cfg.get(\"memory_size\", 512),\n            \"VpcConfig\": {\n                \"SubnetIds\": cfg.get(\"subnet_ids\", []),\n                \"SecurityGroupIds\": cfg.get(\"security_group_ids\", []),\n            },\n            \"Publish\": True,\n        }\n    else:\n        kwargs = {\n            \"FunctionName\": func_name,\n            \"Runtime\": cfg.get(\"runtime\", \"python2.7\"),\n            \"Role\": role,\n            \"Handler\": cfg.get(\"handler\"),\n            \"Code\": {\"ZipFile\": byte_stream},\n            \"Description\": cfg.get(\"description\", \"\"),\n            \"Timeout\": cfg.get(\"timeout\", 15),\n            \"MemorySize\": cfg.get(\"memory_size\", 512),\n            \"VpcConfig\": {\n                \"SubnetIds\": cfg.get(\"subnet_ids\", []),\n                \"SecurityGroupIds\": cfg.get(\"security_group_ids\", []),\n            },\n            \"Publish\": True,\n        }\n\n    if \"tags\" in cfg:\n        kwargs.update(\n            Tags={key: str(value) for key, value in cfg.get(\"tags\").items()}\n        )\n\n    if \"environment_variables\" in cfg:\n        kwargs.update(\n            Environment={\n                \"Variables\": {\n                    key: get_environment_variable_value(value)\n                    for key, value in cfg.get(\"environment_variables\").items()\n                },\n            },\n        )\n\n    client.create_function(**kwargs)\n\n    concurrency = get_concurrency(cfg)\n    if concurrency > 0:\n        client.put_function_concurrency(\n            FunctionName=func_name, ReservedConcurrentExecutions=concurrency\n        )", "documentation": "Register and upload a function to AWS Lambda .", "reputation": {"num_stars": 1426, "num_forks": 229, "num_watchers": 1426, "num_open_issues": 63, "created_at": 1456456866.0}}, "nficano/python-lambda_aws_lambda/aws_lambda.py_11": {"code": "def upload_s3(cfg, path_to_zip_file, *use_s3):\n    \"\"\"Upload a function to AWS S3.\"\"\"\n\n    print(\"Uploading your new Lambda function\")\n    profile_name = cfg.get(\"profile\")\n    aws_access_key_id = cfg.get(\"aws_access_key_id\")\n    aws_secret_access_key = cfg.get(\"aws_secret_access_key\")\n    client = get_client(\n        \"s3\",\n        profile_name,\n        aws_access_key_id,\n        aws_secret_access_key,\n        cfg.get(\"region\"),\n    )\n    byte_stream = b\"\"\n    with open(path_to_zip_file, mode=\"rb\") as fh:\n        byte_stream = fh.read()\n    s3_key_prefix = cfg.get(\"s3_key_prefix\", \"/dist\")\n    checksum = hashlib.new(\"md5\", byte_stream).hexdigest()\n    timestamp = str(time.time())\n    filename = \"{prefix}{checksum}-{ts}.zip\".format(\n        prefix=s3_key_prefix, checksum=checksum, ts=timestamp,\n    )\n\n    # Do we prefer development variable over config?\n    buck_name = os.environ.get(\"S3_BUCKET_NAME\") or cfg.get(\"bucket_name\")\n    func_name = os.environ.get(\"LAMBDA_FUNCTION_NAME\") or cfg.get(\n        \"function_name\"\n    )\n    kwargs = {\n        \"Bucket\": \"{}\".format(buck_name),\n        \"Key\": \"{}\".format(filename),\n        \"Body\": byte_stream,\n    }\n\n    client.put_object(**kwargs)\n    print(\"Finished uploading {} to S3 bucket {}\".format(func_name, buck_name))\n    if use_s3:\n        return filename", "documentation": "Upload a function to AWS S3 .", "reputation": {"num_stars": 1426, "num_forks": 229, "num_watchers": 1426, "num_open_issues": 63, "created_at": 1456456866.0}}, "nficano/python-lambda_aws_lambda/aws_lambda.py_12": {"code": "def get_concurrency(cfg):\n    \"\"\"Return the Reserved Concurrent Executions if present in the config\"\"\"\n    concurrency = int(cfg.get(\"concurrency\", 0))\n    return max(0, concurrency)", "documentation": "Return the Reserved Concurrent Executions if present in the config", "reputation": {"num_stars": 1426, "num_forks": 229, "num_watchers": 1426, "num_open_issues": 63, "created_at": 1456456866.0}}, "mfil/getebook_getebook/epub.py_0": {"code": "def _normalize(name):\n    '''Transform \"Firstname [Middlenames] Lastname\" into\n    \"Lastname, Firstname [Middlenames]\".'''\n    split = name.split()\n    if len(split) == 1:\n        return name\n    return split[-1] + ', ' + ' '.join(name[0:-1])", "documentation": "Normalize a human readable name by adding the last name in front of the last name in the middle", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_1": {"code": "def _make_xml_elem(tag, text, attr = []):\n    'Write a flat xml element.'\n    out = '    <' + tag\n    for (key, val) in attr:\n        out += ' {}=\"{}\"'.format(key, val)\n    if text:\n        out += '>{}</{}>\\n'.format(text, tag)\n    else:\n        out += ' />\\n'\n    return out", "documentation": "Write a flat xml element .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_2": {"code": "def _navp_xml(self, entry, indent_lvl):\n        'Write xml for an entry and all its subentries.'\n        xml = self._navp.format('  '*indent_lvl, str(entry.no), entry.text,\n          entry.target)\n        for sub in entry.entries:\n            xml += self._navp_xml(sub, indent_lvl+1)\n        xml += '  '*indent_lvl + '</navPoint>\\n'\n        return xml", "documentation": "Write xml for an entry and all its subentries .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_3": {"code": "def __init__(self, name, in_spine = True, guide_title = None,\n                 guide_type = None):\n        '''Initialize the object. If the file does not belong in the\n        reading order, in_spine should be set to False. If it should\n        appear in the guide, set guide_title and guide_type.'''\n        self.name = name\n        (self.ident, ext) = os.path.splitext(name)\n        name_split = name.rsplit('.', 1)\n        self.ident = name_split[0]\n        self.in_spine = in_spine\n        self.guide_title = guide_title\n        self.guide_type = guide_type\n        # Infer media-type from file extension\n        ext = ext.lower()\n        if ext in ('.htm', '.html', '.xhtml'):\n            self.media_type = 'application/xhtml+xml'\n        elif ext in ('.png', '.gif', '.jpeg'):\n            self.media_type = 'image/' + ext\n        elif ext == '.jpg':\n            self.media_type = 'image/jpeg'\n        elif ext == '.css':\n            self.media_type = 'text/css'\n        elif ext == '.ncx':\n            self.media_type = 'application/x-dtbncx+xml'\n        else:\n            raise ValueError('Can\\'t infer media-type from extension: %s' % ext)", "documentation": "Initialize the object . If the file does not belong in the reading order in_spine should", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_4": {"code": "def spine_entry(self):\n        '''Write the XML element for the spine.\n        (Empty string if in_spine is False.)'''\n        if self.in_spine:\n            return _make_xml_elem('itemref', '', [('idref', self.ident)])\n        else:\n            return ''", "documentation": "Write the XML element for the spine .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_5": {"code": "def __init__(self, tag, text, *args):\n        '''The metadata entry is an XML element. *args is used for\n        supplying the XML element's attributes as (key, value) pairs.'''\n        self.tag = tag\n        self.text = text\n        self.attr = args", "documentation": "This is the constructor of the XMLMetadata class .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_6": {"code": "def __repr__(self):\n        'Returns the text.'\n        return self.text", "documentation": "Returns the text.' Returns the text .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_7": {"code": "def __init__(self, date):\n        '''date must be a string of the form \"YYYY[-MM[-DD]]\". If it is\n        not of this form, or if the date is invalid, ValueError is\n        raised.'''\n        m = self._date_re.match(date) \n        if not m:\n            raise ValueError('invalid date format')\n        year = int(m.group(1))\n        try:\n            mon = int(m.group(2)[1:])\n            if mon < 0 or mon > 12:\n                raise ValueError('month must be in 1..12')\n        except IndexError:\n            pass\n        try:\n            day = int(m.group(3)[1:])\n            datetime.date(year, mon, day) # raises ValueError if invalid\n        except IndexError:\n            pass\n        self.tag = 'dc:date'\n        self.text = date\n        self.attr = ()", "documentation": "Parse and set date", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_8": {"code": "def __init__(self, lang):\n        '''lang must be a lower-case two-letter language code,\n        optionally followed by a \"-\" and a upper-case two-letter country\n        code. (e.g., \"en\", \"en-US\", \"en-UK\", \"de\", \"de-DE\", \"de-AT\")'''\n        if self._lang_re.match(lang):\n            self.tag = 'dc:language'\n            self.text = lang\n            self.attr = ()\n        else:\n            raise ValueError('invalid language format')", "documentation": "lang must be a two letter language code optionally followed by a \"\" - \"\" or a space -", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_9": {"code": "def __init__(self, name, fileas = None, role = 'aut'):\n        '''Initialize the object. If the argument \"fileas\" is not given,\n        \"Last-name, First-name\" is used for the file-as attribute. If\n        the argument \"role\" is not given, \"aut\" is used for the role\n        attribute.'''\n        if not fileas:\n            fileas = _normalize(name)\n        self.tag = 'dc:creator'\n        self.text = name\n        self.attr = (('opf:file-as', fileas), ('opf:role', role))", "documentation": "Initialize the creator", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_10": {"code": "def __init__(self):\n        'Initialize.'\n        self.meta = []\n        self.filelist = []", "documentation": "Initialize . self . meta = []", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_11": {"code": "def __init__(self, epub_file):\n        '''Initialize the EpubBuilder instance. \"epub_file\" is the\n        filename of the epub to be created.'''\n        self.epub_f = zipfile.ZipFile(epub_file, 'w', zipfile.ZIP_DEFLATED)\n        self.epub_f.writestr('mimetype', 'application/epub+zip')\n        self.epub_f.writestr('META-INF/container.xml', self._container_xml)\n        self.toc = EpubTOC()\n        self.opf = _OPFfile()\n        self.opf.filelist.append(_Fileinfo('toc.ncx', False))\n        self.opf.filelist.append(_Fileinfo('style.css', False))\n        self._authors = []\n        self.opt_meta = {} # Optional metadata (other than authors)\n        self.content = ''\n        self.part_no = 0\n        self.cont_filename = 'part%03d.html' % self.part_no", "documentation": "Create the EpubBuilder instance .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_12": {"code": "def __exit__(self, except_type, except_val, traceback):\n        'Call finalize() and close the file.'\n        try:\n            self.finalize()\n        finally:\n            # Close again in case an exception happened in finalize()\n            self.epub_f.close()\n        return False", "documentation": "Call finalize() and close the file .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_13": {"code": "def uid(self):\n        '''Unique identifier of the ebook. (mandatory)\n\n        If this property is left unset, a pseudo-random string will be\n        generated which is long enough for collisions with existing\n        ebooks to be extremely unlikely.'''\n        try:\n            return self._uid\n        except AttributeError:\n            import random\n            from string import (ascii_letters, digits)\n            alnum = ascii_letters + digits\n            self.uid = ''.join([random.choice(alnum) for i in range(15)])\n            return self._uid", "documentation": "Unique identifier of the ebook . ( mandatory ) If this property is left unset generate a pseudo", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_14": {"code": "def uid(self, val):\n        self._uid = _EpubMeta('dc:identifier', str(val), ('id', 'uid_id'))", "documentation": "Set uid .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_15": {"code": "def title(self):\n        '''Title of the ebook. (mandatory)\n\n        If this property is left unset, it defaults to \"Untitled\".'''\n        try:\n            return self._title\n        except AttributeError:\n            self.title = 'Untitled'\n            return self._title", "documentation": "Title of the ebook . ( mandatory ) If this property is left unset it defaults to \"\"", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_16": {"code": "def title(self, val):\n        # If val is not a string, raise TypeError now rather than later.\n        self._title = _EpubMeta('dc:title', '' + val)", "documentation": "Set title value .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_17": {"code": "def lang(self):\n        '''Language of the ebook. (mandatory)\n\n        The language must be given as a lower-case two-letter code, optionally\n        followed by a \"-\" and an upper-case two-letter country code.\n        (e.g., \"en\", \"en-US\", \"en-UK\", \"de\", \"de-DE\", \"de-AT\")\n\n        If this property is left unset, it defaults to \"en\".'''\n        try:\n            return self._lang\n        except AttributeError:\n            self.lang = 'en'\n            return self._lang", "documentation": "The language of the ebook .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_18": {"code": "def lang(self, val):\n        self._lang = _EpubLang(val)", "documentation": "Set the language of the epub .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_19": {"code": "def author(self):\n        '''Name of the author. (optional)", "documentation": "Name of the author .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_20": {"code": "def author(self, val):\n        if isinstance(val, Author) or isinstance(val, str):\n            authors = [val]\n        else:\n            authors = val\n        for aut in authors:\n            try:\n                self._authors.append(Author('' + aut))\n            except TypeError:\n                # aut is not a string, so it should be an Author object\n                self._authors.append(aut)", "documentation": "sets the author of the recipe", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_21": {"code": "def author(self):\n        self._authors = []", "documentation": "Manual author .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_22": {"code": "def date(self):\n        '''Publication date. (optional)", "documentation": "Publication date .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_23": {"code": "def date(self, val):\n        self.opt_meta['date'] = _EpubDate(val)", "documentation": "Set option for date field", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_24": {"code": "def date(self):\n        del self._date", "documentation": "Deletes the date of the datetime . date object", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_25": {"code": "def rights(self):\n        'Copyright/licensing information. (optional)'\n        try:\n            return self.opt_meta['rights']\n        except KeyError:\n            return None", "documentation": "Copyright / licensing information . ( optional )", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_26": {"code": "def rights(self, val):\n        self.opt_meta['rights'] = _EpubMeta('dc:rights', '' + val)", "documentation": "Set rights of the epub .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_27": {"code": "def rights(self):\n        del self._rights", "documentation": "Deletes all rights", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_28": {"code": "def publisher(self):\n        'Publisher name. (optional)'\n        try:\n            return self.opt_meta['publisher']\n        except KeyError:\n            return None", "documentation": "Publisher name . ( optional )", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_29": {"code": "def publisher(self, val):\n        self.opt_meta['publisher'] = _EpubMeta('dc:publisher', '' + val)", "documentation": "Set publisher of the book", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_30": {"code": "def publisher(self):\n        del self._publisher", "documentation": "Remove the publisher chain .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_31": {"code": "def style_css(self):\n        '''CSS stylesheet for the files that are generated by the EpubBuilder\n        instance. Can be overwritten or extended, but not deleted.'''\n        return self._style_css", "documentation": "CSS stylesheet for the files that are generated by the EpubBuilder instance .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_32": {"code": "def style_css(self, val):\n        self._style_css = '' + val", "documentation": "Add a CSS style sheet to the style tag", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_33": {"code": "def headingpage(self, heading, subtitle = None, toc_text = None):\n        '''Create a page containing only a (large) heading, optionally\n        with a smaller subtitle. If toc_text is not given, it defaults\n        to the heading.'''\n        self.new_part()\n        tag = 'h%d' % min(6, self.toc.depth)\n        self.content += '<div class=\"getebook-tp\">'\n        self.content += '<{} class=\"getebook-tp-title\">{}'.format(tag, heading)\n        if subtitle:\n            self.content += '<div class=\"getebook-tp-sub\">%s</div>' % subtitle\n        self.content += '</%s>\\n' % tag\n        if not toc_text:\n            toc_text = heading\n        self.toc.new_entry(toc_text, self.cont_filename)\n        self.new_part()", "documentation": "Create a page containing only a large heading optionally with a smaller subtitle . If toc_text", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_34": {"code": "def add_file(self, arcname, str_or_bytes, in_spine = False,\n      guide_title = None, guide_type = None):\n        '''Add the string or bytes instance str_or_bytes to the archive\n        under the name arcname.'''\n        self.opf.filelist.append(_Fileinfo(arcname, in_spine, guide_title,\n                                 guide_type))\n        self.epub_f.writestr(arcname, str_or_bytes)", "documentation": "Add a file instance to the OPF file list .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_35": {"code": "def _heading(self, elem):\n        '''Write a heading.'''\n        # Handle paragraph heading if we have one waiting (see the\n        # par_heading method). We don\\'t use _handle_par_h here because\n        # we merge it with the subsequent proper heading.\n        try:\n            par_h = self.par_h\n            del self.par_h\n        except AttributeError:\n            toc_text = elem.text\n        else:\n            # There is a waiting paragraph heading, we merge it with the\n            # new heading.\n            toc_text = par_h.text + '. ' + elem.text\n            par_h.tag = 'div'\n            par_h.attrs['class'] = 'getebook-small-h'\n            elem.children.insert(0, par_h)\n        # Set the class attribute value.\n        elem.attrs['class'] = 'getebook-chapter-h'\n        self.toc.new_entry(toc_text, self.cont_filename)\n        # Add heading to the epub.\n        tag = 'h%d' % min(self.toc.depth, 6)\n        self.content += _make_starttag(tag, elem.attrs)\n        for elem in elem.children:\n            self.handle_elem(elem)\n        self.content += '</%s>\\n' % tag", "documentation": "Write a heading element .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_36": {"code": "def _handle_par_h(self):\n        'Check if there is a waiting paragraph heading and handle it.'\n        try:\n            self._heading(self.par_h)\n        except AttributeError:\n            pass", "documentation": "Check if there is a waiting paragraph heading and handle it .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "mfil/getebook_getebook/epub.py_37": {"code": "def _handle_image(self, attrs):\n        'Returns the alt text of an image tag.'\n        try:\n            return attrs['alt']\n        except KeyError:\n            return ''", "documentation": "Returns the alt text of an image tag .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1452555369.0}}, "banacer/door-wiz_src/identification/Identifier.py_0": {"code": "def __init__(self):\n        columns = ['mean_height', 'min_height', 'max_height', 'mean_width', 'min_width', 'max_width', 'time', 'girth','id']\n        self.data = DataFrame(columns=columns)\n        self.event = []", "documentation": "Create a dataframe with all the information we need to create a plot of the sky and a", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1462686550.0}}, "banacer/door-wiz_src/identification/Identifier.py_1": {"code": "def subscribe(ch, method, properties, body):\n        \"\"\"\n        prints the body message. It's the default callback method\n        :param ch: keep null\n        :param method: keep null\n        :param properties: keep null\n        :param body: the message\n        :return:\n        \"\"\"\n        #first we get the JSON from body\n\n        #we check if it's part of the walking event\n\n        #if walking event is completed, we", "documentation": "print the message", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1462686550.0}}, "wanghuafeng/spider_tools_decorator.py_0": {"code": "def retries(times=3, timeout=1):\n    \"\"\"\u5bf9\u672a\u6355\u83b7\u5f02\u5e38\u8fdb\u884c\u91cd\u8bd5\"\"\"\n    def decorator(func):\n        def _wrapper(*args, **kw):\n            att, retry = 0, 0\n            while retry < times:\n                retry += 1\n                try:\n                    return func(*args, **kw)\n                except:\n                    att += timeout\n                    if retry < times:\n                        time.sleep(att)\n        return _wrapper\n    return decorator", "documentation": "Retries a function a number of times with a delay before giving up and a timeout on the", "reputation": {"num_stars": 3, "num_forks": 1, "num_watchers": 3, "num_open_issues": 0, "created_at": 1495883653.0}}, "wanghuafeng/spider_tools_decorator.py_1": {"code": "def decorator(func):\n        def _wrapper(*args, **kw):\n            att, retry = 0, 0\n            while retry < times:\n                retry += 1\n                ret = func(*args, **kw)\n                if ret:\n                    return ret\n                att += timeout\n                time.sleep(att)\n        return _wrapper", "documentation": "Decorator to wrap a function with a timeout for retrying", "reputation": {"num_stars": 3, "num_forks": 1, "num_watchers": 3, "num_open_issues": 0, "created_at": 1495883653.0}}, "wanghuafeng/spider_tools_decorator.py_2": {"code": "def use_logging(level):\n    \"\"\"\u5e26\u53c2\u6570\u7684\u88c5\u9970\u5668\"\"\"\n    def decorator(func):\n        print func.__name__\n        def wrapper(*args, **kwargs):\n            if level == \"warn\":\n                print (\"level:%s, %s is running\" % (level, func.__name__))\n            elif level == \"info\":\n                print (\"level:%s, %s is running\" % (level, func.__name__))\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "documentation": "Decorator to indicate logging level .", "reputation": {"num_stars": 3, "num_forks": 1, "num_watchers": 3, "num_open_issues": 0, "created_at": 1495883653.0}}, "wanghuafeng/spider_tools_decorator.py_3": {"code": "def foo(name='foo'):\n        print(\"i am %s\" % name)", "documentation": "print a certain name", "reputation": {"num_stars": 3, "num_forks": 1, "num_watchers": 3, "num_open_issues": 0, "created_at": 1495883653.0}}, "bohdan7/python_koans_python3/koans/about_iteration.py_0": {"code": "def test_iterators_are_a_type(self):\n        it = iter(range(1,6))\n\n        total = 0\n\n        for num in it:\n            total += num\n\n        self.assertEqual(15 , total)", "documentation": "Ensure that iters are of the same type .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570687.0}}, "bohdan7/python_koans_python3/koans/about_iteration.py_1": {"code": "def add_ten(self, item):\n        return item + 10", "documentation": "Adds ten to the quantity", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570687.0}}, "bohdan7/python_koans_python3/koans/about_iteration.py_2": {"code": "def test_filter_selects_certain_items_from_a_list(self):\n        def is_even(item):\n            return (item % 2) == 0\n\n        seq = [1, 2, 3, 4, 5, 6]\n        even_numbers = list()\n\n        for item in filter(is_even, seq):\n            even_numbers.append(item)\n\n        self.assertEqual([2,4,6], even_numbers)", "documentation": "Test that the filter removes even items from a list and that it returns a list of those items", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570687.0}}, "bohdan7/python_koans_python3/koans/about_iteration.py_3": {"code": "def is_big_name(item):\n            return len(item) > 4", "documentation": "Returns True if the name is too long .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570687.0}}, "bohdan7/python_koans_python3/koans/about_iteration.py_4": {"code": "def add(self,accum,item):\n        return accum + item", "documentation": "Add an item to the accum", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570687.0}}, "bohdan7/python_koans_python3/koans/about_iteration.py_5": {"code": "def test_reduce_will_blow_your_mind(self):\n        import functools\n        # As of Python 3 reduce() has been demoted from a builtin function\n        # to the functools module.\n\n        result = functools.reduce(self.add, [2, 3, 4])\n        self.assertEqual(int, result.__class__)\n        # Reduce() syntax is same as Python 2\n\n        self.assertEqual(9, result)\n\n        result2 = functools.reduce(self.multiply, [2, 3, 4], 1)\n        self.assertEqual(24, result2)\n\n        # Extra Credit:\n        # Describe in your own words what reduce does.", "documentation": "Test that reduce will blow your mind when used as a builtin function .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570687.0}}, "bohdan7/python_koans_python3/koans/about_iteration.py_6": {"code": "def test_use_pass_for_iterations_with_no_body(self):\n        for num in range(1,5):\n            pass\n\n        self.assertEqual(4, num)", "documentation": "Use a pass for iterations with no body .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570687.0}}, "2Checkout/2checkout-python_twocheckout/sale.py_0": {"code": "def __init__(self, dict_):\n        super(self.__class__, self).__init__(dict_)", "documentation": "Extends RawConfigParser with dict_", "reputation": {"num_stars": 25, "num_forks": 19, "num_watchers": 25, "num_open_issues": 6, "created_at": 1351273541.0}}, "2Checkout/2checkout-python_twocheckout/sale.py_1": {"code": "def find(cls, params=None):\n        if params is None:\n            params = dict()\n        response = cls(Api.call('sales/detail_sale', params))\n        return response.sale", "documentation": "Retrieve a single sale", "reputation": {"num_stars": 25, "num_forks": 19, "num_watchers": 25, "num_open_issues": 6, "created_at": 1351273541.0}}, "2Checkout/2checkout-python_twocheckout/sale.py_2": {"code": "def list(cls, params=None):\n        if params is None:\n            params = dict()\n        response = cls(Api.call('sales/list_sales', params))\n        return response.sale_summary", "documentation": "Get a list of all sales for a given account", "reputation": {"num_stars": 25, "num_forks": 19, "num_watchers": 25, "num_open_issues": 6, "created_at": 1351273541.0}}, "2Checkout/2checkout-python_twocheckout/sale.py_3": {"code": "def stop(self, params=None):\n        if params is None:\n            params = dict()\n        if hasattr(self, 'lineitem_id'):\n            params['lineitem_id'] = self.lineitem_id\n            return Api.call('sales/stop_lineitem_recurring', params)\n        elif hasattr(self, 'sale_id'):\n            active_lineitems = Util.active(self)\n            if dict(active_lineitems):\n                result = dict()\n                i = 0\n                for k, v in active_lineitems.items():\n                    lineitem_id = v\n                    params = {'lineitem_id': lineitem_id}\n                    result[i] = Api.call('sales/stop_lineitem_recurring', params)\n                    i += 1\n                response = { \"response_code\": \"OK\",\n                             \"response_message\": str(len(result)) + \" lineitems stopped successfully\"\n                }\n            else:\n                response = {\n                    \"response_code\": \"NOTICE\",\n                    \"response_message\": \"No active recurring lineitems\"\n                }\n        else:\n            response = { \"response_code\": \"NOTICE\",\n                          \"response_message\": \"This method can only be called on a sale or lineitem\"\n            }\n        return Sale(response)", "documentation": "Stop lineitem recurring .", "reputation": {"num_stars": 25, "num_forks": 19, "num_watchers": 25, "num_open_issues": 6, "created_at": 1351273541.0}}, "2Checkout/2checkout-python_twocheckout/sale.py_4": {"code": "def comment(self, params=None):\n        if params is None:\n            params = dict()\n        params['sale_id'] = self.sale_id\n        return Sale(Api.call('sales/create_comment', params))", "documentation": "Adds a comment to a sale .", "reputation": {"num_stars": 25, "num_forks": 19, "num_watchers": 25, "num_open_issues": 6, "created_at": 1351273541.0}}, "CenterForOpenScience/scinet_scinet/views.py_0": {"code": "def connect_client():\n    \"\"\"Connects to Mongo client\"\"\"\n    try:\n        return MongoClient(app.config['DB_HOST'], int(app.config['DB_PORT']))\n    except errors.ConnectionFailure as e:\n        raise e", "documentation": "Connects to Mongo client", "reputation": {"num_stars": 14, "num_forks": 6, "num_watchers": 14, "num_open_issues": 9, "created_at": 1368717157.0}}, "CenterForOpenScience/scinet_scinet/views.py_1": {"code": "def close_db(error):\n    \"\"\"Closes connection with Mongo client\"\"\"\n    if hasattr(g, 'mongo_client'):\n        g.mongo_client.close()", "documentation": "Closes connection with Mongo client", "reputation": {"num_stars": 14, "num_forks": 6, "num_watchers": 14, "num_open_issues": 9, "created_at": 1368717157.0}}, "CenterForOpenScience/scinet_scinet/views.py_2": {"code": "def index():\n    \"\"\"Landing page for SciNet\"\"\"\n    return render_template(\"index.html\")", "documentation": "landing page for SciNet", "reputation": {"num_stars": 14, "num_forks": 6, "num_watchers": 14, "num_open_issues": 9, "created_at": 1368717157.0}}, "CenterForOpenScience/scinet_scinet/views.py_3": {"code": "def faq():\n    \"\"\"FAQ page for SciNet\"\"\"\n    return render_template(\"faq.html\")", "documentation": "FAQ page for SciNet", "reputation": {"num_stars": 14, "num_forks": 6, "num_watchers": 14, "num_open_issues": 9, "created_at": 1368717157.0}}, "CenterForOpenScience/scinet_scinet/views.py_4": {"code": "def leaderboard():\n    \"\"\"Leaderboard page for SciNet\"\"\"\n    get_db()\n    groups = get_groups(g.groups_collection)\n    return render_template(\"leaderboard.html\", groups=groups)", "documentation": "Leaderboard page for SciNet", "reputation": {"num_stars": 14, "num_forks": 6, "num_watchers": 14, "num_open_issues": 9, "created_at": 1368717157.0}}, "CenterForOpenScience/scinet_scinet/views.py_5": {"code": "def ping_endpoint():\n    \"\"\"API endpoint determines potential article hash exists in db\n\n    :return: status code 204 -- hash not present, continue submission\n    :return: status code 201 -- hash already exists, drop submission\n    \"\"\"\n    db = get_db()\n    target_hash = request.form.get('hash')\n    if db.raw.find({'hash': target_hash}).count():\n        return Response(status=201)\n    else:\n        return Response(status=204)", "documentation": "API endpoint determines potential article hash exists in db", "reputation": {"num_stars": 14, "num_forks": 6, "num_watchers": 14, "num_open_issues": 9, "created_at": 1368717157.0}}, "CenterForOpenScience/scinet_scinet/views.py_6": {"code": "def ArticleEndpoint():\n    \"\"\"Eventual landing page for searching/retrieving articles\"\"\"\n    if request.method == 'GET':\n        return render_template(\"articles.html\")", "documentation": "Eventual landing page for searching/retrieving articles", "reputation": {"num_stars": 14, "num_forks": 6, "num_watchers": 14, "num_open_issues": 9, "created_at": 1368717157.0}}, "CenterForOpenScience/scinet_scinet/views.py_7": {"code": "def raw_endpoint():\n    \"\"\"API endpoint for submitting raw article data\n\n    :return: status code 405 - invalid JSON or invalid request type\n    :return: status code 400 - unsupported content-type or invalid publisher\n    :return: status code 201 - successful submission\n    \"\"\"\n    # Ensure post's content-type is supported\n    if request.headers['content-type'] == 'application/json':\n        # Ensure data is a valid JSON\n        try:\n            user_submission = json.loads(request.data)\n        except ValueError:\n            return Response(status=405)\n        # generate UID for new entry\n        uid = get_id()\n        # store incoming JSON in raw storage\n        file_path = os.path.join(\n                        HERE,\n                        'raw_payloads',\n                        str(uid)\n                    )\n        store_json_to_file(user_submission, file_path)\n        # hand submission to controller and return Resposne\n        db = get_db()\n        controller_response = JSONController(user_submission, db=db, _id=uid).submit()\n        return controller_response\n\n    # User submitted an unsupported content-type\n    else:\n        return Response(status=400)", "documentation": "API endpoint for submitting raw article data", "reputation": {"num_stars": 14, "num_forks": 6, "num_watchers": 14, "num_open_issues": 9, "created_at": 1368717157.0}}, "CenterForOpenScience/scinet_scinet/views.py_8": {"code": "def request_new_group():\n    # Grab submission form data and prepare email message\n    data = request.json\n    msg = \"Someone has request that you add {group_name} to the leaderboard \\\n        groups. The groups website is {group_website} and the submitter can \\\n        be reached at {submitter_email}.\".format(\n                                            group_name=data['new_group_name'],\n                                            group_website=data['new_group_website'],\n                                            submitter_email=data['submitter_email'])\n    return Response(status=200)\n    '''\n    try:\n        email(\n            subject=\"SciNet: A new group has been requested\",\n            fro=\"no-reply@scinet.osf.io\",\n            to='harry@scinet.osf.io',\n            msg=msg)\n        return Response(status=200)\n    except:\n        return Response(status=500)\n    '''", "documentation": "Send an email to a new group", "reputation": {"num_stars": 14, "num_forks": 6, "num_watchers": 14, "num_open_issues": 9, "created_at": 1368717157.0}}, "CenterForOpenScience/scinet_scinet/views.py_9": {"code": "def not_found(error):\n    return make_response(jsonify( { 'error': 'Page Not Found' } ), 404)", "documentation": "Return a 404 Not Found response", "reputation": {"num_stars": 14, "num_forks": 6, "num_watchers": 14, "num_open_issues": 9, "created_at": 1368717157.0}}, "TerryHowe/ansible-modules-hashivault_ansible/modules/hashivault/hashivault_approle_role_get.py_0": {"code": "def main():\n    argspec = hashivault_argspec()\n    argspec['name'] = dict(required=True, type='str')\n    argspec['mount_point'] = dict(required=False, type='str', default='approle')\n    module = hashivault_init(argspec)\n    result = hashivault_approle_role_get(module.params)\n    if result.get('failed'):\n        module.fail_json(**result)\n    else:\n        module.exit_json(**result)", "documentation": "Get an approle from hashivault", "reputation": {"num_stars": 415, "num_forks": 143, "num_watchers": 415, "num_open_issues": 44, "created_at": 1462216977.0}}, "TerryHowe/ansible-modules-hashivault_ansible/modules/hashivault/hashivault_approle_role_get.py_1": {"code": "def hashivault_approle_role_get(params):\n    name = params.get('name')\n    client = hashivault_auth_client(params)\n    result = client.get_role(name, mount_point=params.get('mount_point'))\n    return {'role': result}", "documentation": "Get a hashivault approle", "reputation": {"num_stars": 415, "num_forks": 143, "num_watchers": 415, "num_open_issues": 44, "created_at": 1462216977.0}}, "HashirZahir/FIFA-Player-Ratings_FIFAscrape/spiders/fifa_spider.py_0": {"code": "def parse(self, response):\r\n        #obtains links from page to page and passes links to parse_playerURL\r\n        sel = Selector(response)    #define selector based on response object (points to urls in start_urls by default) \r\n        url_list = sel.xpath('//a[@class=\"display-block padding-0\"]/@href')   #obtain a list of href links that contain relative links of players\r\n        for i in url_list:\r\n            relative_url = self.clean_str(i.extract())    #i is a selector and hence need to extract it to obtain unicode object\r\n            print urljoin(response.url, relative_url)   #urljoin is able to merge absolute and relative paths to form 1 coherent link\r\n            req = Request(urljoin(response.url, relative_url),callback=self.parse_playerURL)   #pass on request with new urls to parse_playerURL\r\n            req.headers[\"User-Agent\"] = self.random_ua()    \r\n            yield req", "documentation": "Parse player s page and get player s url", "reputation": {"num_stars": 8, "num_forks": 8, "num_watchers": 8, "num_open_issues": 1, "created_at": 1455466084.0}}, "HashirZahir/FIFA-Player-Ratings_FIFAscrape/spiders/fifa_spider.py_1": {"code": "def parse_playerURL(self, response):    \r\n        #parses player specific data into items list\r\n        site = Selector(response)\r\n        items = []\r\n        item = PlayerItem()\r\n        item['1name'] = (response.url).rsplit(\"/\")[-2].replace(\"-\",\" \")\r\n        title = self.clean_str(site.xpath('/html/head/title/text()').extract_first())\r\n        item['OVR'] = title.partition(\"FIFA 16 -\")[1].split(\"-\")[0]\r\n        item['POS'] = self.clean_str(site.xpath('//div[@class=\"playercard-position\"]/text()').extract_first())\r\n        #stats = site.xpath('//div[@class=\"row player-center-container\"]/div/a')\r\n        stat_names = site.xpath('//span[@class=\"player-stat-title\"]')\r\n        stat_values = site.xpath('//span[contains(@class, \"player-stat-value\")]')\r\n        for index in range(len(stat_names)):\r\n            attr_name = stat_names[index].xpath('.//text()').extract_first()\r\n            item[attr_name] = stat_values[index].xpath('.//text()').extract_first()\r\n        items.append(item)\r\n        return items", "documentation": "Parses the player s page and returns a list of player items", "reputation": {"num_stars": 8, "num_forks": 8, "num_watchers": 8, "num_open_issues": 1, "created_at": 1455466084.0}}, "HashirZahir/FIFA-Player-Ratings_FIFAscrape/spiders/fifa_spider.py_2": {"code": "def clean_str(self,ustring):    \r\n        #removes wierd unicode chars (/u102 bla), whitespaces, tabspaces, etc to form clean string \r\n        return str(ustring.encode('ascii', 'replace')).strip()", "documentation": "cleans string for parsing", "reputation": {"num_stars": 8, "num_forks": 8, "num_watchers": 8, "num_open_issues": 1, "created_at": 1455466084.0}}, "HashirZahir/FIFA-Player-Ratings_FIFAscrape/spiders/fifa_spider.py_3": {"code": "def random_ua(self):\r\n        #randomise user-agent from list to reduce chance of being banned\r\n        ua  = random.choice(settings.get('USER_AGENT_LIST'))\r\n        if ua:\r\n            ua='Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2226.0 Safari/537.36'\r\n        return ua", "documentation": "Return random user agent", "reputation": {"num_stars": 8, "num_forks": 8, "num_watchers": 8, "num_open_issues": 1, "created_at": 1455466084.0}}, "Azure/azure-sdk-for-python_sdk/sql/azure-mgmt-sqlvirtualmachine/azure/mgmt/sqlvirtualmachine/aio/_sql_virtual_machine_management_client.py_0": {"code": "def __init__(\n        self,\n        credential: \"AsyncTokenCredential\",\n        subscription_id: str,\n        base_url: str = \"https://management.azure.com\",\n        **kwargs: Any", "documentation": "Initialize the management API endpoint .", "reputation": {"num_stars": 3512, "num_forks": 2241, "num_watchers": 3512, "num_open_issues": 950, "created_at": 1335285972.0}}, "Azure/azure-sdk-for-python_sdk/sql/azure-mgmt-sqlvirtualmachine/azure/mgmt/sqlvirtualmachine/aio/_sql_virtual_machine_management_client.py_1": {"code": "def _send_request(\n        self,\n        request: HttpRequest,\n        **kwargs: Any", "documentation": "Send the request to the target .", "reputation": {"num_stars": 3512, "num_forks": 2241, "num_watchers": 3512, "num_open_issues": 950, "created_at": 1335285972.0}}, "TheMasterGhost/CorpBot_Cogs/Time.py_0": {"code": "def __init__(self, bot, settings):\r\n\t\tself.bot = bot\r\n\t\tself.settings = settings", "documentation": "Initializes a new instance of the class .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1501969616.0}}, "TheMasterGhost/CorpBot_Cogs/Time.py_1": {"code": "def getTimeFromOffset(self, offset):\r\n\t\toffset = offset.replace('+', '')\r\n\t\t# Split time string by : and get hour/minute values\r\n\t\ttry:\r\n\t\t\thours, minutes = map(int, offset.split(':'))\r\n\t\texcept Exception:\r\n\t\t\ttry:\r\n\t\t\t\thours = int(offset)\r\n\t\t\t\tminutes = 0\r\n\t\t\texcept Exception:\r\n\t\t\t\treturn None\r\n\t\t\t\t# await ctx.channel.send('Offset has to be in +-H:M!')\r\n\t\t\t\t# return\r\n\t\tmsg = 'UTC'\r\n\t\t# Get the time\r\n\t\tt = datetime.datetime.utcnow()\r\n\t\t# Apply offset\r\n\t\tif hours > 0:\r\n\t\t\t# Apply positive offset\r\n\t\t\tmsg += '+{}'.format(offset)\r\n\t\t\ttd = datetime.timedelta(hours=hours, minutes=minutes)\r\n\t\t\tnewTime = t + td\r\n\t\telif hours < 0:\r\n\t\t\t# Apply negative offset\r\n\t\t\tmsg += '{}'.format(offset)\r\n\t\t\ttd = datetime.timedelta(hours=(-1*hours), minutes=(-1*minutes))\r\n\t\t\tnewTime = t - td\r\n\t\telse:\r\n\t\t\t# No offset\r\n\t\t\tnewTime = t\r\n\t\treturn { \"zone\" : msg, \"time\" : newTime.strftime(\"%I:%M %p\") }", "documentation": "Get the time based on the offset", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1501969616.0}}, "the-zebulan/CodeWars_tests/beta_tests/test_what_color_is_your_name.py_0": {"code": "def test_equal_1(self):\n        self.assertEqual(string_color('Jack'), '79CAE5')", "documentation": "Check if color is equal to another color", "reputation": {"num_stars": 31, "num_forks": 15, "num_watchers": 31, "num_open_issues": 0, "created_at": 1436591903.0}}, "the-zebulan/CodeWars_tests/beta_tests/test_what_color_is_your_name.py_1": {"code": "def test_equal_3(self):\n        self.assertEqual(string_color('Joshua Smith'), '8F00FB')", "documentation": "Test equal between 2 colors", "reputation": {"num_stars": 31, "num_forks": 15, "num_watchers": 31, "num_open_issues": 0, "created_at": 1436591903.0}}, "the-zebulan/CodeWars_tests/beta_tests/test_what_color_is_your_name.py_2": {"code": "def test_equal_5(self):\n        self.assertEqual(string_color('Mathew Smith'), '8B00F1')", "documentation": "Test equal color to 5", "reputation": {"num_stars": 31, "num_forks": 15, "num_watchers": 31, "num_open_issues": 0, "created_at": 1436591903.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_0": {"code": "def setup_class(cls):\n        global users, users_autoinc, metadata\n        metadata = MetaData(testing.db)\n        users = Table('users', metadata,\n            Column('user_id', INT, primary_key=True, autoincrement=False),\n            Column('user_name', VARCHAR(20)),\n        )\n        users_autoinc = Table('users_autoinc', metadata,\n            Column('user_id', INT, primary_key=True,\n                                    test_needs_autoincrement=True),\n            Column('user_name', VARCHAR(20)),\n        )\n        metadata.create_all()", "documentation": "Tables and metadata for the entire class", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_1": {"code": "def teardown(self):\n        testing.db.execute(users.delete())", "documentation": "Teardown user database .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_2": {"code": "def teardown_class(cls):\n        metadata.drop_all()", "documentation": "Drop all tables from pymongo DB .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_3": {"code": "def test_no_params_option(self):\n        stmt = \"SELECT '%'\" + testing.db.dialect.statement_compiler(\n                                    testing.db.dialect, None).default_from()\n\n        conn = testing.db.connect()\n        result = conn.\\\n                execution_options(no_parameters=True).\\\n                scalar(stmt)\n        eq_(result, '%')", "documentation": "Test the % execution_options with no parameters .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_4": {"code": "def test_raw_qmark(self):\n        def go(conn):\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (?, ?)', (1, 'jack'))\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (?, ?)', [2, 'fred'])\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (?, ?)', [3, 'ed'], [4, 'horse'])\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (?, ?)', (5, 'barney'), (6, 'donkey'))\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (?, ?)', 7, 'sally')\n            res = conn.execute('select * from users order by user_id')\n            assert res.fetchall() == [\n                (1, 'jack'),\n                (2, 'fred'),\n                (3, 'ed'),\n                (4, 'horse'),\n                (5, 'barney'),\n                (6, 'donkey'),\n                (7, 'sally'),\n                ]\n            for multiparam, param in [\n                ((\"jack\", \"fred\"), {}),\n                (([\"jack\", \"fred\"],), {})\n            ]:\n                res = conn.execute(\n                    \"select * from users where user_name=? or \"\n                    \"user_name=? order by user_id\",\n                    *multiparam, **param)\n                assert res.fetchall() == [\n                    (1, 'jack'),\n                    (2, 'fred')\n                ]\n            res = conn.execute(\"select * from users where user_name=?\",\n                \"jack\"\n            )\n            assert res.fetchall() == [(1, 'jack')]\n            conn.execute('delete from users')\n\n        go(testing.db)\n        conn = testing.db.connect()\n        try:\n            go(conn)\n        finally:\n            conn.close()", "documentation": "Test that the raw_qmark function works correctly with sqlite3 .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_5": {"code": "def test_raw_sprintf(self):\n        def go(conn):\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (%s, %s)', [1, 'jack'])\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (%s, %s)', [2, 'ed'], [3, 'horse'])\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (%s, %s)', 4, 'sally')\n            conn.execute('insert into users (user_id) values (%s)', 5)\n            res = conn.execute('select * from users order by user_id')\n            assert res.fetchall() == [(1, 'jack'), (2, 'ed'), (3,\n                    'horse'), (4, 'sally'), (5, None)]\n            for multiparam, param in [\n                ((\"jack\", \"ed\"), {}),\n                (([\"jack\", \"ed\"],), {})\n            ]:\n                res = conn.execute(\n                    \"select * from users where user_name=%s or \"\n                    \"user_name=%s order by user_id\",\n                    *multiparam, **param)\n                assert res.fetchall() == [\n                    (1, 'jack'),\n                    (2, 'ed')\n                ]\n            res = conn.execute(\"select * from users where user_name=%s\",\n                \"jack\"\n            )\n            assert res.fetchall() == [(1, 'jack')]\n\n            conn.execute('delete from users')\n        go(testing.db)\n        conn = testing.db.connect()\n        try:\n            go(conn)\n        finally:\n            conn.close()", "documentation": "Run raw_sprintf on the connection object in the test_db object .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_6": {"code": "def test_raw_python(self):\n        def go(conn):\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (%(id)s, %(name)s)', {'id': 1, 'name'\n                         : 'jack'})\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (%(id)s, %(name)s)', {'id': 2, 'name'\n                         : 'ed'}, {'id': 3, 'name': 'horse'})\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (%(id)s, %(name)s)', id=4, name='sally'\n                         )\n            res = conn.execute('select * from users order by user_id')\n            assert res.fetchall() == [(1, 'jack'), (2, 'ed'), (3,\n                    'horse'), (4, 'sally')]\n            conn.execute('delete from users')\n        go(testing.db)\n        conn = testing.db.connect()\n        try:\n            go(conn)\n        finally:\n            conn.close()", "documentation": "Test that we can use raw_python to run our tests .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_7": {"code": "def test_raw_named(self):\n        def go(conn):\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (:id, :name)', {'id': 1, 'name': 'jack'\n                         })\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (:id, :name)', {'id': 2, 'name': 'ed'\n                         }, {'id': 3, 'name': 'horse'})\n            conn.execute('insert into users (user_id, user_name) '\n                         'values (:id, :name)', id=4, name='sally')\n            res = conn.execute('select * from users order by user_id')\n            assert res.fetchall() == [(1, 'jack'), (2, 'ed'), (3,\n                    'horse'), (4, 'sally')]\n            conn.execute('delete from users')\n        go(testing.db)\n        conn= testing.db.connect()\n        try:\n            go(conn)\n        finally:\n            conn.close()", "documentation": "Test that raw_named works as expected", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_8": {"code": "def test_exception_wrapping_dbapi(self):\n        conn = testing.db.connect()\n        for _c in testing.db, conn:\n            assert_raises_message(\n                tsa.exc.DBAPIError,\n                r\"not_a_valid_statement\",\n                _c.execute, 'not_a_valid_statement'\n            )", "documentation": "Test that we can wrap an exception in the dbapi . exc . DBAPIError", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_9": {"code": "def test_exception_wrapping_non_dbapi_error(self):\n        e = create_engine('sqlite://')\n        e.dialect.is_disconnect = is_disconnect = Mock()\n\n        with e.connect() as c:\n            c.connection.cursor = Mock(\n                    return_value=Mock(\n                        execute=Mock(\n                                side_effect=TypeError(\"I'm not a DBAPI error\")\n                        ))\n                    )\n\n            assert_raises_message(\n                TypeError,\n                \"I'm not a DBAPI error\",\n                c.execute, \"select \"\n            )\n            eq_(is_disconnect.call_count, 0)", "documentation": "Test that an exception is not a DBAPI error", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_10": {"code": "def process_bind_param(self, value, dialect):\n                raise Exception(\"nope\")", "documentation": "Implement the DatabaseBinding abstract method", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_11": {"code": "def test_stmt_exception_non_ascii(self):\n        name = util.u('m\u00e9il')\n        with testing.db.connect() as conn:\n            assert_raises_message(\n                tsa.exc.StatementError,\n                util.u(\n                    \"A value is required for bind parameter 'uname'\"\n                    r'.*SELECT users.user_name AS .m\\\\xe9il.') if util.py2k\n                else\n                    util.u(\n                        \"A value is required for bind parameter 'uname'\"\n                        '.*SELECT users.user_name AS .m\u00e9il.')\n                    ,\n                conn.execute,\n                select([users.c.user_name.label(name)]).where(\n                                users.c.user_name == bindparam(\"uname\")),\n                {'uname_incorrect': 'foo'}\n            )", "documentation": "Test for TSA exception when trying to insert non ASCII characters", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_12": {"code": "def test_stmt_exception_pickleable_plus_dbapi(self):\n        raw = testing.db.raw_connection()\n        the_orig = None\n        try:\n            try:\n                cursor = raw.cursor()\n                cursor.execute(\"SELECTINCORRECT\")\n            except testing.db.dialect.dbapi.DatabaseError as orig:\n                # py3k has \"orig\" in local scope...\n                the_orig = orig\n        finally:\n            raw.close()\n        self._test_stmt_exception_pickleable(the_orig)", "documentation": "Test that we catch pickleable exceptions from Statements and raise them as subclasses .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_13": {"code": "def test_dont_wrap_mixin(self):\n        class MyException(Exception, tsa.exc.DontWrapMixin):\n            pass\n\n        class MyType(TypeDecorator):\n            impl = Integer\n            def process_bind_param(self, value, dialect):\n                raise MyException(\"nope\")\n\n        def _go(conn):\n            assert_raises_message(\n                MyException,\n                \"nope\",\n                conn.execute,\n                    select([1]).\\\n                        where(\n                            column('foo') == literal('bar', MyType())\n                        )\n            )\n        _go(testing.db)\n        conn = testing.db.connect()\n        try:\n            _go(conn)\n        finally:\n            conn.close()", "documentation": "Test that we can use the DontWrapMixin .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_14": {"code": "def test_engine_level_options(self):\n        eng = engines.testing_engine(options={'execution_options':\n                                            {'foo': 'bar'}})\n        with eng.contextual_connect() as conn:\n            eq_(conn._execution_options['foo'], 'bar')\n            eq_(conn.execution_options(bat='hoho')._execution_options['foo'\n                ], 'bar')\n            eq_(conn.execution_options(bat='hoho')._execution_options['bat'\n                ], 'hoho')\n            eq_(conn.execution_options(foo='hoho')._execution_options['foo'\n                ], 'hoho')\n            eng.update_execution_options(foo='hoho')\n            conn = eng.contextual_connect()\n            eq_(conn._execution_options['foo'], 'hoho')", "documentation": "Test Engine level options with contextual connect .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_15": {"code": "def test_generative_engine_execution_options(self):\n        eng = engines.testing_engine(options={'execution_options':\n                                            {'base': 'x1'}})\n\n        eng1 = eng.execution_options(foo=\"b1\")\n        eng2 = eng.execution_options(foo=\"b2\")\n        eng1a = eng1.execution_options(bar=\"a1\")\n        eng2a = eng2.execution_options(foo=\"b3\", bar=\"a2\")\n\n        eq_(eng._execution_options,\n                {'base': 'x1'})\n        eq_(eng1._execution_options,\n                {'base': 'x1', 'foo': 'b1'})\n        eq_(eng2._execution_options,\n                {'base': 'x1', 'foo': 'b2'})\n        eq_(eng1a._execution_options,\n                {'base': 'x1', 'foo': 'b1', 'bar': 'a1'})\n        eq_(eng2a._execution_options,\n                {'base': 'x1', 'foo': 'b3', 'bar': 'a2'})\n        is_(eng1a.pool, eng.pool)\n\n        # test pool is shared\n        eng2.dispose()\n        is_(eng1a.pool, eng2.pool)\n        is_(eng.pool, eng2.pool)", "documentation": "Test the generation of execution options with multiple engines .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_16": {"code": "def test_generative_engine_event_dispatch(self):\n        canary = []\n        def l1(*arg, **kw):\n            canary.append(\"l1\")\n        def l2(*arg, **kw):\n            canary.append(\"l2\")\n        def l3(*arg, **kw):\n            canary.append(\"l3\")\n\n        eng = engines.testing_engine(options={'execution_options':\n                                            {'base': 'x1'}})\n        event.listen(eng, \"before_execute\", l1)\n\n        eng1 = eng.execution_options(foo=\"b1\")\n        event.listen(eng, \"before_execute\", l2)\n        event.listen(eng1, \"before_execute\", l3)\n\n        eng.execute(select([1])).close()\n        eng1.execute(select([1])).close()\n\n        eq_(canary, [\"l1\", \"l2\", \"l3\", \"l1\", \"l2\"])", "documentation": "Test that the event dispatching works .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_17": {"code": "def test_generative_engine_event_dispatch_hasevents(self):\n        def l1(*arg, **kw):\n            pass\n        eng = create_engine(testing.db.url)\n        assert not eng._has_events\n        event.listen(eng, \"before_execute\", l1)\n        eng2 = eng.execution_options(foo='bar')\n        assert eng2._has_events", "documentation": "Test that the engine fires events not only before_execute", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_18": {"code": "def execute(self, stmt, params=None, **kw):\n                if \"test unicode returns\" in stmt:\n                    raise self.engine.dialect.dbapi.DatabaseError(\"boom\")\n                else:\n                    return super(MockCursor, self).execute(stmt, params, **kw)", "documentation": "Testing execute - raises DatabaseError if returns unicode", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_19": {"code": "def test_works_after_dispose(self):\n        eng = create_engine(testing.db.url)\n        for i in range(3):\n            eq_(eng.scalar(select([1])), 1)\n            eng.dispose()", "documentation": "Test works after dispose of an engine .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_20": {"code": "def define_tables(cls, metadata):\n        cls.table = Table('exec_test', metadata,\n            Column('a', Integer),\n            Column('b', Integer),\n            test_needs_acid=True\n        )", "documentation": "Defines the required tables in the test namespace .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_21": {"code": "def go(conn, x, value=None):\n            if is_transaction:\n                conn = conn.connection\n            conn.execute(self.table.insert().values(a=x, b=value))", "documentation": "Executes a INSERT query on the current connection .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_22": {"code": "def _trans_rollback_fn(self, is_transaction=False):\n        def go(conn, x, value=None):\n            if is_transaction:\n                conn = conn.connection\n            conn.execute(self.table.insert().values(a=x, b=value))\n            raise Exception(\"breakage\")\n        return go", "documentation": "Rollback rollback function decorator .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_23": {"code": "def _assert_fn(self, x, value=None):\n        eq_(\n            testing.db.execute(self.table.select()).fetchall(),\n            [(x, value)]\n        )", "documentation": "Assert a function is called with the given arguments", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_24": {"code": "def test_transaction_engine_ctx_begin_fails(self):\n        engine = engines.testing_engine()\n\n        mock_connection = Mock(\n            return_value=Mock(\n                        begin=Mock(side_effect=Exception(\"boom\"))\n                    )\n        )\n        engine._connection_cls = mock_connection\n        assert_raises(\n            Exception,\n            engine.begin\n        )\n\n        eq_(\n            mock_connection.return_value.close.mock_calls,\n            [call()]\n        )", "documentation": "Tests the behavior of TransactionEngine . begin", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_25": {"code": "def test_transaction_tlocal_engine_ctx_commit(self):\n        fn = self._trans_fn()\n        engine = engines.testing_engine(options=dict(\n                                strategy='threadlocal',\n                                pool=testing.db.pool))\n        ctx = engine.begin()\n        testing.run_as_contextmanager(ctx, fn, 5, value=8)\n        self._assert_fn(5, value=8)", "documentation": "Test transaction with threadlocal engine with transaction context .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_26": {"code": "def test_transaction_connection_ctx_commit(self):\n        fn = self._trans_fn(True)\n        conn = testing.db.connect()\n        ctx = conn.begin()\n        testing.run_as_contextmanager(ctx, fn, 5, value=8)\n        self._assert_fn(5, value=8)", "documentation": "Test transaction connection with context manager commit .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_27": {"code": "def test_connection_as_ctx(self):\n        fn = self._trans_fn()\n        ctx = testing.db.connect()\n        testing.run_as_contextmanager(ctx, fn, 5, value=8)\n        # autocommit is on\n        self._assert_fn(5, value=8)", "documentation": "Make sure the connection is as contextmanager", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_28": {"code": "def test_connect_as_ctx_noautocommit(self):\n        fn = self._trans_fn()\n        self._assert_no_data()\n        ctx = testing.db.connect().execution_options(autocommit=False)\n        testing.run_as_contextmanager(ctx, fn, 5, value=8)\n        # autocommit is off\n        self._assert_no_data()", "documentation": "Connect using a contextmanager with autocommit False", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_29": {"code": "def test_transaction_engine_fn_rollback(self):\n        fn = self._trans_rollback_fn()\n        assert_raises_message(\n            Exception,\n            \"breakage\",\n            testing.db.transaction, fn, 5, value=8\n        )\n        self._assert_no_data()", "documentation": "Test the rollback function for a transaction engine that rolls back the current transaction .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_30": {"code": "def test_transaction_connection_fn_rollback(self):\n        fn = self._trans_rollback_fn()\n        conn = testing.db.connect()\n        assert_raises(\n            Exception,\n            conn.transaction, fn, 5, value=8\n        )\n        self._assert_no_data()", "documentation": "Test the rollback function for a connection that does not have a data value set", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_31": {"code": "def setup_class(cls):\n        global users, metadata\n        metadata = MetaData(testing.db)\n        users = Table('users', metadata,\n            Column('user_id', INT, primary_key=True,\n                            test_needs_autoincrement=True),\n            Column('user_name', VARCHAR(20)),\n        )\n        metadata.create_all()", "documentation": "Create the tables needed for the test cases", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_32": {"code": "def teardown(self):\n        testing.db.execute(users.delete())", "documentation": "Teardown user database .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_33": {"code": "def teardown_class(cls):\n        metadata.drop_all()", "documentation": "Drop all tables from pymongo DB .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_34": {"code": "def _engine_fixture(self):\n        buf = util.StringIO()\n        def dump(sql, *multiparams, **params):\n            buf.write(util.text_type(sql.compile(dialect=engine.dialect)))\n        engine = create_engine('postgresql://', strategy='mock', executor=dump)\n        return engine, buf", "documentation": "Fixture method creates an engine and a StringIO object", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_35": {"code": "def test_nontuple_row(self):\n        \"\"\"ensure the C version of BaseRowProxy handles\n        duck-type-dependent rows.\"\"\"\n\n        from sqlalchemy.engine import RowProxy\n\n        class MyList(object):\n            def __init__(self, l):\n                self.l = l\n\n            def __len__(self):\n                return len(self.l)\n\n            def __getitem__(self, i):\n                return list.__getitem__(self.l, i)\n\n        proxy = RowProxy(object(), MyList(['value']), [None], {'key'\n                         : (None, None, 0), 0: (None, None, 0)})\n        eq_(list(proxy), ['value'])\n        eq_(proxy[0], 'value')\n        eq_(proxy['key'], 'value')", "documentation": "make sure the C version of RowProxy works for duck - type - dependent rows", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_36": {"code": "def test_no_rowcount_on_selects_inserts(self):\n        \"\"\"assert that rowcount is only called on deletes and updates.\n\n        This because cursor.rowcount may can be expensive on some dialects\n        such as Firebird, however many dialects require it be called\n        before the cursor is closed.\n\n        \"\"\"\n\n        metadata = self.metadata\n\n        engine = engines.testing_engine()\n\n        t = Table('t1', metadata,\n            Column('data', String(10))\n        )\n        metadata.create_all(engine)\n\n        with patch.object(engine.dialect.execution_ctx_cls, \"rowcount\") as mock_rowcount:\n            mock_rowcount.__get__ = Mock()\n            engine.execute(t.insert(),\n                                {'data': 'd1'},\n                                {'data': 'd2'},\n                                {'data': 'd3'})\n\n            eq_(len(mock_rowcount.__get__.mock_calls), 0)\n\n            eq_(\n                    engine.execute(t.select()).fetchall(),\n                    [('d1', ), ('d2', ), ('d3', )]\n            )\n            eq_(len(mock_rowcount.__get__.mock_calls), 0)\n\n            engine.execute(t.update(), {'data': 'd4'})\n\n            eq_(len(mock_rowcount.__get__.mock_calls), 1)\n\n            engine.execute(t.delete())\n            eq_(len(mock_rowcount.__get__.mock_calls), 2)", "documentation": "Assert that the rowcount is not called on deletes and updates .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_37": {"code": "def test_row_c_sequence_check(self):\n        import csv\n        import collections\n\n        metadata = MetaData()\n        metadata.bind = 'sqlite://'\n        users = Table('users', metadata,\n            Column('id', Integer, primary_key=True),\n            Column('name', String(40)),\n        )\n        users.create()\n\n        users.insert().execute(name='Test')\n        row = users.select().execute().fetchone()\n\n        s = util.StringIO()\n        writer = csv.writer(s)\n        # csv performs PySequenceCheck call\n        writer.writerow(row)\n        assert s.getvalue().strip() == '1,Test'", "documentation": "Test that row c sequences are not returned by CSV", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_38": {"code": "def test_empty_accessors(self):\n        statements = [\n            (\n                \"select 1\",\n                [\n                    lambda r: r.last_inserted_params(),\n                    lambda r: r.last_updated_params(),\n                    lambda r: r.prefetch_cols(),\n                    lambda r: r.postfetch_cols(),\n                    lambda r : r.inserted_primary_key\n                ],\n                \"Statement is not a compiled expression construct.\"\n            ),\n            (\n                select([1]),\n                [\n                    lambda r: r.last_inserted_params(),\n                    lambda r : r.inserted_primary_key\n                ],\n                r\"Statement is not an insert\\(\\) expression construct.\"\n            ),\n            (\n                select([1]),\n                [\n                    lambda r: r.last_updated_params(),\n                ],\n                r\"Statement is not an update\\(\\) expression construct.\"\n            ),\n            (\n                select([1]),\n                [\n                    lambda r: r.prefetch_cols(),\n                    lambda r : r.postfetch_cols()\n                ],\n                r\"Statement is not an insert\\(\\) \"\n                r\"or update\\(\\) expression construct.\"\n            ),\n        ]\n\n        for stmt, meths, msg in statements:\n            r = testing.db.execute(stmt)\n            try:\n                for meth in meths:\n                    assert_raises_message(\n                        tsa.exc.InvalidRequestError,\n                        msg,\n                        meth, r\n                    )\n\n            finally:\n                r.close()", "documentation": "Test empty accessors of the database .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_39": {"code": "def test_dialect_conn_options(self):\n        engine = testing_engine(\"sqlite://\", options=dict(_initialize=False))\n        engine.dialect = Mock()\n        conn = engine.connect()\n        c2 = conn.execution_options(foo=\"bar\")\n        eq_(\n            engine.dialect.set_connection_execution_options.mock_calls,\n            [call(c2, {\"foo\": \"bar\"})]\n        )", "documentation": "Test setting connection execution options when the dialect is set to psycopg2 .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_40": {"code": "def test_dialect_engine_construction_options(self):\n        dialect = Mock()\n        engine = Engine(Mock(), dialect, Mock(),\n                                execution_options={\"foo\": \"bar\"})\n        eq_(\n            dialect.set_engine_execution_options.mock_calls,\n            [call(engine, {\"foo\": \"bar\"})]\n        )", "documentation": "Test dialects that use set_engine_execution_options", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_41": {"code": "def test_propagate_option_engine_to_connection(self):\n        e1 = testing_engine(\"sqlite://\",\n                        options=dict(execution_options={\"foo\": \"bar\"}))\n        e2 = e1.execution_options(bat=\"hoho\")\n        c1 = e1.connect()\n        c2 = e2.connect()\n        eq_(c1._execution_options, {\"foo\": \"bar\"})\n        eq_(c2._execution_options, {\"foo\": \"bar\", \"bat\": \"hoho\"})", "documentation": "Test that we can propagate the execution options to the engine by passing a dictionary", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_42": {"code": "def setup_class(cls):\n        from sqlalchemy.engine import base, default\n        cls.engine = engine = testing_engine('sqlite://')\n        m = MetaData()\n        cls.table = t = Table('test', m,\n            Column('x', Integer, primary_key=True),\n            Column('y', String(50, convert_unicode='force'))\n        )\n        m.create_all(engine)\n        engine.execute(t.insert(), [\n            {'x':i, 'y':\"t_%d\" % i} for i in range(1, 12)\n        ])", "documentation": "Setup the sqlalchemy engine and table .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_43": {"code": "def get_result_proxy(self):\n                return cls(self)", "documentation": "Get a proxy for the result of a query .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_44": {"code": "def test_plain(self):\n        self._test_proxy(_result.ResultProxy)", "documentation": "Run the test as a plain text test", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_45": {"code": "def test_fully_buffered_result_proxy(self):\n        self._test_proxy(_result.FullyBufferedResultProxy)", "documentation": "Tests the FullyBufferedResultProxy .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_46": {"code": "def tearDown(self):\n        Engine.dispatch._clear()\n        Engine._has_events = False", "documentation": "Clears the dispatch stack and event dispatch stack during engine tearDown", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_47": {"code": "def test_per_engine_independence(self):\n        e1 = testing_engine(config.db_url)\n        e2 = testing_engine(config.db_url)\n\n        canary = Mock()\n        event.listen(e1, \"before_execute\", canary)\n        s1 = select([1])\n        s2 = select([2])\n        e1.execute(s1)\n        e2.execute(s2)\n        eq_(\n            [arg[1][1] for arg in canary.mock_calls], [s1]\n        )\n        event.listen(e2, \"before_execute\", canary)\n        e1.execute(s1)\n        e2.execute(s2)\n        eq_([arg[1][1] for arg in canary.mock_calls], [s1, s1, s2])", "documentation": "Test one engine depends on another engine .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_48": {"code": "def test_per_connection_plus_engine(self):\n        canary = Mock()\n        e1 = testing_engine(config.db_url)\n\n        event.listen(e1, \"before_execute\", canary.be1)\n\n        conn = e1.connect()\n        event.listen(conn, \"before_execute\", canary.be2)\n        conn.execute(select([1]))\n\n        eq_(canary.be1.call_count, 1)\n        eq_(canary.be2.call_count, 1)\n\n        conn._branch().execute(select([1]))\n        eq_(canary.be1.call_count, 2)\n        eq_(canary.be2.call_count, 2)", "documentation": "Test that we can connect to a single engine with multiple connections .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_49": {"code": "def test_force_conn_events_false(self):\n        canary = Mock()\n        e1 = create_engine(config.db_url)\n        assert not e1._has_events\n\n        event.listen(e1, \"before_execute\", canary.be1)\n\n        conn = e1._connection_cls(e1, connection=e1.raw_connection(),\n                            _has_events=False)\n\n        conn.execute(select([1]))\n\n        eq_(canary.be1.call_count, 0)\n\n        conn._branch().execute(select([1]))\n        eq_(canary.be1.call_count, 0)", "documentation": "Test that we dont call event . listen to all events but do not use them", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_50": {"code": "def test_cursor_events_execute(self):\n        canary = Mock()\n        e1 = testing_engine(config.db_url)\n\n        event.listen(e1, \"before_cursor_execute\", canary.bce)\n        event.listen(e1, \"after_cursor_execute\", canary.ace)\n\n        stmt = str(select([1]).compile(dialect=e1.dialect))\n\n        with e1.connect() as conn:\n\n            result = conn.execute(stmt)\n\n        ctx = result.context\n        eq_(canary.bce.mock_calls,\n                [call(conn, ctx.cursor, stmt, ctx.parameters[0], ctx, False)])\n        eq_(canary.ace.mock_calls,\n                [call(conn, ctx.cursor, stmt, ctx.parameters[0], ctx, False)])", "documentation": "Test that cursor events execute with Mock", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_51": {"code": "def before_execute(conn, clauseelement, multiparams, params):\n            assert isinstance(multiparams, (list, tuple))\n            assert isinstance(params, dict)", "documentation": "Called before a query is executed", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_52": {"code": "def test_execute_events(self):\n\n        stmts = []\n        cursor_stmts = []\n\n        def execute(conn, clauseelement, multiparams,\n                                                    params ):\n            stmts.append((str(clauseelement), params, multiparams))\n\n        def cursor_execute(conn, cursor, statement, parameters,\n                                context, executemany):\n            cursor_stmts.append((str(statement), parameters, None))\n\n\n        for engine in [\n            engines.testing_engine(options=dict(implicit_returning=False)),\n            engines.testing_engine(options=dict(implicit_returning=False,\n                                   strategy='threadlocal')),\n            engines.testing_engine(options=dict(implicit_returning=False)).\\\n                connect()\n            ]:\n            event.listen(engine, 'before_execute', execute)\n            event.listen(engine, 'before_cursor_execute', cursor_execute)\n            m = MetaData(engine)\n            t1 = Table('t1', m,\n                Column('c1', Integer, primary_key=True),\n                Column('c2', String(50), default=func.lower('Foo'),\n                                            primary_key=True)\n            )\n            m.create_all()\n            try:\n                t1.insert().execute(c1=5, c2='some data')\n                t1.insert().execute(c1=6)\n                eq_(engine.execute('select * from t1').fetchall(), [(5,\n                    'some data'), (6, 'foo')])\n            finally:\n                m.drop_all()\n\n            compiled = [('CREATE TABLE t1', {}, None),\n                        ('INSERT INTO t1 (c1, c2)',\n                                {'c2': 'some data', 'c1': 5}, None),\n                        ('INSERT INTO t1 (c1, c2)',\n                        {'c1': 6}, None),\n                        ('select * from t1', {}, None),\n                        ('DROP TABLE t1', {}, None)]\n\n            # or engine.dialect.preexecute_pk_sequences:\n            if not testing.against('oracle+zxjdbc'):\n                cursor = [\n                    ('CREATE TABLE t1', {}, ()),\n                    ('INSERT INTO t1 (c1, c2)', {\n                        'c2': 'some data', 'c1': 5},\n                        (5, 'some data')),\n                    ('SELECT lower', {'lower_2': 'Foo'},\n                        ('Foo', )),\n                    ('INSERT INTO t1 (c1, c2)',\n                     {'c2': 'foo', 'c1': 6},\n                     (6, 'foo')),\n                    ('select * from t1', {}, ()),\n                    ('DROP TABLE t1', {}, ()),\n                    ]\n            else:\n                insert2_params = 6, 'Foo'\n                if testing.against('oracle+zxjdbc'):\n                    insert2_params += (ReturningParam(12), )\n                cursor = [('CREATE TABLE t1', {}, ()),\n                          ('INSERT INTO t1 (c1, c2)',\n                            {'c2': 'some data', 'c1': 5}, (5, 'some data')),\n                          ('INSERT INTO t1 (c1, c2)', {'c1': 6,\n                          'lower_2': 'Foo'}, insert2_params),\n                          ('select * from t1', {}, ()),\n                          ('DROP TABLE t1', {}, ())]\n                                # bind param name 'lower_2' might\n                                # be incorrect\n            self._assert_stmts(compiled, stmts)\n            self._assert_stmts(cursor, cursor_stmts)", "documentation": "Test the event dispatching to execute with prepared statements and cursor execute .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_53": {"code": "def execute(conn, *args, **kw):\n            canary.append('execute')", "documentation": "Add a execute statement to the canary", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_54": {"code": "def test_retval_flag(self):\n        canary = []\n        def tracker(name):\n            def go(conn, *args, **kw):\n                canary.append(name)\n            return go\n\n        def execute(conn, clauseelement, multiparams, params):\n            canary.append('execute')\n            return clauseelement, multiparams, params\n\n        def cursor_execute(conn, cursor, statement,\n                        parameters, context, executemany):\n            canary.append('cursor_execute')\n            return statement, parameters\n\n        engine = engines.testing_engine()\n\n        assert_raises(\n            tsa.exc.ArgumentError,\n            event.listen, engine, \"begin\", tracker(\"begin\"), retval=True\n        )\n\n        event.listen(engine, \"before_execute\", execute, retval=True)\n        event.listen(engine, \"before_cursor_execute\", cursor_execute, retval=True)\n        engine.execute(select([1]))\n        eq_(\n            canary, ['execute', 'cursor_execute']\n        )", "documentation": "Test return value of True in sqlalchemy .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_55": {"code": "def test_execution_options(self):\n        engine = engines.testing_engine()\n\n        engine_tracker = Mock()\n        conn_tracker = Mock()\n\n        event.listen(engine, \"set_engine_execution_options\", engine_tracker)\n        event.listen(engine, \"set_connection_execution_options\", conn_tracker)\n\n        e2 = engine.execution_options(e1='opt_e1')\n        c1 = engine.connect()\n        c2 = c1.execution_options(c1='opt_c1')\n        c3 = e2.connect()\n        c4 = c3.execution_options(c3='opt_c3')\n        eq_(\n            engine_tracker.mock_calls,\n            [call(e2, {'e1': 'opt_e1'})]\n        )\n        eq_(\n            conn_tracker.mock_calls,\n            [call(c2, {\"c1\": \"opt_c1\"}), call(c4, {\"c3\": \"opt_c3\"})]\n        )", "documentation": "Test execution options with set_engine_execution_options and set_connection_execution_options", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_56": {"code": "def test_cursor_execute(self):\n        canary = []\n        def tracker(name):\n            def go(conn, cursor, statement, parameters, context, executemany):\n                canary.append((statement, context))\n            return go\n        engine = engines.testing_engine()\n\n\n        t = Table('t', self.metadata,\n                    Column('x', Integer, Sequence('t_id_seq'), primary_key=True),\n                    implicit_returning=False\n                    )\n        self.metadata.create_all(engine)\n        with engine.begin() as conn:\n            event.listen(conn, 'before_cursor_execute', tracker('cursor_execute'))\n            conn.execute(t.insert())\n        # we see the sequence pre-executed in the first call\n        assert \"t_id_seq\" in canary[0][0]\n        assert \"INSERT\" in canary[1][0]\n        # same context\n        is_(\n            canary[0][1], canary[1][1]\n        )", "documentation": "Test that cursor_execute works as expected in the first call to executemany .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_57": {"code": "def tracker(name):\n            def go(conn, *args, **kw):\n                canary.append(name)\n            return go", "documentation": "Decorator to add a name to the list of names that this canary will track .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_58": {"code": "def test_transactional_advanced(self):\n        canary1 = []\n        def tracker1(name):\n            def go(*args, **kw):\n                canary1.append(name)\n            return go\n        canary2 = []\n        def tracker2(name):\n            def go(*args, **kw):\n                canary2.append(name)\n            return go\n\n        engine = engines.testing_engine()\n        for name in ['begin', 'savepoint',\n                    'rollback_savepoint', 'release_savepoint',\n                    'rollback', 'begin_twophase',\n                       'prepare_twophase', 'commit_twophase']:\n            event.listen(engine, '%s' % name, tracker1(name))\n\n        conn = engine.connect()\n        for name in ['begin', 'savepoint',\n                    'rollback_savepoint', 'release_savepoint',\n                    'rollback', 'begin_twophase',\n                       'prepare_twophase', 'commit_twophase']:\n            event.listen(conn, '%s' % name, tracker2(name))\n\n        trans = conn.begin()\n        trans2 = conn.begin_nested()\n        conn.execute(select([1]))\n        trans2.rollback()\n        trans2 = conn.begin_nested()\n        conn.execute(select([1]))\n        trans2.commit()\n        trans.rollback()\n\n        trans = conn.begin_twophase()\n        conn.execute(select([1]))\n        trans.prepare()\n        trans.commit()\n\n        eq_(canary1, ['begin', 'savepoint',\n                    'rollback_savepoint', 'savepoint', 'release_savepoint',\n                    'rollback', 'begin_twophase',\n                       'prepare_twophase', 'commit_twophase']\n        )\n        eq_(canary2, ['begin', 'savepoint',\n                    'rollback_savepoint', 'savepoint', 'release_savepoint',\n                    'rollback', 'begin_twophase',\n                       'prepare_twophase', 'commit_twophase']\n        )", "documentation": "Advanced transactions test with savepoint rollback and prepare_twophase events .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_59": {"code": "def tearDown(self):\n        Engine.dispatch._clear()\n        Engine._has_events = False", "documentation": "Clears the dispatch stack and event dispatch stack during engine tearDown", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_60": {"code": "def test_legacy_dbapi_error_no_ad_hoc_context(self):\n        engine = engines.testing_engine()\n\n        listener = Mock(return_value=None)\n        event.listen(engine, 'dbapi_error', listener)\n\n        nope = Exception(\"nope\")\n        class MyType(TypeDecorator):\n            impl = Integer\n            def process_bind_param(self, value, dialect):\n                raise nope\n\n        with engine.connect() as conn:\n            assert_raises_message(\n                tsa.exc.StatementError,\n                r\"nope \\(original cause: Exception: nope\\) u?'SELECT 1 \",\n                conn.execute,\n                    select([1]).where(\n                            column('foo') == literal('bar', MyType()))\n            )\n        # no legacy event\n        eq_(listener.mock_calls, [])", "documentation": "Test legacy DBAPI error handling with no ad hoc context", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_61": {"code": "def test_handle_error(self):\n        engine = engines.testing_engine()\n        canary = Mock(return_value=None)\n\n        event.listen(engine, \"handle_error\", canary)\n\n        with engine.connect() as conn:\n            try:\n                conn.execute(\"SELECT FOO FROM I_DONT_EXIST\")\n                assert False\n            except tsa.exc.DBAPIError as e:\n                ctx = canary.mock_calls[0][1][0]\n\n                eq_(ctx.original_exception, e.orig)\n                is_(ctx.sqlalchemy_exception, e)\n                eq_(ctx.statement, \"SELECT FOO FROM I_DONT_EXIST\")", "documentation": "Test that handle_error works if the engine crashes .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_62": {"code": "def err(context):\n            stmt = context.statement\n            exception = context.original_exception\n            if \"ERROR ONE\" in str(stmt):\n                return MyException(\"my exception\")\n            elif \"ERROR TWO\" in str(stmt):\n                return exception\n            else:\n                return None", "documentation": "Return error message if statement is unhandled", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_63": {"code": "def test_exception_event_reraise_chaining(self):\n        engine = engines.testing_engine()\n\n        class MyException1(Exception):\n            pass\n\n        class MyException2(Exception):\n            pass\n\n        class MyException3(Exception):\n            pass\n\n        @event.listens_for(engine, 'handle_error', retval=True)\n        def err1(context):\n            stmt = context.statement\n\n            if \"ERROR ONE\" in str(stmt) or \"ERROR TWO\" in str(stmt) \\\n                    or \"ERROR THREE\" in str(stmt):\n                return MyException1(\"my exception\")\n            elif \"ERROR FOUR\" in str(stmt):\n                raise MyException3(\"my exception short circuit\")\n\n        @event.listens_for(engine, 'handle_error', retval=True)\n        def err2(context):\n            stmt = context.statement\n            if (\"ERROR ONE\" in str(stmt) or \"ERROR FOUR\" in str(stmt)) \\\n                    and isinstance(context.chained_exception, MyException1):\n                raise MyException2(\"my exception chained\")\n            elif \"ERROR TWO\" in str(stmt):\n                return context.chained_exception\n            else:\n                return None\n\n        conn = engine.connect()\n\n        with patch.object(engine.\n                dialect.execution_ctx_cls,\n                \"handle_dbapi_exception\") as patched:\n            assert_raises_message(\n                MyException2,\n                \"my exception chained\",\n                conn.execute, \"SELECT 'ERROR ONE' FROM I_DONT_EXIST\"\n            )\n            eq_(patched.call_count, 1)\n\n        with patch.object(engine.\n                dialect.execution_ctx_cls,\n                \"handle_dbapi_exception\") as patched:\n            assert_raises(\n                MyException1,\n                conn.execute, \"SELECT 'ERROR TWO' FROM I_DONT_EXIST\"\n            )\n            eq_(patched.call_count, 1)\n\n        with patch.object(engine.\n                dialect.execution_ctx_cls,\n                \"handle_dbapi_exception\") as patched:\n            # test that non None from err1 isn't cancelled out\n            # by err2\n            assert_raises(\n                MyException1,\n                conn.execute, \"SELECT 'ERROR THREE' FROM I_DONT_EXIST\"\n            )\n            eq_(patched.call_count, 1)\n\n        with patch.object(engine.\n                dialect.execution_ctx_cls,\n                \"handle_dbapi_exception\") as patched:\n            assert_raises(\n                tsa.exc.DBAPIError,\n                conn.execute, \"SELECT 'ERROR FIVE' FROM I_DONT_EXIST\"\n            )\n            eq_(patched.call_count, 1)\n\n        with patch.object(engine.\n                dialect.execution_ctx_cls,\n                \"handle_dbapi_exception\") as patched:\n            assert_raises_message(\n                MyException3,\n                \"my exception short circuit\",\n                conn.execute, \"SELECT 'ERROR FOUR' FROM I_DONT_EXIST\"\n            )\n            eq_(patched.call_count, 1)", "documentation": "Reraises chained exceptions with a message .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_64": {"code": "def process_bind_param(self, value, dialect):\n                raise nope", "documentation": "Hook for processing the bind_param for SQLAlchemy queries .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_65": {"code": "def test_exception_event_non_dbapi_error(self):\n        \"\"\"test that dbapi_error is called with a context in\n        cases where DBAPI raises an exception that is not a DBAPI\n        exception, e.g. internal errors or encoding problems.\n\n        \"\"\"\n        engine = engines.testing_engine()\n\n        listener = Mock(return_value=None)\n        event.listen(engine, 'handle_error', listener)\n\n        nope = TypeError(\"I'm not a DBAPI error\")\n        with engine.connect() as c:\n            c.connection.cursor = Mock(\n                    return_value=Mock(\n                        execute=Mock(\n                                side_effect=nope\n                        ))\n                    )\n\n            assert_raises_message(\n                TypeError,\n                \"I'm not a DBAPI error\",\n                c.execute, \"select \"\n            )\n        ctx = listener.mock_calls[0][1][0]\n        eq_(ctx.statement, \"select \")\n        is_(ctx.is_disconnect, False)\n        is_(ctx.original_exception, nope)", "documentation": "dbapi_error is called with a context in case of DBAPI errors", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_66": {"code": "def evt(ctx):\n            ctx.is_disconnect = evt_value", "documentation": "Fired when any event is fired", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_67": {"code": "def test_alter_disconnect_to_true(self):\n        self._test_alter_disconnect(False, True)\n        self._test_alter_disconnect(True, True)", "documentation": "Test to set disconnect to True", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_68": {"code": "def test_proxy(self):\n\n        stmts = []\n        cursor_stmts = []\n\n        class MyProxy(ConnectionProxy):\n            def execute(\n                self,\n                conn,\n                execute,\n                clauseelement,\n                *multiparams,\n                **params\n                ):\n                stmts.append((str(clauseelement), params, multiparams))\n                return execute(clauseelement, *multiparams, **params)\n\n            def cursor_execute(\n                self,\n                execute,\n                cursor,\n                statement,\n                parameters,\n                context,\n                executemany,\n                ):\n                cursor_stmts.append((str(statement), parameters, None))\n                return execute(cursor, statement, parameters, context)\n\n        def assert_stmts(expected, received):\n            for stmt, params, posn in expected:\n                if not received:\n                    assert False, \"Nothing available for stmt: %s\" % stmt\n                while received:\n                    teststmt, testparams, testmultiparams = \\\n                        received.pop(0)\n                    teststmt = re.compile(r'[\\n\\t ]+', re.M).sub(' ',\n                            teststmt).strip()\n                    if teststmt.startswith(stmt) and (testparams\n                            == params or testparams == posn):\n                        break\n\n        for engine in \\\n            engines.testing_engine(options=dict(implicit_returning=False,\n                                   proxy=MyProxy())), \\\n            engines.testing_engine(options=dict(implicit_returning=False,\n                                   proxy=MyProxy(),\n                                   strategy='threadlocal')):\n            m = MetaData(engine)\n            t1 = Table('t1', m,\n                Column('c1', Integer, primary_key=True),\n                Column('c2', String(50), default=func.lower('Foo'),\n                                            primary_key=True)\n            )\n            m.create_all()\n            try:\n                t1.insert().execute(c1=5, c2='some data')\n                t1.insert().execute(c1=6)\n                eq_(engine.execute('select * from t1').fetchall(), [(5,\n                    'some data'), (6, 'foo')])\n            finally:\n                m.drop_all()\n            engine.dispose()\n            compiled = [('CREATE TABLE t1', {}, None),\n                        ('INSERT INTO t1 (c1, c2)', {'c2': 'some data',\n                        'c1': 5}, None), ('INSERT INTO t1 (c1, c2)',\n                        {'c1': 6}, None), ('select * from t1', {},\n                        None), ('DROP TABLE t1', {}, None)]\n            if not testing.against('oracle+zxjdbc'):  # or engine.dialect.pr\n                                                      # eexecute_pk_sequence\n                                                      # s:\n                cursor = [\n                    ('CREATE TABLE t1', {}, ()),\n                    ('INSERT INTO t1 (c1, c2)', {'c2': 'some data', 'c1'\n                     : 5}, (5, 'some data')),\n                    ('SELECT lower', {'lower_2': 'Foo'},\n                        ('Foo', )),\n                    ('INSERT INTO t1 (c1, c2)', {'c2': 'foo', 'c1': 6},\n                     (6, 'foo')),\n                    ('select * from t1', {}, ()),\n                    ('DROP TABLE t1', {}, ()),\n                    ]\n            else:\n                insert2_params = 6, 'Foo'\n                if testing.against('oracle+zxjdbc'):\n                    insert2_params += (ReturningParam(12), )\n                cursor = [('CREATE TABLE t1', {}, ()),\n                          ('INSERT INTO t1 (c1, c2)', {'c2': 'some data'\n                          , 'c1': 5}, (5, 'some data')),\n                          ('INSERT INTO t1 (c1, c2)', {'c1': 6,\n                          'lower_2': 'Foo'}, insert2_params),\n                          ('select * from t1', {}, ()), ('DROP TABLE t1'\n                          , {}, ())]  # bind param name 'lower_2' might\n                                      # be incorrect\n            assert_stmts(compiled, stmts)\n            assert_stmts(cursor, cursor_stmts)", "documentation": "Test connection proxy in oracle", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_69": {"code": "def test_options(self):\n        canary = []\n        class TrackProxy(ConnectionProxy):\n            def __getattribute__(self, key):\n                fn = object.__getattribute__(self, key)\n                def go(*arg, **kw):\n                    canary.append(fn.__name__)\n                    return fn(*arg, **kw)\n                return go\n        engine = engines.testing_engine(options={'proxy':TrackProxy()})\n        conn = engine.connect()\n        c2 = conn.execution_options(foo='bar')\n        eq_(c2._execution_options, {'foo':'bar'})\n        c2.execute(select([1]))\n        c3 = c2.execution_options(bar='bat')\n        eq_(c3._execution_options, {'foo':'bar', 'bar':'bat'})\n        eq_(canary, ['execute', 'cursor_execute'])", "documentation": "Test execution options with proxy", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_70": {"code": "def test_transactional(self):\n        canary = []\n        class TrackProxy(ConnectionProxy):\n            def __getattribute__(self, key):\n                fn = object.__getattribute__(self, key)\n                def go(*arg, **kw):\n                    canary.append(fn.__name__)\n                    return fn(*arg, **kw)\n                return go\n\n        engine = engines.testing_engine(options={'proxy':TrackProxy()})\n        conn = engine.connect()\n        trans = conn.begin()\n        conn.execute(select([1]))\n        trans.rollback()\n        trans = conn.begin()\n        conn.execute(select([1]))\n        trans.commit()\n\n        eq_(canary, [\n            'begin', 'execute', 'cursor_execute', 'rollback',\n            'begin', 'execute', 'cursor_execute', 'commit',\n            ])", "documentation": "Test that the connection is in a transactional mode .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_71": {"code": "def test_transactional_advanced(self):\n        canary = []\n        class TrackProxy(ConnectionProxy):\n            def __getattribute__(self, key):\n                fn = object.__getattribute__(self, key)\n                def go(*arg, **kw):\n                    canary.append(fn.__name__)\n                    return fn(*arg, **kw)\n                return go\n\n        engine = engines.testing_engine(options={'proxy':TrackProxy()})\n        conn = engine.connect()\n\n        trans = conn.begin()\n        trans2 = conn.begin_nested()\n        conn.execute(select([1]))\n        trans2.rollback()\n        trans2 = conn.begin_nested()\n        conn.execute(select([1]))\n        trans2.commit()\n        trans.rollback()\n\n        trans = conn.begin_twophase()\n        conn.execute(select([1]))\n        trans.prepare()\n        trans.commit()\n\n        canary = [t for t in canary if t not in ('cursor_execute', 'execute')]\n        eq_(canary, ['begin', 'savepoint',\n                    'rollback_savepoint', 'savepoint', 'release_savepoint',\n                    'rollback', 'begin_twophase',\n                       'prepare_twophase', 'commit_twophase']\n        )", "documentation": "Advanced transaction test with savepoint and rollback .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_72": {"code": "def _run_test(self, retval):\n        m1 = Mock()\n\n        m1.do_execute.return_value = retval\n        m1.do_executemany.return_value = retval\n        m1.do_execute_no_params.return_value = retval\n        e = engines.testing_engine(options={\"_initialize\": False})\n\n        event.listen(e, \"do_execute\", m1.do_execute)\n        event.listen(e, \"do_executemany\", m1.do_executemany)\n        event.listen(e, \"do_execute_no_params\", m1.do_execute_no_params)\n\n        e.dialect.do_execute = m1.real_do_execute\n        e.dialect.do_executemany = m1.real_do_executemany\n        e.dialect.do_execute_no_params = m1.real_do_execute_no_params\n\n        def mock_the_cursor(cursor, *arg):\n            arg[-1].get_result_proxy = Mock(return_value=Mock(context=arg[-1]))\n            return retval\n\n        m1.real_do_execute.side_effect = m1.do_execute.side_effect = mock_the_cursor\n        m1.real_do_executemany.side_effect = m1.do_executemany.side_effect = mock_the_cursor\n        m1.real_do_execute_no_params.side_effect = m1.do_execute_no_params.side_effect = mock_the_cursor\n\n        with e.connect() as conn:\n            yield conn, m1", "documentation": "Run a single test with a connection to a database .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_73": {"code": "def _test_do_execute(self, retval):\n        with self._run_test(retval) as (conn, m1):\n            result = conn.execute(\"insert into table foo\", {\"foo\": \"bar\"})\n        self._assert(\n            retval,\n            m1.do_execute, m1.real_do_execute,\n            [call(\n                    result.context.cursor,\n                    \"insert into table foo\",\n                    {\"foo\": \"bar\"}, result.context)]\n        )", "documentation": "Test do_execute in the mocked connection .", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_74": {"code": "def _test_do_execute_no_params(self, retval):\n        with self._run_test(retval) as (conn, m1):\n            result = conn.execution_options(no_parameters=True).\\\n                execute(\"insert into table foo\")\n        self._assert(\n            retval,\n            m1.do_execute_no_params, m1.real_do_execute_no_params,\n            [call(\n                    result.context.cursor,\n                    \"insert into table foo\", result.context)]\n        )", "documentation": "Test the do_execute call with no parameters with the connection", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_75": {"code": "def test_do_execute_w_replace(self):\n        self._test_do_execute(True)", "documentation": "Test execute w replacing with replace", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_76": {"code": "def test_do_executemany_w_replace(self):\n        self._test_do_executemany(True)", "documentation": "Test for _test_do_executemany_w_replace", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_77": {"code": "def test_do_execute_no_params_w_replace(self):\n        self._test_do_execute_no_params(True)", "documentation": "Test for _test_do_execute_no_params_w_replace", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "michaelBenin/sqlalchemy_test/engine/test_execute.py_78": {"code": "def test_cursor_execute_w_replace(self):\n        self._test_cursor_execute(True)", "documentation": "Test cursor execute w_replace", "reputation": {"num_stars": 1, "num_forks": 0, "num_watchers": 1, "num_open_issues": 0, "created_at": 1404698852.0}}, "guori12321/todo_todo/parser.py_0": {"code": "def t_ID(self, t):\n        r'\\d+\\.([uU]|[lL]|[uU][lL]|[lL][uU])?'\n        t.value = int(t.value[:-1])\n        return t", "documentation": "\\ . \\d+\\ .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "guori12321/todo_todo/parser.py_1": {"code": "def t_TASK(self, t):\n        r'((?!\\(x\\))).+'\n        return t", "documentation": "r Avoid regex errors in Python 3 .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "guori12321/todo_todo/parser.py_2": {"code": "def t_error(self, t):\n        raise SyntaxError(\n            \"Illegal character: '%s' at Line %d\" % (t.value[0], t.lineno)\n        )", "documentation": "Raise SyntaxError after encountering illegal character in token t .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "guori12321/todo_todo/parser.py_3": {"code": "def p_error(self, p):\n        if p:\n            raise SyntaxError(\n                \"Character '%s' at line %d\" % (p.value[0], p.lineno)\n            )\n        else:\n            raise SyntaxError(\"SyntaxError at EOF\")", "documentation": "Raise SyntaxError at EOF", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "guori12321/todo_todo/parser.py_4": {"code": "def p_translation_unit(self, p):\n        \"\"\"\n        translation_unit : translate_task\n                         | translation_unit translate_task\n                         |\n        \"\"\"\n        pass", "documentation": "translation_unit translate_task", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "guori12321/todo_todo/parser.py_5": {"code": "def __init__(self):\n        self.parser = yacc.yacc(module=self, debug=0, write_tables=0)", "documentation": "Initialize the parser", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "alisaifee/limits_tests/storage/test_memcached.py_0": {"code": "def setup(self, memcached, memcached_cluster):\n        self.storage_url = \"memcached://localhost:22122\"", "documentation": "Setup Memcached class for storing memcached data", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "alisaifee/limits_tests/storage/test_memcached.py_1": {"code": "def test_fixed_window(self):\n        storage = MemcachedStorage(\"memcached://localhost:22122\")\n        limiter = FixedWindowRateLimiter(storage)\n        per_min = RateLimitItemPerSecond(10)\n        start = time.time()\n        count = 0\n\n        while time.time() - start < 0.5 and count < 10:\n            assert limiter.hit(per_min)\n            count += 1\n        assert not limiter.hit(per_min)\n\n        while time.time() - start <= 1:\n            time.sleep(0.1)\n        assert limiter.hit(per_min)", "documentation": "Test that we can make a rate limiter behave as if it were fixed .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "alisaifee/limits_tests/storage/test_memcached.py_2": {"code": "def test_fixed_window_cluster(self):\n        storage = MemcachedStorage(\"memcached://localhost:22122,localhost:22123\")\n        limiter = FixedWindowRateLimiter(storage)\n        per_min = RateLimitItemPerSecond(10)\n        start = time.time()\n        count = 0\n\n        while time.time() - start < 0.5 and count < 10:\n            assert limiter.hit(per_min)\n            count += 1\n        assert not limiter.hit(per_min)\n\n        while time.time() - start <= 1:\n            time.sleep(0.1)\n        assert limiter.hit(per_min)", "documentation": "Test that we can make a request with a fixed rate limiter", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "alisaifee/limits_tests/storage/test_memcached.py_3": {"code": "def test_fixed_window_with_elastic_expiry(self):\n        storage = MemcachedStorage(\"memcached://localhost:22122\")\n        limiter = FixedWindowElasticExpiryRateLimiter(storage)\n        per_sec = RateLimitItemPerSecond(2, 2)\n\n        assert limiter.hit(per_sec)\n        time.sleep(1)\n        assert limiter.hit(per_sec)\n        assert not limiter.test(per_sec)\n        time.sleep(1)\n        assert not limiter.test(per_sec)\n        time.sleep(1)\n        assert limiter.test(per_sec)", "documentation": "Test that we can set up an elastic expiration limiter with a fixed window and an elastic expiry", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "alisaifee/limits_tests/storage/test_memcached.py_4": {"code": "def test_fixed_window_with_elastic_expiry_cluster(self):\n        storage = MemcachedStorage(\"memcached://localhost:22122,localhost:22123\")\n        limiter = FixedWindowElasticExpiryRateLimiter(storage)\n        per_sec = RateLimitItemPerSecond(2, 2)\n\n        assert limiter.hit(per_sec)\n        time.sleep(1)\n        assert limiter.hit(per_sec)\n        assert not limiter.test(per_sec)\n        time.sleep(1)\n        assert not limiter.test(per_sec)\n        time.sleep(1)\n        assert limiter.test(per_sec)", "documentation": "Test that we can create a rate limiter with an elastic expiry using a fixed window .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "ff0000/red-fab-deploy_fab_deploy/joyent/postgres.py_0": {"code": "def _get_data_dir(self, db_version):\n        # Try to get from svc first\n        output = run('svcprop -p config/data postgresql')\n        if output.stdout and exists(output.stdout, use_sudo=True):\n            return output.stdout\n        return base_postgres.PostgresInstall._get_data_dir(self, db_version)", "documentation": "Get the data dir for the given db_version", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "ff0000/red-fab-deploy_fab_deploy/joyent/postgres.py_1": {"code": "def _restart_db_server(self, db_version):\n        sudo('svcadm restart postgresql')", "documentation": "Restart the database server", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "ff0000/red-fab-deploy_fab_deploy/joyent/postgres.py_2": {"code": "def _start_db_server(self, db_version):\n        sudo('svcadm enable postgresql')", "documentation": "Starts a db server", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "ff0000/red-fab-deploy_fab_deploy/joyent/postgres.py_3": {"code": "def install_package(self):\n        sudo('pkg_add libevent')\n        with cd('/tmp'):\n            run('wget %s' %self.pgbouncer_src)\n            sudo('pkg_add %s' %self.pkg_name)", "documentation": "install the package", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "ff0000/red-fab-deploy_fab_deploy/joyent/postgres.py_4": {"code": "def _get_passwd(self, username):\n        with hide('output'):\n            string = run('echo \"select usename, passwd from pg_shadow where '\n                         'usename=\\'%s\\' order by 1\" | sudo su postgres -c '\n                         '\"psql\"' %username)\n\n        user, passwd = string.split('\\n')[2].split('|')\n        user = user.strip()\n        passwd = passwd.strip()\n\n        __, tmp_name = tempfile.mkstemp()\n        fn = open(tmp_name, 'w')\n        fn.write('\"%s\" \"%s\" \"\"\\n' %(user, passwd))\n        fn.close()\n        put(tmp_name, '%s/pgbouncer.userlist'%self.config_dir, use_sudo=True)\n        local('rm %s' %tmp_name)", "documentation": "Get username passwd for a user", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "ff0000/red-fab-deploy_fab_deploy/joyent/postgres.py_5": {"code": "def run(self, section=None):\n        \"\"\"\n        \"\"\"\n\n        sudo('mkdir -p /opt/pkg/bin')\n        sudo(\"ln -sf /opt/local/bin/awk /opt/pkg/bin/nawk\")\n        sudo(\"ln -sf /opt/local/bin/sed /opt/pkg/bin/nbsed\")\n\n        self.install_package()\n\n        svc_method = os.path.join(env.configs_dir, 'pgbouncer.xml')\n        put(svc_method, self.config_dir, use_sudo=True)\n\n        home = run('bash -c \"echo ~postgres\"')\n        bounce_home = os.path.join(home, 'pgbouncer')\n\n        pidfile = os.path.join(bounce_home, 'pgbouncer.pid')\n        self._setup_parameter('%s/pgbouncer.ini' %self.config_dir,\n                              pidfile=pidfile, **self.config)\n\n        if not section:\n            section = 'db-server'\n        username = self._get_username(section)\n        self._get_passwd(username)\n        # postgres should be the owner of these config files\n        sudo('chown -R postgres:postgres %s' %self.config_dir)\n\n        sudo('mkdir -p %s' % bounce_home)\n        sudo('chown postgres:postgres %s' % bounce_home)\n\n        sudo('mkdir -p /var/log/pgbouncer')\n        sudo('chown postgres:postgres /var/log/pgbouncer')\n\n        # set up log\n        sudo('logadm -C 3 -p1d -c -w /var/log/pgbouncer/pgbouncer.log -z 1')\n        run('svccfg import %s/pgbouncer.xml' %self.config_dir)\n\n        # start pgbouncer\n        sudo('svcadm enable pgbouncer')", "documentation": "install the bouncer service", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "chriso/gauged_gauged/drivers/__init__.py_0": {"code": "def parse_dsn(dsn_string):\n    \"\"\"Parse a connection string and return the associated driver\"\"\"\n    dsn = urlparse(dsn_string)\n    scheme = dsn.scheme.split('+')[0]\n    username = password = host = port = None\n    host = dsn.netloc\n    if '@' in host:\n        username, host = host.split('@')\n        if ':' in username:\n            username, password = username.split(':')\n            password = unquote(password)\n        username = unquote(username)\n    if ':' in host:\n        host, port = host.split(':')\n        port = int(port)\n    database = dsn.path.split('?')[0][1:]\n    query = dsn.path.split('?')[1] if '?' in dsn.path else dsn.query\n    kwargs = dict(parse_qsl(query, True))\n    if scheme == 'sqlite':\n        return SQLiteDriver, [dsn.path], {}\n    elif scheme == 'mysql':\n        kwargs['user'] = username or 'root'\n        kwargs['db'] = database\n        if port:\n            kwargs['port'] = port\n        if host:\n            kwargs['host'] = host\n        if password:\n            kwargs['passwd'] = password\n        return MySQLDriver, [], kwargs\n    elif scheme == 'postgresql':\n        kwargs['user'] = username or 'postgres'\n        kwargs['database'] = database\n        if port:\n            kwargs['port'] = port\n        if 'unix_socket' in kwargs:\n            kwargs['host'] = kwargs.pop('unix_socket')\n        elif host:\n            kwargs['host'] = host\n        if password:\n            kwargs['password'] = password\n        return PostgreSQLDriver, [], kwargs\n    else:\n        raise ValueError('Unknown driver %s' % dsn_string)", "documentation": "Parse a connection string and return the associated driver", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "laterpay/rubberjack-cli_tests/test_cli.py_0": {"code": "def test_deploy(self, cav, ue):\n        s3 = boto.connect_s3()\n        s3.create_bucket(\"laterpay-rubberjack-ebdeploy\")  # FIXME Remove hardcoded bucket name\n\n        with tempfile.NamedTemporaryFile() as tmp:\n            result = CliRunner().invoke(rubberjack, ['deploy', tmp.name], catch_exceptions=False)\n\n            self.assertEquals(result.exit_code, 0, result.output)", "documentation": "Deploy the bucket we just deployed and verify the command succeeds", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "laterpay/rubberjack-cli_tests/test_cli.py_1": {"code": "def test_promote(self, ue, de):\n        de.return_value = {\n            'DescribeEnvironmentsResponse': {\n                'DescribeEnvironmentsResult': {\n                    'Environments': [\n                        {\n                            'EnvironmentName': 'laterpay-devnull-live',  # FIXME Remove hardcoded EnvName\n                            'VersionLabel': 'old',\n                        },\n                        {\n                            'EnvironmentName': 'laterpay-devnull-dev',  # FIXME Remove hardcoded EnvName\n                            'VersionLabel': 'new',\n                        },\n                    ],\n                },\n            },\n        }\n\n        CliRunner().invoke(rubberjack, ['promote'], catch_exceptions=False)", "documentation": "Test promote - it will promote the environment to the next release", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "laterpay/rubberjack-cli_tests/test_cli.py_2": {"code": "def test_promoting_same_version(self, ue, de, se):\n        de.return_value = {\n            'DescribeEnvironmentsResponse': {\n                'DescribeEnvironmentsResult': {\n                    'Environments': [\n                        {\n                            'EnvironmentName': 'laterpay-devnull-live',  # FIXME Remove hardcoded EnvName\n                            'VersionLabel': 'same',\n                        },\n                        {\n                            'EnvironmentName': 'laterpay-devnull-dev',  # FIXME Remove hardcoded EnvName\n                            'VersionLabel': 'same',\n                        },\n                    ],\n                },\n            },\n        }\n\n        CliRunner().invoke(rubberjack, ['promote'], catch_exceptions=False)\n\n        self.assertTrue(se.called)", "documentation": "Test promoteing same version of environment", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "laterpay/rubberjack-cli_tests/test_cli.py_3": {"code": "def test_sigv4(self):\n        CliRunner().invoke(rubberjack, ['--sigv4-host', 'foo', 'deploy'], catch_exceptions=False)", "documentation": "Use sigv4 to deploy your project to the server", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "laterpay/rubberjack-cli_tests/test_cli.py_4": {"code": "def test_deploy_to_custom_environment(self, ue, cav):\n        s3 = boto.connect_s3()\n        s3.create_bucket(\"laterpay-rubberjack-ebdeploy\")  # FIXME Remove hardcoded bucket name\n\n        with tempfile.NamedTemporaryFile() as tmp:\n            result = CliRunner().invoke(rubberjack, ['deploy', '--environment', 'wibble', tmp.name], catch_exceptions=False)\n\n            self.assertEquals(result.exit_code, 0, result.output)\n\n        self.assertEqual(cav.call_count, 1, \"create_application_version wasn't called, but it should\")\n        self.assertEqual(ue.call_count, 1, \"update_environment wasn't called, but it should\")", "documentation": "Deploy to a custom environment .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "laterpay/rubberjack-cli_tests/test_cli.py_5": {"code": "def test_deploy_without_updating_the_environment(self, ue, cav):\n        s3 = boto.connect_s3()\n        s3.create_bucket(\"laterpay-rubberjack-ebdeploy\")  # FIXME Remove hardcoded bucket name\n\n        with tempfile.NamedTemporaryFile() as tmp:\n            result = CliRunner().invoke(rubberjack, ['deploy', '--no-update-environment', tmp.name], catch_exceptions=False)\n\n            self.assertEquals(result.exit_code, 0, result.output)\n\n        self.assertEqual(cav.call_count, 1, \"create_application_version wasn't called, but it should\")\n        self.assertEqual(ue.call_count, 0, \"update_environment was called, but it shouldn't\")", "documentation": "Deploy to the bucket with laterpay - rubberjack - ebdeploy - no", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "laterpay/rubberjack-cli_tests/test_cli.py_6": {"code": "def test_deploy_to_custom_bucket(self, ue, cav):\n        bucket_name = 'rbbrjck-test'\n        s3 = boto.connect_s3()\n        s3.create_bucket(bucket_name)\n\n        with tempfile.NamedTemporaryFile() as tmp:\n            result = CliRunner().invoke(rubberjack, ['--bucket', bucket_name, 'deploy', tmp.name], catch_exceptions=False)\n\n            self.assertEquals(result.exit_code, 0, result.output)\n\n        self.assertEqual(cav.call_count, 1, \"create_application_version wasn't called, but it should\")\n        self.assertEqual(ue.call_count, 1, \"update_environment wasn't called, but it should\")\n\n        _, cav_kwargs = cav.call_args\n        self.assertEqual(bucket_name, cav_kwargs['s3_bucket'])", "documentation": "Deploys the app to a custom S3 bucket .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_0": {"code": "def tobytes(obj):\n        if isinstance(obj, str):\n            obj = obj.encode('UTF-8')\n        assert isinstance(obj, bytes)\n        return obj", "documentation": "Convert an object to bytes", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_1": {"code": "def tobytes(obj):\n        if isinstance(obj, unicode):\n            obj = obj.encode('UTF-8')\n        assert isinstance(obj, str)\n        return obj", "documentation": "Convert object to bytes", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_2": {"code": "def oswritebytes(fd, obj):\n    os.write(fd, tobytes(obj))", "documentation": "Write bytes to a file descriptor .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_3": {"code": "def StdCapture(out=True, err=True, in_=True):\n    return capture.MultiCapture(out, err, in_, Capture=capture.SysCapture)", "documentation": "MultiCapture wrapper for sys . stdout .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_4": {"code": "def test_getmethod_default_no_fd(self, monkeypatch):\n        from _pytest.capture import pytest_addoption\n        from _pytest.config import Parser\n        parser = Parser()\n        pytest_addoption(parser)\n        default = parser._groups[0].options[0].default\n        assert default == \"fd\" if hasattr(os, \"dup\") else \"sys\"\n        parser = Parser()\n        monkeypatch.delattr(os, 'dup', raising=False)\n        pytest_addoption(parser)\n        assert parser._groups[0].options[0].default == \"sys\"", "documentation": "Test getmethod default when no file descriptor specified", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_5": {"code": "def test_capturing_basic_api(self, method):\n        capouter = StdCaptureFD()\n        old = sys.stdout, sys.stderr, sys.stdin\n        try:\n            capman = CaptureManager(method)\n            capman.start_global_capturing()\n            outerr = capman.suspend_global_capture()\n            assert outerr == (\"\", \"\")\n            outerr = capman.suspend_global_capture()\n            assert outerr == (\"\", \"\")\n            print(\"hello\")\n            out, err = capman.suspend_global_capture()\n            if method == \"no\":\n                assert old == (sys.stdout, sys.stderr, sys.stdin)\n            else:\n                assert not out\n            capman.resume_global_capture()\n            print(\"hello\")\n            out, err = capman.suspend_global_capture()\n            if method != \"no\":\n                assert out == \"hello\\n\"\n            capman.stop_global_capturing()\n        finally:\n            capouter.stop_capturing()", "documentation": "Test that we can resume and suspend global capturing", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_6": {"code": "def test_init_capturing(self):\n        capouter = StdCaptureFD()\n        try:\n            capman = CaptureManager(\"fd\")\n            capman.start_global_capturing()\n            pytest.raises(AssertionError, \"capman.start_global_capturing()\")\n            capman.stop_global_capturing()\n        finally:\n            capouter.stop_capturing()", "documentation": "Start global capturing and then stop global capturing", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_7": {"code": "def test_capturing_unicode(testdir, method):\n    if hasattr(sys, \"pypy_version_info\") and sys.pypy_version_info < (2, 2):\n        pytest.xfail(\"does not work on pypy < 2.2\")\n    if sys.version_info >= (3, 0):\n        obj = \"'b\\u00f6y'\"\n    else:\n        obj = \"u'\\u00f6y'\"\n    testdir.makepyfile(\"\"\"\n        # coding=utf8\n        # taken from issue 227 from nosetests\n        def test_unicode():\n            import sys\n            print (sys.stdout)\n            print (%s)\n    \"\"\" % obj)\n    result = testdir.runpytest(\"--capture=%s\" % method)\n    result.stdout.fnmatch_lines([\n        \"*1 passed*\"\n    ])", "documentation": "test unicode encoding", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_8": {"code": "def test_capturing_bytes_in_utf8_encoding(testdir, method):\n    testdir.makepyfile(\"\"\"\n        def test_unicode():\n            print ('b\\\\u00f6y')\n    \"\"\")\n    result = testdir.runpytest(\"--capture=%s\" % method)\n    result.stdout.fnmatch_lines([\n        \"*1 passed*\"\n    ])", "documentation": "Make sure that tests with the python - C code in unicode are working properly", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_9": {"code": "def test_capture_and_fixtures(self, testdir):\n        p = testdir.makepyfile(\"\"\"\n            def setup_module(mod):\n                print (\"setup module\")\n            def setup_function(function):\n                print (\"setup \" + function.__name__)\n            def test_func1():\n                print (\"in func1\")\n                assert 0\n            def test_func2():\n                print (\"in func2\")\n                assert 0\n        \"\"\")\n        result = testdir.runpytest(p)\n        result.stdout.fnmatch_lines([\n            \"setup module*\",\n            \"setup test_func1*\",\n            \"in func1*\",\n            \"setup test_func2*\",\n            \"in func2*\",\n        ])", "documentation": "Makes a pytest that captures and fixtures the modules and tests for capture", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_10": {"code": "def test_capture_scope_cache(self, testdir):\n        p = testdir.makepyfile(\"\"\"\n            import sys\n            def setup_module(func):\n                print (\"module-setup\")\n            def setup_function(func):\n                print (\"function-setup\")\n            def test_func():\n                print (\"in function\")\n                assert 0\n            def teardown_function(func):\n                print (\"in teardown\")\n        \"\"\")\n        result = testdir.runpytest(p)\n        result.stdout.fnmatch_lines([\n            \"*test_func():*\",\n            \"*Captured stdout during setup*\",\n            \"module-setup*\",\n            \"function-setup*\",\n            \"*Captured stdout*\",\n            \"in teardown*\",\n        ])", "documentation": "Test that the capture scope cache is correctly cached on Python 3 .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_11": {"code": "def test_func1():\n                print (\"in func1\")", "documentation": "test for the in operator", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_12": {"code": "def test_teardown_capturing(self, testdir):\n        p = testdir.makepyfile(\"\"\"\n            def setup_function(function):\n                print (\"setup func1\")\n            def teardown_function(function):\n                print (\"teardown func1\")\n                assert 0\n            def test_func1():\n                print (\"in func1\")\n                pass\n        \"\"\")\n        result = testdir.runpytest(p)\n        result.stdout.fnmatch_lines([\n            '*teardown_function*',\n            '*Captured stdout*',\n            \"setup func1*\",\n            \"in func1*\",\n            \"teardown func1*\",\n            # \"*1 fixture failure*\"\n        ])", "documentation": "Test that functions with capturing stdout are properly captured", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_13": {"code": "def teardown_module(mod):\n                print (\"teardown module\")\n                assert 0", "documentation": "teardown a python module", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_14": {"code": "def test_capturing_outerr(self, testdir):\n        p1 = testdir.makepyfile(\"\"\"\n            import sys\n            def test_capturing():\n                print (42)\n                sys.stderr.write(str(23))\n            def test_capturing_error():\n                print (1)\n                sys.stderr.write(str(2))\n                raise ValueError\n        \"\"\")\n        result = testdir.runpytest(p1)\n        result.stdout.fnmatch_lines([\n            \"*test_capturing_outerr.py .F*\",\n            \"====* FAILURES *====\",\n            \"____*____\",\n            \"*test_capturing_outerr.py:8: ValueError\",\n            \"*--- Captured stdout *call*\",\n            \"1\",\n            \"*--- Captured stderr *call*\",\n            \"2\",\n        ])", "documentation": "This function tests a bug where capturing output and error output are not the same .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_15": {"code": "def test_logging_stream_ownership(self, testdir):\n        p = testdir.makepyfile(\"\"\"\n            def test_logging():\n                import logging\n                import pytest\n                stream = capture.CaptureIO()\n                logging.basicConfig(stream=stream)\n                stream.close() # to free memory/release resources\n        \"\"\")\n        result = testdir.runpytest_subprocess(p)\n        assert result.stderr.str().find(\"atexit\") == -1", "documentation": "Make sure logging stream ownership is checked when execution is done", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_16": {"code": "def setup_function(function):\n                logging.warn(\"hello1\")", "documentation": "Called after the test function is run", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_17": {"code": "def teardown_function(function):\n                logging.warn(\"hello3\")\n                assert 0", "documentation": "Teardown a function from a decorator .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_18": {"code": "def test_logging_and_crossscope_fixtures(self, testdir):\n        p = testdir.makepyfile(\"\"\"\n            import logging\n            def setup_module(function):\n                logging.warn(\"hello1\")\n\n            def test_logging():\n                logging.warn(\"hello2\")\n                assert 0\n\n            def teardown_module(function):\n                logging.warn(\"hello3\")\n                assert 0\n        \"\"\")\n        for optargs in (('--capture=sys',), ('--capture=fd',)):\n            print(optargs)\n            result = testdir.runpytest_subprocess(p, *optargs)\n            s = result.stdout.str()\n            result.stdout.fnmatch_lines([\n                \"*WARN*hello3\",  # errors come first\n                \"*WARN*hello1\",\n                \"*WARN*hello2\",\n            ])\n            # verify proper termination\n            assert \"closed\" not in s", "documentation": "Run pytest with both logging and crossscope fixtures", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_19": {"code": "def test_conftestlogging_and_test_logging(self, testdir):\n        testdir.makeconftest(\"\"\"\n                import logging\n                logging.basicConfig()\n        \"\"\")\n        # make sure that logging is still captured in tests\n        p = testdir.makepyfile(\"\"\"\n            def test_hello():\n                import logging\n                logging.warn(\"hello433\")\n                assert 0\n        \"\"\")\n        result = testdir.runpytest_subprocess(p, \"-p\", \"no:capturelog\")\n        assert result.ret != 0\n        result.stdout.fnmatch_lines([\n            \"WARNING*hello433*\",\n        ])\n        assert 'something' not in result.stderr.str()\n        assert 'operation on closed file' not in result.stderr.str()", "documentation": "Make sure conftest and test logging are captured in tests .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_20": {"code": "def test_std_functional(self, testdir, opt):\n        reprec = testdir.inline_runsource(\"\"\"\n            def test_hello(capsys):\n                print (42)\n                out, err = capsys.readouterr()\n                assert out.startswith(\"42\")\n        \"\"\", *opt)\n        reprec.assertoutcome(passed=1)", "documentation": "Test that the test_hello functional can be called from the command line", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_21": {"code": "def test_one(capsys, capfd):\n                pass", "documentation": "Implement the same interface as unittest . TestCase", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_22": {"code": "def test_capturing_getfixturevalue(self, testdir):\n        \"\"\"Test that asking for \"capfd\" and \"capsys\" using request.getfixturevalue\n        in the same test is an error.\n        \"\"\"\n        testdir.makepyfile(\"\"\"\n            def test_one(capsys, request):\n                request.getfixturevalue(\"capfd\")\n            def test_two(capfd, request):\n                request.getfixturevalue(\"capsys\")\n        \"\"\")\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\n            \"*test_one*\",\n            \"*capsys*capfd*same*time*\",\n            \"*test_two*\",\n            \"*capfd*capsys*same*time*\",\n            \"*2 failed in*\",\n        ])", "documentation": "Test that asking for capfd and capsys using request . getfixturevalue works as expected", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_23": {"code": "def test_one(capsys, capfdbinary):\n                pass", "documentation": "Check if the capability test passes", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_24": {"code": "def test_capture_is_represented_on_failure_issue128(self, testdir, method):\n        p = testdir.makepyfile(\"\"\"\n            def test_hello(cap%s):\n                print (\"xxx42xxx\")\n                assert 0\n        \"\"\" % method)\n        result = testdir.runpytest(p)\n        result.stdout.fnmatch_lines([\n            \"xxx42xxx\",\n        ])", "documentation": "This test is used to see if the capture is present on failure .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_25": {"code": "def test_stdfd_functional(self, testdir):\n        reprec = testdir.inline_runsource(\"\"\"\n            def test_hello(capfd):\n                import os\n                os.write(1, \"42\".encode('ascii'))\n                out, err = capfd.readouterr()\n                assert out.startswith(\"42\")\n                capfd.close()\n        \"\"\")\n        reprec.assertoutcome(passed=1)", "documentation": "Make sure that tests run in a functional way", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_26": {"code": "def test_capfdbinary(self, testdir):\n        reprec = testdir.inline_runsource(\"\"\"\n            def test_hello(capfdbinary):\n                import os\n                # some likely un-decodable bytes\n                os.write(1, b'\\\\xfe\\\\x98\\\\x20')\n                out, err = capfdbinary.readouterr()\n                assert out == b'\\\\xfe\\\\x98\\\\x20'\n                assert err == b''\n        \"\"\")\n        reprec.assertoutcome(passed=1)", "documentation": "Test that the capfdbinary Python module can read and write data", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_27": {"code": "def test_capsysbinary(self, testdir):\n        reprec = testdir.inline_runsource(\"\"\"\n            def test_hello(capsysbinary):\n                import sys\n                # some likely un-decodable bytes\n                sys.stdout.buffer.write(b'\\\\xfe\\\\x98\\\\x20')\n                out, err = capsysbinary.readouterr()\n                assert out == b'\\\\xfe\\\\x98\\\\x20'\n                assert err == b''\n        \"\"\")\n        reprec.assertoutcome(passed=1)", "documentation": "Test that the capsysbinary binary is properly installed", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_28": {"code": "def test_capsysbinary_forbidden_in_python2(self, testdir):\n        testdir.makepyfile(\"\"\"\n            def test_hello(capsysbinary):\n                pass\n        \"\"\")\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\n            \"*test_hello*\",\n            \"*capsysbinary is only supported on python 3*\",\n            \"*1 error in*\",\n        ])", "documentation": "Test that capsysbinary is forbidden in python 2", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_29": {"code": "def test_hello(capsys, missingarg):\n                pass", "documentation": "Test that the capability is available only when the command - line argument is missing", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_30": {"code": "def test_keyboardinterrupt_disables_capturing(self, testdir):\n        p = testdir.makepyfile(\"\"\"\n            def test_hello(capfd):\n                import os\n                os.write(1, str(42).encode('ascii'))\n                raise KeyboardInterrupt()\n        \"\"\")\n        result = testdir.runpytest_subprocess(p)\n        result.stdout.fnmatch_lines([\n            \"*KeyboardInterrupt*\"\n        ])\n        assert result.ret == 2", "documentation": "Test that a Python program disables the usage of capturing keyboard interrupts .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_31": {"code": "def test_capture_and_logging(self, testdir):\n        p = testdir.makepyfile(\"\"\"\n            import logging\n            def test_log(capsys):\n                logging.error('x')\n            \"\"\")\n        result = testdir.runpytest_subprocess(p)\n        assert 'closed' not in result.stderr.str()", "documentation": "Capture and logging with Python3 and Python2 .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_32": {"code": "def test_disabled_capture_fixture(self, testdir, fixture, no_capture):\n        testdir.makepyfile(\"\"\"\n            def test_disabled({fixture}):\n                print('captured before')\n                with {fixture}.disabled():\n                    print('while capture is disabled')\n                print('captured after')\n                assert {fixture}.readouterr() == ('captured before\\\\ncaptured after\\\\n', '')\n\n            def test_normal():\n                print('test_normal executed')\n        \"\"\".format(fixture=fixture))\n        args = ('-s',) if no_capture else ()\n        result = testdir.runpytest_subprocess(*args)\n        result.stdout.fnmatch_lines(\"\"\"\n            *while capture is disabled*\n        \"\"\")\n        assert 'captured before' not in result.stdout.str()\n        assert 'captured after' not in result.stdout.str()\n        if no_capture:\n            assert 'test_normal executed' in result.stdout.str()\n        else:\n            assert 'test_normal executed' not in result.stdout.str()", "documentation": "Test that a fixture is set when capture is disabled", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_33": {"code": "def test_fixture_use_by_other_fixtures(self, testdir, fixture):\n        \"\"\"\n        Ensure that capsys and capfd can be used by other fixtures during setup and teardown.\n        \"\"\"\n        testdir.makepyfile(\"\"\"\n            from __future__ import print_function\n            import sys\n            import pytest\n\n            @pytest.fixture\n            def captured_print({fixture}):\n                print('stdout contents begin')\n                print('stderr contents begin', file=sys.stderr)\n                out, err = {fixture}.readouterr()\n\n                yield out, err\n\n                print('stdout contents end')\n                print('stderr contents end', file=sys.stderr)\n                out, err = {fixture}.readouterr()\n                assert out == 'stdout contents end\\\\n'\n                assert err == 'stderr contents end\\\\n'\n\n            def test_captured_print(captured_print):\n                out, err = captured_print\n                assert out == 'stdout contents begin\\\\n'\n                assert err == 'stderr contents begin\\\\n'\n        \"\"\".format(fixture=fixture))\n        result = testdir.runpytest_subprocess()\n        result.stdout.fnmatch_lines(\"*1 passed*\")\n        assert 'stdout contents begin' not in result.stdout.str()\n        assert 'stderr contents begin' not in result.stdout.str()", "documentation": "Test that capsys and capfd are not used by other fixtures when setup and teardown", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_34": {"code": "def pytest_runtest_setup(item):\n            raise ValueError(42)", "documentation": "Pending pytest imports not implemented", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_35": {"code": "def test_fdfuncarg_skips_on_no_osdup(testdir):\n    testdir.makepyfile(\"\"\"\n        import os\n        if hasattr(os, 'dup'):\n            del os.dup\n        def test_hello(capfd):\n            pass\n    \"\"\")\n    result = testdir.runpytest_subprocess(\"--capture=no\")\n    result.stdout.fnmatch_lines([\n        \"*1 skipped*\"\n    ])", "documentation": "Run tests with a combination of fdfuncarg and osdup .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_36": {"code": "def pytest_runtest_setup():\n            print (\"hello19\")", "documentation": "Epy test setup for E pytest .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_37": {"code": "def test_capture_badoutput_issue412(testdir):\n    testdir.makepyfile(\"\"\"\n        import os\n\n        def test_func():\n            omg = bytearray([1,129,1])\n            os.write(1, omg)\n            assert 0\n        \"\"\")\n    result = testdir.runpytest('--cap=fd')\n    result.stdout.fnmatch_lines('''\n        *def test_func*\n        *assert 0*\n        *Captured*\n        *1 failed*\n    ''')", "documentation": "Test that capture_badoutput_issue412 works in python 3 .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_38": {"code": "def pytest_runtest_setup():\n            print (\"hello19\")", "documentation": "Epy test setup for E pytest .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_39": {"code": "def test_capture_binary_output(testdir):\n    testdir.makepyfile(r\"\"\"\n        import pytest\n\n        def test_a():\n            import sys\n            import subprocess\n            subprocess.call([sys.executable, __file__])\n\n        def test_foo():\n            import os;os.write(1, b'\\xc3')\n\n        if __name__ == '__main__':\n            test_foo()\n        \"\"\")\n    result = testdir.runpytest('--assert=plain')\n    result.assert_outcomes(passed=2)", "documentation": "Run tests with capture binary output", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_40": {"code": "def bad_snap(self):\n            raise Exception('boom')", "documentation": "Snaps are not supported yet .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_41": {"code": "def test_text(self):\n        f = capture.CaptureIO()\n        f.write(\"hello\")\n        s = f.getvalue()\n        assert s == \"hello\"\n        f.close()", "documentation": "Text test for captureioio .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_42": {"code": "def test_write_bytes_to_buffer(self):\n        \"\"\"In python3, stdout / stderr are text io wrappers (exposing a buffer\n        property of the underlying bytestream).  See issue #1407\n        \"\"\"\n        f = capture.CaptureIO()\n        f.buffer.write(b'foo\\r\\n')\n        assert f.getvalue() == 'foo\\r\\n'", "documentation": "In python3 stdout & stderr are text io wrappers", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_43": {"code": "def test_dontreadfrominput():\n    from _pytest.capture import DontReadFromInput\n    f = DontReadFromInput()\n    assert not f.isatty()\n    pytest.raises(IOError, f.read)\n    pytest.raises(IOError, f.readlines)\n    pytest.raises(IOError, iter, f)\n    pytest.raises(UnsupportedOperation, f.fileno)\n    f.close()  # just for completeness", "documentation": "Test that we don't read from stdin when not using python3", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_44": {"code": "def test_dontreadfrominput_buffer_python3():\n    from _pytest.capture import DontReadFromInput\n    f = DontReadFromInput()\n    fb = f.buffer\n    assert not fb.isatty()\n    pytest.raises(IOError, fb.read)\n    pytest.raises(IOError, fb.readlines)\n    pytest.raises(IOError, iter, fb)\n    pytest.raises(ValueError, fb.fileno)\n    f.close()  # just for completeness", "documentation": "Python 3 doesn t allow reading from input buffer .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_45": {"code": "def test_dontreadfrominput_buffer_python2():\n    from _pytest.capture import DontReadFromInput\n    f = DontReadFromInput()\n    with pytest.raises(AttributeError):\n        f.buffer\n    f.close()  # just for completeness", "documentation": "Regression test for HARKY-5017", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_46": {"code": "def tmpfile(testdir):\n    f = testdir.makepyfile(\"\").open('wb+')\n    yield f\n    if not f.closed:\n        f.close()", "documentation": "Create a temporary Python file .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_47": {"code": "def test_dupfile(tmpfile):\n    flist = []\n    for i in range(5):\n        nf = capture.safe_text_dupfile(tmpfile, \"wb\")\n        assert nf != tmpfile\n        assert nf.fileno() != tmpfile.fileno()\n        assert nf not in flist\n        print(i, end=\"\", file=nf)\n        flist.append(nf)\n\n    fname_open = flist[0].name\n    assert fname_open == repr(flist[0].buffer)\n\n    for i in range(5):\n        f = flist[i]\n        f.close()\n    fname_closed = flist[0].name\n    assert fname_closed == repr(flist[0].buffer)\n    assert fname_closed != fname_open\n    tmpfile.seek(0)\n    s = tmpfile.read()\n    assert \"01234\" in repr(s)\n    tmpfile.close()\n    assert fname_closed == repr(flist[0].buffer)", "documentation": "Check that a file is not duped by more than 5 lines .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_48": {"code": "def test_dupfile_on_textio():\n    io = py.io.TextIO()\n    f = capture.safe_text_dupfile(io, \"wb\")\n    f.write(\"hello\")\n    assert io.getvalue() == \"hello\"\n    assert not hasattr(f, 'name')", "documentation": "Make sure dupfile works on TextIO object", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_49": {"code": "def lsof_check():\n    pid = os.getpid()\n    try:\n        out = py.process.cmdexec(\"lsof -p %d\" % pid)\n    except (py.process.cmdexec.Error, UnicodeDecodeError):\n        # about UnicodeDecodeError, see note on pytester\n        pytest.skip(\"could not run 'lsof'\")\n    yield\n    out2 = py.process.cmdexec(\"lsof -p %d\" % pid)\n    len1 = len([x for x in out.split(\"\\n\") if \"REG\" in x])\n    len2 = len([x for x in out2.split(\"\\n\") if \"REG\" in x])\n    assert len2 < len1 + 3, out2", "documentation": "Check if the lsof of the current process is readable .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_50": {"code": "def test_simple(self, tmpfile):\n        fd = tmpfile.fileno()\n        cap = capture.FDCapture(fd)\n        data = tobytes(\"hello\")\n        os.write(fd, data)\n        s = cap.snap()\n        cap.done()\n        assert not s\n        cap = capture.FDCapture(fd)\n        cap.start()\n        os.write(fd, data)\n        s = cap.snap()\n        cap.done()\n        assert s == \"hello\"", "documentation": "Simple test for the FDCapture class .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_51": {"code": "def test_simple_many_check_open_files(self, testdir):\n        with lsof_check():\n            with testdir.makepyfile(\"\").open('wb+') as tmpfile:\n                self.test_simple_many(tmpfile)", "documentation": "Check that open file handles are correct .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_52": {"code": "def test_stderr(self):\n        cap = capture.FDCapture(2)\n        cap.start()\n        print(\"hello\", file=sys.stderr)\n        s = cap.snap()\n        cap.done()\n        assert s == \"hello\\n\"", "documentation": "Print to sys . stderr and discard the contents .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_53": {"code": "def test_writeorg(self, tmpfile):\n        data1, data2 = tobytes(\"foo\"), tobytes(\"bar\")\n        cap = capture.FDCapture(tmpfile.fileno())\n        cap.start()\n        tmpfile.write(data1)\n        tmpfile.flush()\n        cap.writeorg(data2)\n        scap = cap.snap()\n        cap.done()\n        assert scap == totext(data1)\n        with open(tmpfile.name, 'rb') as stmp_file:\n            stmp = stmp_file.read()\n            assert stmp == data2", "documentation": "Write and writeorg test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_54": {"code": "def saved_fd(fd):\n    new_fd = os.dup(fd)\n    try:\n        yield\n    finally:\n        os.dup2(new_fd, fd)\n        os.close(new_fd)", "documentation": "Context manager to temporarily duplicate fd .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_55": {"code": "def getcapture(self, **kw):\n        cap = self.__class__.captureclass(**kw)\n        cap.start_capturing()\n        try:\n            yield cap\n        finally:\n            cap.stop_capturing()", "documentation": "Get a new capture object and yield it .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_56": {"code": "def test_capturing_reset_simple(self):\n        with self.getcapture() as cap:\n            print(\"hello world\")\n            sys.stderr.write(\"hello error\\n\")\n            out, err = cap.readouterr()\n        assert out == \"hello world\\n\"\n        assert err == \"hello error\\n\"", "documentation": "Capture resets the terminal and tests that it returns the output and error message from the capture .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_57": {"code": "def test_capture_results_accessible_by_attribute(self):\n        with self.getcapture() as cap:\n            sys.stdout.write(\"hello\")\n            sys.stderr.write(\"world\")\n            capture_result = cap.readouterr()\n        assert capture_result.out == \"hello\"\n        assert capture_result.err == \"world\"", "documentation": "Test that the capture_result attribute is accessible by python", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_58": {"code": "def test_capturing_readouterr_decode_error_handling(self):\n        with self.getcapture() as cap:\n            # triggered a internal error in pytest\n            print('\\xa6')\n            out, err = cap.readouterr()\n        assert out == py.builtin._totext('\\ufffd\\n', 'unicode-escape')", "documentation": "Test decoding errors when capture stream is closed .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_59": {"code": "def test_capturing_modify_sysouterr_in_between(self):\n        oldout = sys.stdout\n        olderr = sys.stderr\n        with self.getcapture() as cap:\n            sys.stdout.write(\"hello\")\n            sys.stderr.write(\"world\")\n            sys.stdout = capture.CaptureIO()\n            sys.stderr = capture.CaptureIO()\n            print(\"not seen\")\n            sys.stderr.write(\"not seen\\n\")\n            out, err = cap.readouterr()\n        assert out == \"hello\"\n        assert err == \"world\"\n        assert sys.stdout == oldout\n        assert sys.stderr == olderr", "documentation": "Modify sys . stdout and sys . stderr after capture IO in between .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_60": {"code": "def test_just_out_capture(self):\n        with self.getcapture(out=True, err=False) as cap:\n            sys.stdout.write(\"hello\")\n            sys.stderr.write(\"world\")\n            out, err = cap.readouterr()\n        assert out == \"hello\"\n        assert not err", "documentation": "Test just out capture with no error .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_61": {"code": "def test_stdin_restored(self):\n        old = sys.stdin\n        with self.getcapture(in_=True):\n            newstdin = sys.stdin\n        assert newstdin != sys.stdin\n        assert sys.stdin is old", "documentation": "Restore sys . stdin to its original state", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_62": {"code": "def test_simple_only_fd(self, testdir):\n        testdir.makepyfile(\"\"\"\n            import os\n            def test_x():\n                os.write(1, \"hello\\\\n\".encode(\"ascii\"))\n                assert 0\n        \"\"\")\n        result = testdir.runpytest_subprocess()\n        result.stdout.fnmatch_lines(\"\"\"\n            *test_x*\n            *assert 0*\n            *Captured stdout*\n        \"\"\")", "documentation": "Make sure we capture stdout with open file descriptor", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_63": {"code": "def test_many(self, capfd):\n        with lsof_check():\n            for i in range(10):\n                cap = StdCaptureFD()\n                cap.stop_capturing()", "documentation": "Stop capturing the file descriptor", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_64": {"code": "def test_stdcapture_fd_invalid_fd(self, testdir):\n        testdir.makepyfile(\"\"\"\n            import os\n            from _pytest import capture\n            def StdCaptureFD(out=True, err=True, in_=True):\n                return capture.MultiCapture(out, err, in_,\n                                              Capture=capture.FDCapture)\n            def test_stdout():\n                os.close(1)\n                cap = StdCaptureFD(out=True, err=False, in_=False)\n                cap.stop_capturing()\n            def test_stderr():\n                os.close(2)\n                cap = StdCaptureFD(out=False, err=True, in_=False)\n                cap.stop_capturing()\n            def test_stdin():\n                os.close(0)\n                cap = StdCaptureFD(out=False, err=False, in_=True)\n                cap.stop_capturing()\n        \"\"\")\n        result = testdir.runpytest_subprocess(\"--capture=fd\")\n        assert result.ret == 0\n        assert result.parseoutcomes()['passed'] == 3", "documentation": "Test that the stdin and stdout capture work as expected on invalid file descriptors when using the fd capture", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_65": {"code": "def test_using_capsys_fixture_works_with_sys_stdout_encoding(capsys):\n    test_text = 'test text'\n\n    print(test_text.encode(sys.stdout.encoding, 'replace'))\n    (out, err) = capsys.readouterr()\n    assert out\n    assert err == ''", "documentation": "Use the fixture fixture to ensure that the encoding of the stdout is the same as the sys .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_66": {"code": "def test_fdcapture_tmpfile_remains_the_same(tmpfile, use):\n    if not use:\n        tmpfile = True\n    cap = StdCaptureFD(out=False, err=tmpfile)\n    try:\n        cap.start_capturing()\n        capfile = cap.err.tmpfile\n        cap.readouterr()\n    finally:\n        cap.stop_capturing()\n    capfile2 = cap.err.tmpfile\n    assert capfile2 == capfile", "documentation": "Make sure that the tmpfile returned by fdcapture doesn t stay the same", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_67": {"code": "def test_close_and_capture_again(testdir):\n    testdir.makepyfile(\"\"\"\n        import os\n        def test_close():\n            os.close(1)\n        def test_capture_again():\n            os.write(1, b\"hello\\\\n\")\n            assert 0\n    \"\"\")\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines(\"\"\"\n        *test_capture_again*\n        *assert 0*\n        *stdout*\n        *hello*\n    \"\"\")", "documentation": "Close and capture again test_close ( testdir )", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_68": {"code": "def test_capturing_and_logging_fundamentals(testdir, method):\n    if method == \"StdCaptureFD\" and not hasattr(os, 'dup'):\n        pytest.skip(\"need os.dup\")\n    # here we check a fundamental feature\n    p = testdir.makepyfile(\"\"\"\n        import sys, os\n        import py, logging\n        from _pytest import capture\n        cap = capture.MultiCapture(out=False, in_=False,\n                                     Capture=capture.%s)\n        cap.start_capturing()\n\n        logging.warn(\"hello1\")\n        outerr = cap.readouterr()\n        print (\"suspend, captured %%s\" %%(outerr,))\n        logging.warn(\"hello2\")\n\n        cap.pop_outerr_to_orig()\n        logging.warn(\"hello3\")\n\n        outerr = cap.readouterr()\n        print (\"suspend2, captured %%s\" %% (outerr,))\n    \"\"\" % (method,))\n    result = testdir.runpython(p)\n    result.stdout.fnmatch_lines(\"\"\"\n        suspend, captured*hello1*\n        suspend2, captured*WARNING:root:hello3*\n    \"\"\")\n    result.stderr.fnmatch_lines(\"\"\"\n        WARNING:root:hello2\n    \"\"\")\n    assert \"atexit\" not in result.stderr.str()", "documentation": "Test that we can set up the test with a basic capability that uses the test_basic_", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_69": {"code": "def test_capattr():\n            assert sys.stdout.errors == \"strict\"\n            assert sys.stderr.errors == \"strict\"", "documentation": "\"\" # Strict", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_70": {"code": "def test_py36_windowsconsoleio_workaround_non_standard_streams():\n    \"\"\"\n    Ensure _py36_windowsconsoleio_workaround function works with objects that\n    do not implement the full ``io``-based stream protocol, for example execnet channels (#2666).\n    \"\"\"\n    from _pytest.capture import _py36_windowsconsoleio_workaround\n\n    class DummyStream(object):\n        def write(self, s):\n            pass\n\n    stream = DummyStream()\n    _py36_windowsconsoleio_workaround(stream)", "documentation": "Synchronization test to ensure that non standard streams do not get written to .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_71": {"code": "def test_capattr():\n            # should not raise AttributeError\n            assert sys.stdout.encoding\n            assert sys.stderr.encoding", "documentation": "Test that stdout and stderr encoding are the same as that of the Python codec module", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "tareqalayan/pytest_testing/test_capture.py_72": {"code": "def test_crash_on_closing_tmpfile_py27(testdir):\n    testdir.makepyfile('''\n        from __future__ import print_function\n        import time\n        import threading\n        import sys\n\n        def spam():\n            f = sys.stderr\n            while True:\n                print('.', end='', file=f)\n\n        def test_silly():\n            t = threading.Thread(target=spam)\n            t.daemon = True\n            t.start()\n            time.sleep(0.5)\n\n    ''')\n    result = testdir.runpytest_subprocess()\n    assert result.ret == 0\n    assert 'IOError' not in result.stdout.str()", "documentation": "Python 3 . 7 crashes when the Python file is closed .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570690.0}}, "greencoder/hopefullysunny-django_registrations/management/commands/registration_worker.py_0": {"code": "def log(self, message):\n        f = open(settings.TASK_LOG_PATH, 'a')\n        now = datetime.datetime.utcnow().replace(tzinfo=pytz.utc)\n        log_message = \"%s\\t%s\\n\" % (now, message)\n        self.stdout.write(log_message)\n        f.write(log_message)\n        f.close()", "documentation": "Writes the given message to the task log file .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "dustlab/noisemapper_scripts/nmcollector.py_0": {"code": "def main():", "documentation": "pass", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "dcorney/text-generation_core/dialogue.py_0": {"code": "def __init__(self, names, pronouns, mc):\n        self.speakers = [{\"name\": n, \"pronoun\": p} for n, p in list(zip(names, pronouns))]\n        self._transitions = self.make_transition_probs()\n        self._speech_acts = [\"said\", \"whispered\", \"shouted\", \"cried\"]\n        self._acts_transitions = [25, 2, 2, 2]\n        self.mc = mc\n        # self.seeds = seeds\n        self.target_len = np.random.randint(5, 50, size=len(names))  # rough words per sentence", "documentation": "Create a speaker object", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "dcorney/text-generation_core/dialogue.py_1": {"code": "def after(self, speaker_id):\n        \"\"\"Pick next person to speak\"\"\"\n        row = self._transitions[speaker_id]\n        sucessor = searchsorted(cumsum(row), rand() * sum(row))\n        return sucessor", "documentation": "Pick next person to speak after transition", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "dcorney/text-generation_core/dialogue.py_2": {"code": "def speech_sequence(self, n):\n        speech_acts_seq = []\n        next_speech_id = 0\n        for i in range(n):\n            next_speech_id = searchsorted(cumsum(self._acts_transitions), rand() * sum(self._acts_transitions))\n            speech_acts_seq.append(self._speech_acts[next_speech_id])\n        return speech_acts_seq", "documentation": "Generates a random speech sequence for n sentences", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "dcorney/text-generation_core/dialogue.py_3": {"code": "def make_speech_bits(self, seeds):\n        n = len(seeds)\n        speaker_id = self.speaker_sequence(0, n)\n        speech_acts_seq = self.speech_sequence(n)\n        bits = []\n        ss = sentence.SentenceMaker(self.mc)\n        for i in range(n):\n            sent_toks = ss.generate_sentence_tokens([seeds[i]], self.target_len[speaker_id[i]])\n            sent_toks = ss.polish_sentence(sent_toks)\n            bits.append({'speaker_name': self.speakers[speaker_id[i]][\"name\"],\n                         'speech_act': speech_acts_seq[speaker_id[i]],\n                         'seq_id': speaker_id[i],\n                         'speech': sent_toks,\n                         'paragraph': True})\n        return(bits)", "documentation": "Generate speech bits from seed list", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "dcorney/text-generation_core/dialogue.py_4": {"code": "def report_seq(self, seq_map):\n        \"\"\"Convert sequence of speeches to a tokens.\"\"\"\n        sents = []\n        for i in range(0, len(seq_map)):\n\n            if seq_map[i]['paragraph']:\n                # text += \"\\n    \"\n                quote_start = '\"'\n            else:\n                quote_start = \"\"\n            if i > len(seq_map) - 2 or seq_map[i + 1]['paragraph']:\n                quote_end = '\"'\n            else:\n                quote_end = \" \"\n            if len(seq_map[i]['speech_act']) > 0:\n                speech_act = seq_map[i]['speech_act'] + \",\"\n            else:\n                speech_act = seq_map[i]['speech_act']\n            tokens = [utils.START_TOKEN]\n            tokens.append(seq_map[i]['speaker_str'])\n            tokens.append(speech_act)\n            tokens.append(quote_start)\n            tokens.extend(seq_map[i]['speech'][1:-1])\n            tokens.append(quote_end)\n            tokens.append(utils.END_TOKEN)\n            sents.append(tokens)\n        return sents", "documentation": "Convert sequence of speeches to a tokens .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_0": {"code": "def make_render_children(separator: str) -> Render:\n    def render_children(\n        node: RenderTreeNode,\n        context: RenderContext,\n    ) -> str:\n        return separator.join(child.render(context) for child in node.children)\n\n    return render_children", "documentation": "Generate render function for render children nodes with a separator .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_1": {"code": "def code_inline(node: RenderTreeNode, context: RenderContext) -> str:\n    code = node.content\n    all_chars_are_whitespace = not code.strip()\n    longest_backtick_seq = longest_consecutive_sequence(code, \"`\")\n    if longest_backtick_seq:\n        separator = \"`\" * (longest_backtick_seq + 1)\n        return f\"{separator} {code} {separator}\"\n    if code.startswith(\" \") and code.endswith(\" \") and not all_chars_are_whitespace:\n        return f\"` {code} `\"\n    return f\"`{code}`\"", "documentation": "Inline code block", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_2": {"code": "def html_inline(node: RenderTreeNode, context: RenderContext) -> str:\n    return node.content", "documentation": "Returns node inline html", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_3": {"code": "def hardbreak(node: RenderTreeNode, context: RenderContext) -> str:\n    if _in_block(\"heading\", node):\n        return \"<br /> \"\n    return \"\\\\\" + \"\\n\"", "documentation": "Returns a hardbreak line depending on the node .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_4": {"code": "def text(node: RenderTreeNode, context: RenderContext) -> str:\n    \"\"\"Process a text token.\n\n    Text should always be a child of an inline token. An inline token\n    should always be enclosed by a heading or a paragraph.\n    \"\"\"\n    text = node.content\n\n    # Escape backslash to prevent it from making unintended escapes.\n    # This escape has to be first, else we start multiplying backslashes.\n    text = text.replace(\"\\\\\", \"\\\\\\\\\")\n\n    text = escape_asterisk_emphasis(text)  # Escape emphasis/strong marker.\n    text = escape_underscore_emphasis(text)  # Escape emphasis/strong marker.\n    text = text.replace(\"[\", \"\\\\[\")  # Escape link label enclosure\n    text = text.replace(\"]\", \"\\\\]\")  # Escape link label enclosure\n    text = text.replace(\"<\", \"\\\\<\")  # Escape URI enclosure\n    text = text.replace(\"`\", \"\\\\`\")  # Escape code span marker\n\n    # Escape \"&\" if it starts a sequence that can be interpreted as\n    # a character reference.\n    text = RE_CHAR_REFERENCE.sub(r\"\\\\\\g<0>\", text)\n\n    # The parser can give us consecutive newlines which can break\n    # the markdown structure. Replace two or more consecutive newlines\n    # with newline character's decimal reference.\n    text = text.replace(\"\\n\\n\", \"&#10;&#10;\")\n\n    # If the last character is a \"!\" and the token next up is a link, we\n    # have to escape the \"!\" or else the link will be interpreted as image.\n    next_sibling = node.next_sibling\n    if text.endswith(\"!\") and next_sibling and next_sibling.type == \"link\":\n        text = text[:-1] + \"\\\\!\"\n\n    if context.do_wrap and _in_block(\"paragraph\", node):\n        text = re.sub(r\"\\s+\", WRAP_POINT, text)\n\n    return text", "documentation": "Process a text token . Text should always be a child of an inline token . An inline token", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_5": {"code": "def code_block(node: RenderTreeNode, context: RenderContext) -> str:\n    return fence(node, context)", "documentation": "Returns code block begining with given node and then with fenced code block begining with given", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_6": {"code": "def _render_inline_as_text(node: RenderTreeNode, context: RenderContext) -> str:\n    \"\"\"Special kludge for image `alt` attributes to conform CommonMark spec.\n\n    Don't try to use it! Spec requires to show `alt` content with\n    stripped markup, instead of simple escaping.\n    \"\"\"\n\n    def text_renderer(node: RenderTreeNode, context: RenderContext) -> str:\n        return node.content\n\n    def image_renderer(node: RenderTreeNode, context: RenderContext) -> str:\n        return _render_inline_as_text(node, context)\n\n    inline_renderers: Mapping[str, Render] = defaultdict(\n        lambda: make_render_children(\"\"),\n        {\n            \"text\": text_renderer,\n            \"image\": image_renderer,\n            \"link\": link,\n            \"softbreak\": softbreak,\n        },\n    )\n    inline_context = RenderContext(\n        inline_renderers, context.postprocessors, context.options, context.env\n    )\n    return make_render_children(\"\")(node, inline_context)", "documentation": "Special kludge for image alt attributes to conform CommonMark spec . Don t try to use", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_7": {"code": "def em(node: RenderTreeNode, context: RenderContext) -> str:\n    text = make_render_children(separator=\"\")(node, context)\n    indicator = node.markup\n    return indicator + text + indicator", "documentation": "Returns node markup as em block", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_8": {"code": "def heading(node: RenderTreeNode, context: RenderContext) -> str:\n    text = make_render_children(separator=\"\")(node, context)\n\n    if node.markup == \"=\":\n        prefix = \"# \"\n    elif node.markup == \"-\":\n        prefix = \"## \"\n    else:  # ATX heading\n        prefix = node.markup + \" \"\n\n    # There can be newlines in setext headers, but we make an ATX\n    # header always. Convert newlines to spaces.\n    text = text.replace(\"\\n\", \" \")\n\n    # If the text ends in a sequence of hashes (#), the hashes will be\n    # interpreted as an optional closing sequence of the heading, and\n    # will not be rendered. Escape a line ending hash to prevent this.\n    if text.endswith(\"#\"):\n        text = text[:-1] + \"\\\\#\"\n\n    return prefix + text", "documentation": "Returns the rendered text of a heading element", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_9": {"code": "def _wrap(text: str, *, width: int | Literal[\"no\"]) -> str:\n    \"\"\"Wrap text at locations pointed by `WRAP_POINT`s.\n\n    Converts `WRAP_POINT`s to either a space or newline character, thus\n    wrapping the text. Already existing whitespace will be preserved as\n    is.\n    \"\"\"\n    text, replacements = _prepare_wrap(text)\n    if width == \"no\":\n        return _recover_preserve_chars(text, replacements)\n\n    wrapper = textwrap.TextWrapper(\n        break_long_words=False,\n        break_on_hyphens=False,\n        width=width,\n        expand_tabs=False,\n        replace_whitespace=False,\n    )\n    wrapped = wrapper.fill(text)\n    wrapped = _recover_preserve_chars(wrapped, replacements)\n    return \" \" + wrapped if text.startswith(\" \") else wrapped", "documentation": "Wrap text at locations pointed by WRAP_POINTs . Converts WRAP_POINTs to", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_10": {"code": "def _recover_preserve_chars(text: str, replacements: str) -> str:\n    replacement_iterator = iter(replacements)\n    return \"\".join(\n        next(replacement_iterator) if c == PRESERVE_CHAR else c for c in text\n    )", "documentation": "Recover the text to contain PRESERVE_CHAR from replacements .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_11": {"code": "def list_item(node: RenderTreeNode, context: RenderContext) -> str:\n    \"\"\"Return one list item as string.\n\n    This returns just the content. List item markers and indentation are\n    added in `bullet_list` and `ordered_list` renderers.\n    \"\"\"\n    block_separator = \"\\n\" if is_tight_list_item(node) else \"\\n\\n\"\n    text = make_render_children(block_separator)(node, context)\n\n    if not text.strip():\n        return \"\"\n    return text", "documentation": "Return one list item as string . This returns just the content . List item markers and indentation are", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_12": {"code": "def ordered_list(node: RenderTreeNode, context: RenderContext) -> str:\n    consecutive_numbering = context.options.get(\"mdformat\", {}).get(\n        \"number\", DEFAULT_OPTS[\"number\"]\n    )\n    marker_type = get_list_marker_type(node)\n    first_line_indent = \" \"\n    block_separator = \"\\n\" if is_tight_list(node) else \"\\n\\n\"\n    list_len = len(node.children)\n\n    starting_number = node.attrs.get(\"start\")\n    if starting_number is None:\n        starting_number = 1\n    assert isinstance(starting_number, int)\n\n    if consecutive_numbering:\n        indent_width = len(\n            f\"{list_len + starting_number - 1}{marker_type}{first_line_indent}\"\n        )\n    else:\n        indent_width = len(f\"{starting_number}{marker_type}{first_line_indent}\")\n\n    text = \"\"\n    with context.indented(indent_width):\n        for list_item_index, list_item in enumerate(node.children):\n            list_item_text = list_item.render(context)\n            formatted_lines = []\n            line_iterator = iter(list_item_text.split(\"\\n\"))\n            first_line = next(line_iterator)\n            if consecutive_numbering:\n                # Prefix first line of the list item with consecutive numbering,\n                # padded with zeros to make all markers of even length.\n                # E.g.\n                #   002. This is the first list item\n                #   003. Second item\n                #   ...\n                #   112. Last item\n                number = starting_number + list_item_index\n                pad = len(str(list_len + starting_number - 1))\n                number_str = str(number).rjust(pad, \"0\")\n                formatted_lines.append(\n                    f\"{number_str}{marker_type}{first_line_indent}{first_line}\"\n                    if first_line\n                    else f\"{number_str}{marker_type}\"\n                )\n            else:\n                # Prefix first line of first item with the starting number of the\n                # list. Prefix following list items with the number one\n                # prefixed by zeros to make the list item marker of even length\n                # with the first one.\n                # E.g.\n                #   5321. This is the first list item\n                #   0001. Second item\n                #   0001. Third item\n                first_item_marker = f\"{starting_number}{marker_type}\"\n                other_item_marker = (\n                    \"0\" * (len(str(starting_number)) - 1) + \"1\" + marker_type\n                )\n                if list_item_index == 0:\n                    formatted_lines.append(\n                        f\"{first_item_marker}{first_line_indent}{first_line}\"\n                        if first_line\n                        else first_item_marker\n                    )\n                else:\n                    formatted_lines.append(\n                        f\"{other_item_marker}{first_line_indent}{first_line}\"\n                        if first_line\n                        else other_item_marker\n                    )\n            for line in line_iterator:\n                formatted_lines.append(\" \" * indent_width + line if line else \"\")\n\n            text += \"\\n\".join(formatted_lines)\n            if list_item_index != len(node.children) - 1:\n                text += block_separator\n\n        return text", "documentation": "Returns ordered list of items for the node", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_13": {"code": "def indented(self, width: int) -> Generator[None, None, None]:\n        self.env[\"indent_width\"] += width\n        try:\n            yield\n        finally:\n            self.env[\"indent_width\"] -= width", "documentation": "Add width indentation to the environment .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "executablebooks/mdformat_src/mdformat/renderer/_context.py_14": {"code": "def do_wrap(self) -> bool:\n        wrap_mode = self.options.get(\"mdformat\", {}).get(\"wrap\", DEFAULT_OPTS[\"wrap\"])\n        return isinstance(wrap_mode, int) or wrap_mode == \"no\"", "documentation": "Return whether to wrap the output md - formatted file .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "alfateam123/Teca_tests/test_utils.py_0": {"code": "def setUp(self):\n        self.conf = tecaconf.ConfigHandler(\n            \"tests/test_data/configuration.json\",\n            {\"starting_path\": \"tests/test_data/images\"}\n        )\n        self.files_list = [\n          \"foo.doc\",\n          \"yukinon.jpg\",\n          \"cuteflushadoingflushathings.webm\"\n        ]", "documentation": "Setup the test files", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "DayGitH/Python-Challenges_DailyProgrammer/DP20150713A.py_0": {"code": "def main():\n    pass", "documentation": "pass", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "redsolution/django-generic-ratings_ratings/forms/widgets.py_0": {"code": "def get_parent_id(self, name, attrs):\n        final_attrs = self.build_attrs(attrs, type=self.input_type, name=name)\n        return final_attrs['id']", "documentation": "Get the parent id for the element .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "redsolution/django-generic-ratings_ratings/forms/widgets.py_1": {"code": "def get_values(self, min_value, max_value, step=1):\n        decimal_step = Decimal(str(step))\n        value = Decimal(str(min_value))\n        while value <= max_value:\n            yield value\n            value += decimal_step", "documentation": "Generate a range of decimals between min_value and max_value", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "redsolution/django-generic-ratings_ratings/forms/widgets.py_2": {"code": "def __init__(self, min_value, max_value, step, instance=None,\n        can_delete_vote=True, key='', read_only=False, default='',\n        template='ratings/slider_widget.html', attrs=None):\n        \"\"\"\n        The argument *default* is used when the initial value is None.\n        \"\"\"\n        super(SliderWidget, self).__init__(attrs)\n        self.min_value = min_value\n        self.max_value = max_value\n        self.step = step\n        self.instance = instance\n        self.can_delete_vote = can_delete_vote\n        self.read_only = read_only\n        self.default = default\n        self.template = template\n        self.key = key", "documentation": "Initializes the slider widget", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "redsolution/django-generic-ratings_ratings/forms/widgets.py_3": {"code": "def render(self, name, value, attrs=None):\n        context = self.get_context(name, value, attrs or {})\n        return render_to_string(self.template, context)", "documentation": "Render the widget with the given name value and attributes .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "redsolution/django-generic-ratings_ratings/forms/widgets.py_4": {"code": "def __init__(self, min_value, max_value, step, instance=None,\n        can_delete_vote=True, key='', read_only=False,\n        template='ratings/star_widget.html', attrs=None):\n        super(StarWidget, self).__init__(attrs)\n        self.min_value = min_value\n        self.max_value = max_value\n        self.step = step\n        self.instance = instance\n        self.can_delete_vote = can_delete_vote\n        self.read_only = read_only\n        self.template = template\n        self.key = key", "documentation": "Creates a widget for making a rating .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "redsolution/django-generic-ratings_ratings/forms/widgets.py_5": {"code": "def _get_value(self, original, split):\n        if original:\n            value = round(original * split) / split\n            return Decimal(str(value))", "documentation": "Get value from original and split", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "redsolution/django-generic-ratings_ratings/forms/widgets.py_6": {"code": "def __init__(self, min_value, max_value, instance=None,\n        can_delete_vote=True, template='ratings/like_widget.html', attrs=None):\n        super(LikeWidget, self).__init__(attrs)\n        self.min_value = min_value\n        self.max_value = max_value\n        self.instance = instance\n        self.can_delete_vote = can_delete_vote\n        self.template = template", "documentation": "Creates a widget with a min and max value .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "redsolution/django-generic-ratings_ratings/forms/widgets.py_7": {"code": "def get_context(self, name, value, attrs=None):\n        # here we convert *min_value*, *max_value* and *step*\n        # to string to avoid odd behaviours of Django localization\n        # in the template (and, for backward compatibility we do not\n        # want to use the *unlocalize* filter)\n        attrs['type'] = 'hidden'\n        return {\n            'min_value': str(self.min_value),\n            'max_value': str(self.max_value),\n            'can_delete_vote': self.can_delete_vote,\n            'parent': super(LikeWidget, self).render(name, value, attrs),\n            'parent_id': self.get_parent_id(name, attrs),\n            'value': str(value),\n            'like_id': self.get_widget_id('like', name),\n        }", "documentation": "Get the context for the widget .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "IQSS/miniverse_dv_apps/metrics/stats_views_dataverses.py_0": {"code": "def get_stats_result(self, request):\n        \"\"\"Return the StatsResult object for this statistic\"\"\"\n        stats_datasets = StatsMakerDataverses(**request.GET.dict())\n\n        pub_state = self.get_pub_state(request)\n\n        if pub_state == self.PUB_STATE_ALL:\n            stats_result = stats_datasets.get_dataverse_counts_by_month()\n        elif pub_state == self.PUB_STATE_UNPUBLISHED:\n            stats_result = stats_datasets.get_dataverse_counts_by_month_unpublished()\n        else:\n            stats_result = stats_datasets.get_dataverse_counts_by_month_published()\n\n        return stats_result", "documentation": "Return the StatsResult object for this statistic", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "IQSS/miniverse_dv_apps/metrics/stats_views_dataverses.py_1": {"code": "def get_stats_result(self, request):\n        \"\"\"Return the StatsResult object for this statistic\"\"\"\n        stats_datasets = StatsMakerDataverses(**request.GET.dict())\n\n        pub_state = self.get_pub_state(request)\n\n        if pub_state == self.PUB_STATE_ALL:\n            stats_result = stats_datasets.get_dataverse_count()\n        elif pub_state == self.PUB_STATE_UNPUBLISHED:\n            stats_result = stats_datasets.get_dataverse_count_unpublished()\n        else:\n            stats_result = stats_datasets.get_dataverse_count_published()\n\n        return stats_result", "documentation": "Return the StatsResult object for this statistic", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "IQSS/miniverse_dv_apps/metrics/stats_views_dataverses.py_2": {"code": "def get_stats_result(self, request):\n        \"\"\"Return the StatsResult object for this statistic\"\"\"\n        stats_datasets = StatsMakerDataverses(**request.GET.dict())\n\n        pub_state = self.get_pub_state(request)\n\n        if pub_state == self.PUB_STATE_ALL:\n            stats_result = stats_datasets.get_dataverse_affiliation_counts()\n        elif pub_state == self.PUB_STATE_UNPUBLISHED:\n            stats_result = stats_datasets.get_dataverse_affiliation_counts_unpublished()\n        else:\n            stats_result = stats_datasets.get_dataverse_affiliation_counts_published()\n\n        return stats_result", "documentation": "Return the StatsResult object for this statistic", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "IQSS/miniverse_dv_apps/metrics/stats_views_dataverses.py_3": {"code": "def is_show_uncategorized(self, request):\n        \"\"\"Return the result of the \"?show_uncategorized\" query string param\"\"\"\n\n        show_uncategorized = request.GET.get('show_uncategorized', False)\n        if show_uncategorized is True or show_uncategorized == 'true':\n            return True\n        return False", "documentation": "Return the result of the show_uncategorized query string param", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_0": {"code": "def _create_user(self, email, password,\n                     is_staff, is_superuser, **extra_fields):\n        \"\"\"\n        Creates and saves an User with the given email and password.\n        \"\"\"\n        now = timezone.now()\n        if not email:\n            raise ValueError('An email address must be provided.')\n        email = self.normalize_email(email)\n        if \"is_active\" not in extra_fields:\n            extra_fields[\"is_active\"] = True\n        if \"username\" not in extra_fields:\n            # For now we need to have a unique id that is at\n            # most 30 characters long.  Using uuid and truncating.\n            # Ideally username goes away entirely at some point\n            # since we're really using email.  If we have to keep\n            # username for some reason then we could switch over\n            # to a string version of the pk which is guaranteed\n            # be unique.\n            extra_fields[\"username\"] = str(uuid.uuid4())[:MAX_USERNAME_LENGTH]\n        user = self.model(email=email,\n                          is_staff=is_staff,\n                          is_superuser=is_superuser,\n                          last_login=None,\n                          date_joined=now,\n                          **extra_fields)\n        user.set_password(password)\n        user.save(using=self._db)\n        return user", "documentation": "Creates and saves a User with the given email and password .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_1": {"code": "def create_superuser(self, email, password, **extra_fields):\n        return self._create_user(email, password, True, True,\n                                 **extra_fields)", "documentation": "Creates a new user with super permissions .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_2": {"code": "def __init__(self, *args, **kwargs):\n        super(User, self).__init__(*args, **kwargs)\n        self.startup = None\n        self.team_member = None\n        self.profile = None\n        self.user_finalist_roles = None", "documentation": "Populates the user attributes .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_3": {"code": "def __str__(self):\n        return self.email", "documentation": "Email address in string format", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_4": {"code": "def user_phone(self):\n        return self._get_profile().phone", "documentation": "Return the user s phone number .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_5": {"code": "def team_member_id(self):\n        return self.team_member.id if self._get_member() else ''", "documentation": "Returns the team member s ID or an empty string if the team member does not exist", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_6": {"code": "def user_twitter_handle(self):\n        return self._get_profile().twitter_handle", "documentation": "Get the twitter handle for the user .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_7": {"code": "def user_facebook_url(self):\n        return self._get_profile().facebook_url", "documentation": "Facebook URL of the user .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_8": {"code": "def type(self):\n        return self._get_profile().user_type", "documentation": "Return the type of the user .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_9": {"code": "def _get_title_and_company(self):\n        if self._is_expert() and self._has_expert_details():\n            profile = self._get_profile()\n            title = profile.title\n            company = profile.company\n            return {\n                \"title\": title,\n                \"company\": company\n            }\n        self._get_member()\n        title = self.team_member.title if self.team_member else \"\"\n        company = self.startup.name if self._get_startup() else None\n        return {\n            \"title\": title,\n            \"company\": company\n        }", "documentation": "Get title and company for current user .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_10": {"code": "def startup_industry(self):\n        return self.startup.primary_industry if self._get_startup() else None", "documentation": "Returns the primary industry of the startup if it exists", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_11": {"code": "def startup_status_names(self):\n        if self._get_startup():\n            return [startup_status.program_startup_status.startup_status\n                    for startup_status in self.startup.startupstatus_set.all()]", "documentation": "Returns a list of the startup statuses known by the system", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_12": {"code": "def program(self):\n        return self.startup.current_program() if self._get_startup() else None", "documentation": "Current program name used by the simulation .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_13": {"code": "def year(self):\n        program = self.program()\n        return program.start_date.year if program else None", "documentation": "Return the year of the start of the program", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_14": {"code": "def _get_startup(self):\n        if not self.startup:\n            self._get_member()\n            if self.team_member:\n                self.startup = self.team_member.startup\n        return self.startup", "documentation": "Get startup value from API if not set", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "masschallenge/django-accelerator_simpleuser/models.py_15": {"code": "def _get_profile(self):\n        if self.profile:\n            return self.profile\n        self.profile = self.get_profile()\n        return self.profile", "documentation": "Get the current profile from the API or cache it .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "keithhamilton/transposer_setup.py_0": {"code": "def read(*paths):\n    \"\"\"Build a file path from *paths* and return the contents.\"\"\"\n    with open(os.path.join(*paths), 'r') as f:\n        return f.read()", "documentation": "Build a file path from * paths* and return the contents .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570691.0}}, "DestructHub/bcs-contest_2016/Main/L/Python/solution_1_wrong.py_0": {"code": "def calc():\n\th, l = input().split(' ')\n\n\tmapa = []\n\n\tfor i_row in range(int(h)):\n\t\tmapa.append(input().split(' '))\n\n\tmaior_num = 0", "documentation": "calculate maior number", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "jonasrauber/c2s_c2s/experiment.py_0": {"code": "def __str__(self):\n\t\t\"\"\"\n\t\tSummarize information about the experiment.\n\n\t\t@rtype: string\n\t\t@return: summary of the experiment\n\t\t\"\"\"\n\n\t\tstrl = []\n\n\t\t# date and duration of experiment\n\t\tstrl.append(strftime('date \\t\\t %a, %d %b %Y %H:%M:%S', localtime(self.time)))\n\t\tstrl.append('duration \\t ' + str(int(self.duration)) + 's')\n\t\tstrl.append('hostname \\t ' + self.hostname)\n\n\t\t# commit hash\n\t\tif self.commit:\n\t\t\tif self.modified:\n\t\t\t\tstrl.append('commit \\t\\t ' + self.commit + ' (modified)')\n\t\t\telse:\n\t\t\t\tstrl.append('commit \\t\\t ' + self.commit)\n\n\t\t# results\n\t\tstrl.append('results \\t {' + ', '.join(map(str, self.results.keys())) + '}')\n\n\t\t# comment\n\t\tif self.comment:\n\t\t\tstrl.append('\\n' + self.comment)\n\n\t\treturn '\\n'.join(strl)", "documentation": "Returns a string summarizing information about the experiment .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "jonasrauber/c2s_c2s/experiment.py_1": {"code": "def __init__(self, filename='', comment='', seed=None, server=None, port=8000):\n\t\t\"\"\"\n\t\tIf the filename is given and points to an existing experiment, load it.\n\t\tOtherwise store the current timestamp and try to get commit information\n\t\tfrom the repository in the current directory.\n\n\t\t@type  filename: string\n\t\t@param filename: path to where the experiment will be stored", "documentation": "Load a file if it is not given . Otherwise store the current timestamp and comment in the current", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "jonasrauber/c2s_c2s/experiment.py_2": {"code": "def status(self, status, **kwargs):\n\t\tif self.server:\n\t\t\ttry:\n\t\t\t\tconn = HTTPConnection(self.server, self.port)\n\t\t\t\tconn.request('GET', '/version/')\n\t\t\t\tresp = conn.getresponse()\n\n\t\t\t\tif not resp.read().startswith('Experiment'):\n\t\t\t\t\traise RuntimeError()\n\n\t\t\t\tHTTPConnection(self.server, self.port).request('POST', '', str(dict({\n\t\t\t\t\t\t'id': self.id,\n\t\t\t\t\t\t'version': __version__,\n\t\t\t\t\t\t'status': status,\n\t\t\t\t\t\t'hostname': self.hostname,\n\t\t\t\t\t\t'cwd': self.cwd,\n\t\t\t\t\t\t'script_path': self.script_path,\n\t\t\t\t\t\t'script': self.script,\n\t\t\t\t\t\t'comment': self.comment,\n\t\t\t\t\t\t'time': self.time,\n\t\t\t\t\t}, **kwargs)))\n\t\t\texcept:\n\t\t\t\twarn('Unable to connect to \\'{0}:{1}\\'.'.format(self.server, self.port))", "documentation": "Set the status of the plugin .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "jonasrauber/c2s_c2s/experiment.py_3": {"code": "def save(self, filename=None, overwrite=False):\n\t\t\"\"\"\n\t\tStore results. If a filename is given, the default is overwritten.\n\n\t\t@type  filename: string\n\t\t@param filename: path to where the experiment will be stored\n\n\t\t@type  overwrite: boolean\n\t\t@param overwrite: overwrite existing files\n\t\t\"\"\"\n\n\t\tself.duration = time() - self.time\n\n\t\tif filename is None:\n\t\t\tfilename = self.filename\n\t\telse:\n\t\t\t# replace {0} and {1} by date and time\n\t\t\ttmp1 = strftime('%d%m%Y', localtime(time()))\n\t\t\ttmp2 = strftime('%H%M%S', localtime(time()))\n\t\t\tfilename = filename.format(tmp1, tmp2)\n\n\t\t\tself.filename = filename\n\n\t\t# make sure directory exists\n\t\ttry:\n\t\t\tos.makedirs(path.dirname(filename))\n\t\texcept OSError:\n\t\t\tpass\n\n\t\t# make sure filename is unique\n\t\tcounter = 0\n\t\tpieces = path.splitext(filename)\n\n\t\tif not overwrite:\n\t\t\twhile path.exists(filename):\n\t\t\t\tcounter += 1\n\t\t\t\tfilename = pieces[0] + '.' + str(counter) + pieces[1]\n\n\t\t\tif counter:\n\t\t\t\twarn(''.join(pieces) + ' already exists. Saving to ' + filename + '.')\n\n\t\t# store experiment\n\t\twith open(filename, 'wb') as handle:\n\t\t\tdump({\n\t\t\t\t'version': __version__,\n\t\t\t\t'id': self.id,\n\t\t\t\t'time': self.time,\n\t\t\t\t'seed': self.seed,\n\t\t\t\t'duration': self.duration,\n\t\t\t\t'environ': self.environ,\n\t\t\t\t'hostname': self.hostname,\n\t\t\t\t'cwd': self.cwd,\n\t\t\t\t'argv': self.argv,\n\t\t\t\t'script': self.script,\n\t\t\t\t'script_path': self.script_path,\n\t\t\t\t'processors': self.processors,\n\t\t\t\t'platform': self.platform,\n\t\t\t\t'comment': self.comment,\n\t\t\t\t'commit': self.commit,\n\t\t\t\t'modified': self.modified,\n\t\t\t\t'versions': self.versions,\n\t\t\t\t'results': self.results}, handle, 1)\n\n\t\tself.status('SAVE', filename=filename, duration=self.duration)", "documentation": "Save experiment to a file .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "jonasrauber/c2s_c2s/experiment.py_4": {"code": "def __getitem__(self, key):\n\t\treturn self.results[key]", "documentation": "Return a result by key", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "jonasrauber/c2s_c2s/experiment.py_5": {"code": "def __delitem__(self, key):\n\t\tdel self.results[key]", "documentation": "Remove an item from the dictionary", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "jonasrauber/c2s_c2s/experiment.py_6": {"code": "def do_GET(self):\n\t\t\"\"\"\n\t\tRenders HTML displaying running and saved experiments.\n\t\t\"\"\"\n\n\t\t# number of bars representing progress\n\t\tmax_bars = 20\n\n\t\tif self.path == '/version/':\n\t\t\tself.send_response(200)\n\t\t\tself.send_header('Content-type', 'text/plain')\n\t\t\tself.end_headers()\n\n\t\t\tself.wfile.write('Experiment {0}'.format(__version__))\n\n\t\telif self.path.startswith('/running/'):\n\t\t\tid = int([s for s in self.path.split('/') if s != ''][-1])\n\n\t\t\t# display running experiment\n\t\t\tif id in ExperimentRequestHandler.running:\n\t\t\t\tself.send_response(200)\n\t\t\t\tself.send_header('Content-type', 'text/html')\n\t\t\t\tself.end_headers()\n\n\t\t\t\tself.wfile.write(HTML_HEADER)\n\t\t\t\tself.wfile.write('<h2>Experiment</h2>')\n\n\t\t\t\tinstance = ExperimentRequestHandler.running[id]\n\n\t\t\t\tnum_bars = int(instance['progress']) * max_bars / 100\n\n\t\t\t\tself.wfile.write('<table>')\n\t\t\t\tself.wfile.write('<tr><th>Experiment:</th><td>{0}</td></tr>'.format(\n\t\t\t\t\tos.path.join(instance['cwd'], instance['script_path'])))\n\t\t\t\tself.wfile.write('<tr><th>Hostname:</th><td>{0}</td></tr>'.format(instance['hostname']))\n\t\t\t\tself.wfile.write('<tr><th>Status:</th><td class=\"running\">{0}</td></tr>'.format(instance['status']))\n\t\t\t\tself.wfile.write('<tr><th>Progress:</th><td class=\"progress\"><span class=\"bars\">{0}</span>{1}</td></tr>'.format(\n\t\t\t\t\t'|' * num_bars, '|' * (max_bars - num_bars)))\n\t\t\t\tself.wfile.write('<tr><th>Start:</th><td>{0}</td></tr>'.format(\n\t\t\t\t\tstrftime('%a, %d %b %Y %H:%M:%S', localtime(instance['time']))))\n\t\t\t\tself.wfile.write('<tr><th>Comment:</th><td>{0}</td></tr>'.format(\n\t\t\t\t\tinstance['comment']  if instance['comment'] else '-'))\n\t\t\t\tself.wfile.write('</table>')\n\n\t\t\t\tself.wfile.write('<h2>Script</h2>')\n\t\t\t\tself.wfile.write('<pre>{0}</pre>'.format(instance['script']))\n\t\t\t\tself.wfile.write(HTML_FOOTER)\n\n\t\t\telif id in ExperimentRequestHandler.finished:\n\t\t\t\tself.send_response(302)\n\t\t\t\tself.send_header('Location', '/finished/{0}/'.format(id))\n\t\t\t\tself.end_headers()\n\n\t\t\telse:\n\t\t\t\tself.send_response(200)\n\t\t\t\tself.send_header('Content-type', 'text/html')\n\t\t\t\tself.end_headers()\n\n\t\t\t\tself.wfile.write(HTML_HEADER)\n\t\t\t\tself.wfile.write('<h2>404</h2>')\n\t\t\t\tself.wfile.write('Requested experiment not found.')\n\t\t\t\tself.wfile.write(HTML_FOOTER)\n\n\t\telif self.path.startswith('/finished/'):\n\t\t\tself.send_response(200)\n\t\t\tself.send_header('Content-type', 'text/html')\n\t\t\tself.end_headers()\n\n\t\t\tself.wfile.write(HTML_HEADER)\n\n\t\t\tid = int([s for s in self.path.split('/') if s != ''][-1])\n\n\t\t\t# display finished experiment\n\t\t\tif id in ExperimentRequestHandler.finished:\n\t\t\t\tinstance = ExperimentRequestHandler.finished[id]\n\n\t\t\t\tif id in ExperimentRequestHandler.running:\n\t\t\t\t\tprogress = ExperimentRequestHandler.running[id]['progress']\n\t\t\t\telse:\n\t\t\t\t\tprogress = 100\n\n\t\t\t\tnum_bars = int(progress) * max_bars / 100\n\n\t\t\t\tself.wfile.write('<h2>Experiment</h2>')\n\t\t\t\tself.wfile.write('<table>')\n\t\t\t\tself.wfile.write('<tr><th>Experiment:</th><td>{0}</td></tr>'.format(\n\t\t\t\t\tos.path.join(instance['cwd'], instance['script_path'])))\n\t\t\t\tself.wfile.write('<tr><th>Results:</th><td>{0}</td></tr>'.format(\n\t\t\t\t\tos.path.join(instance['cwd'], instance['filename'])))\n\t\t\t\tself.wfile.write('<tr><th>Status:</th><td class=\"finished\">{0}</td></tr>'.format(instance['status']))\n\t\t\t\tself.wfile.write('<tr><th>Progress:</th><td class=\"progress\"><span class=\"bars\">{0}</span>{1}</td></tr>'.format(\n\t\t\t\t\t'|' * num_bars, '|' * (max_bars - num_bars)))\n\t\t\t\tself.wfile.write('<tr><th>Start:</th><td>{0}</td></tr>'.format(\n\t\t\t\t\tstrftime('%a, %d %b %Y %H:%M:%S', localtime(instance['time']))))\n\t\t\t\tself.wfile.write('<tr><th>End:</th><td>{0}</td></tr>'.format(\n\t\t\t\t\tstrftime('%a, %d %b %Y %H:%M:%S', localtime(instance['duration']))))\n\t\t\t\tself.wfile.write('<tr><th>Comment:</th><td>{0}</td></tr>'.format(\n\t\t\t\t\tinstance['comment']  if instance['comment'] else '-'))\n\t\t\t\tself.wfile.write('</table>')\n\n\t\t\t\tself.wfile.write('<h2>Results</h2>')\n\n\t\t\t\ttry:\n\t\t\t\t\texperiment = Experiment(os.path.join(instance['cwd'], instance['filename']))\n\t\t\t\texcept:\n\t\t\t\t\tself.wfile.write('Could not open file.')\n\t\t\t\telse:\n\t\t\t\t\tself.wfile.write('<table>')\n\t\t\t\t\tfor key, value in experiment.results.items():\n\t\t\t\t\t\tself.wfile.write('<tr><th>{0}</th><td>{1}</td></tr>'.format(key, value))\n\t\t\t\t\tself.wfile.write('</table>')\n\n\t\t\t\tself.wfile.write('<h2>Script</h2>')\n\t\t\t\tself.wfile.write('<pre>{0}</pre>'.format(instance['script']))\n\n\n\t\t\telse:\n\t\t\t\tself.wfile.write('<h2>404</h2>')\n\t\t\t\tself.wfile.write('Requested experiment not found.')\n\n\t\t\tself.wfile.write(HTML_FOOTER)\n\n\t\telse:\n\t\t\tfiles = []\n\n\t\t\tif 'xpck_path' in ExperimentRequestHandler.__dict__:\n\t\t\t\tif ExperimentRequestHandler.xpck_path != '':\n\t\t\t\t\tfor path in ExperimentRequestHandler.xpck_path.split(':'):\n\t\t\t\t\t\tfiles += [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.xpck')]", "documentation": "This method handles GET requests to the server to display a running or finished experiment .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "jonasrauber/c2s_c2s/experiment.py_7": {"code": "def do_POST(self):\n\t\tinstances = ExperimentRequestHandler.running\n\t\tinstance = eval(self.rfile.read(int(self.headers['Content-Length'])))", "documentation": "Read the file and run the instance .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "jonasrauber/c2s_c2s/experiment.py_8": {"code": "def find_class(self, module, name):\n\t\t\"\"\"\n\t\tHelps Unpickler to find certain Numpy modules.\n\t\t\"\"\"\n\n\t\ttry:\n\t\t\tnumpy_version = StrictVersion(numpy.__version__)\n\n\t\t\tif numpy_version >= '1.5.0':\n\t\t\t\tif module == 'numpy.core.defmatrix':\n\t\t\t\t\tmodule = 'numpy.matrixlib.defmatrix'\n\n\t\texcept ValueError:\n\t\t\tpass\n\n\t\treturn Unpickler.find_class(self, module, name)", "documentation": "A hack to allow Unpickler to use Numpy classes for matrix unpickling . Num", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "jonasrauber/c2s_c2s/experiment.py_9": {"code": "def main(argv):\n\t\"\"\"\n\tLoad and display experiment information.\n\t\"\"\"\n\n\tif len(argv) < 2:\n\t\tprint 'Usage:', argv[0], '[--server] [--port=<port>] [--path=<path>] [filename]'\n\t\treturn 0\n\n\toptlist, argv = getopt(argv[1:], '', ['server', 'port=', 'path='])\n\toptlist = dict(optlist)\n\n\tif '--server' in optlist:\n\t\ttry:\n\t\t\tExperimentRequestHandler.xpck_path = optlist.get('--path', '')\n\t\t\tport = optlist.get('--port', 8000)\n\n\t\t\t# start server\n\t\t\tserver = HTTPServer(('', port), ExperimentRequestHandler)\n\t\t\tserver.serve_forever()\n\n\t\texcept KeyboardInterrupt:\n\t\t\tserver.socket.close()\n\n\t\treturn 0\n\n\t# load experiment\n\texperiment = Experiment(sys.argv[1])\n\n\tif len(argv) > 1:\n\t\t# print arguments\n\t\tfor arg in argv[1:]:\n\t\t\ttry:\n\t\t\t\tprint experiment[arg]\n\t\t\texcept:\n\t\t\t\tprint experiment[int(arg)]\n\t\treturn 0\n\n\t# print summary of experiment\n\tprint experiment\n\n\treturn 0", "documentation": "Main function for running this script .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "mikhtonyuk/rxpython_concurrent/futures/cooperative/ensure_exception_handled.py_0": {"code": "def __init__(self, exc, handler):\n        self.exc = exc\n        self.hndl = handler\n        self.cls = type(exc)\n        self.tb = None", "documentation": "Initialize the exception object", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "mikhtonyuk/rxpython_concurrent/futures/cooperative/ensure_exception_handled.py_1": {"code": "def clear(self):\n        self.exc = None\n        self.tb = None", "documentation": "Clear traceback object", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "KevinJMcGrath/Symphony-Ares_modules/plugins/PABot/logging.py_0": {"code": "def LogPABotMessage(message):\n    _pabotlog.info(message)", "documentation": "Log a message from the PABot to the log file .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "nicorellius/pdxpixel_pdxpixel/core/mailgun.py_0": {"code": "def send_simple_message():\n    return requests.post(\n        \"https://api.mailgun.net/v3/sandbox049ff464a4d54974bb0143935f9577ef.mailgun.org/messages\",\n        auth=(\"api\", \"key-679dc79b890e700f11f001a6bf86f4a1\"),\n        data={\"from\": \"Mailgun Sandbox <postmaster@sandbox049ff464a4d54974bb0143935f9577ef.mailgun.org>\",\n              \"to\": \"nick <nicorellius@gmail.com>\",\n              \"subject\": \"Hello nick\",\n              \"text\": \"Congratulations nick, you just sent an email with Mailgun!  You are truly awesome!  You can see a record of this email in your logs: https://mailgun.com/cp/log .  You can send up to 300 emails/day from this sandbox server.  Next, you should add your own domain so you can send 10,000 emails/month for free.\"})", "documentation": "Send a simple message using the mailgun api .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "snfactory/cubefit_cubefit/main.py_0": {"code": "def snfpsf(wave, psfparams, header, psftype):\n    \"\"\"Create a 3-d PSF based on SNFactory-specific parameterization of\n    Gaussian + Moffat PSF parameters and ADR.\"\"\"\n\n    # Get Gaussian+Moffat parameters at each wavelength.\n    relwave = wave / REFWAVE - 1.0\n    ellipticity = abs(psfparams[0]) * np.ones_like(wave)\n    alpha = np.abs(psfparams[1] +\n                   psfparams[2] * relwave +\n                   psfparams[3] * relwave**2)\n\n    # correlated parameters (coefficients determined externally)\n    sigma = 0.545 + 0.215 * alpha  # Gaussian parameter\n    beta  = 1.685 + 0.345 * alpha  # Moffat parameter\n    eta   = 1.040 + 0.0   * alpha  # gaussian ampl. / moffat ampl.\n\n    # Atmospheric differential refraction (ADR): Because of ADR,\n    # the center of the PSF will be different at each wavelength,\n    # by an amount that we can determine (pretty well) from the\n    # atmospheric conditions and the pointing and angle of the\n    # instrument. We calculate the offsets here as a function of\n    # observation and wavelength and input these to the model.\n\n    # Correction to parallactic angle and airmass for 2nd-order effects\n    # such as MLA rotation, mechanical flexures or finite-exposure\n    # corrections. These values have been trained on faint-std star\n    # exposures.\n    #\n    # `predict_adr_params` uses 'AIRMASS', 'PARANG' and 'CHANNEL' keys\n    # in input dictionary.\n    delta, theta = Hyper_PSF3D_PL.predict_adr_params(header)\n\n    # check for crazy values of pressure and temperature, and assign default\n    # values.\n    pressure = header.get('PRESSURE', 617.)\n    if not 550. < pressure < 650.:\n        pressure = 617.\n    temp = header.get('TEMP', 2.)\n    if not -20. < temp < 20.:\n        temp = 2.\n\n    adr = ADR(pressure, temp, lref=REFWAVE, delta=delta, theta=theta)\n    adr_refract = adr.refract(0, 0, wave, unit=SPAXEL_SIZE)", "documentation": "Create a 3-d PSF based on SNFactory - specific parameterization of Gaussian + M", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "snfactory/cubefit_cubefit/main.py_1": {"code": "def setup_logging(loglevel, logfname=None):\n\n    # if loglevel isn't an integer, parse it as \"debug\", \"info\", etc:\n    if not isinstance(loglevel, int):\n        loglevel = getattr(logging, loglevel.upper(), None)\n    if not isinstance(loglevel, int):\n        print('Invalid log level: %s' % loglevel)\n        exit(1)\n\n    # remove logfile if it already exists\n    if logfname is not None and os.path.exists(logfname):\n        os.remove(logfname)\n\n    logging.basicConfig(filename=logfname, format=\"%(levelname)s %(message)s\",\n                        level=loglevel)", "documentation": "Set up logging with a loglevel of DEBUG or INFO .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "snfactory/cubefit_cubefit/main.py_2": {"code": "def cubefit_subtract(argv=None):\n    DESCRIPTION = \\", "documentation": "Subtract a cube from a cube of 2D images .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "vicenteneto/online-judge-solutions_URI/1-Beginner/1021.py_0": {"code": "def calc_note(count, value):\n    qnt = 0\n    if count >= value:\n        qnt = int(count) / value\n    print '%d nota(s) de R$ %d.00' % (qnt, value)\n    return count - qnt * value", "documentation": "Calculates the number of notes in a range of a given value", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_0": {"code": "def index(request, **kwargs):\n    if request.method == \"GET\" and request.subdomain and request.subdomain not in ['dev', 'www', 'debug']:\n        username = request.subdomain\n        try:\n            if '.' in username:\n                username = username.split('.')[0]\n            user = User.objects.get(username__iexact=username)\n        except User.DoesNotExist:\n            return HttpResponseRedirect('http://%s%s' % (\n                Site.objects.get_current().domain,\n                reverse('index')))\n        return load_social_page(request, user_id=user.pk, username=request.subdomain, **kwargs)\n\n    if request.user.is_anonymous():\n        return welcome(request, **kwargs)\n    else:\n        return dashboard(request, **kwargs)", "documentation": "Handles GET requests and redirects to the social index page .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_1": {"code": "def welcome(request, **kwargs):\n    user              = get_user(request)\n    statistics        = MStatistics.all()\n    social_profile    = MSocialProfile.get_user(user.pk)", "documentation": "welcome user to the site", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_2": {"code": "def login(request):\n    code = -1\n    message = \"\"\n    if request.method == \"POST\":\n        form = LoginForm(request.POST, prefix='login')\n        if form.is_valid():\n            login_user(request, form.get_user())\n            if request.POST.get('api'):\n                logging.user(form.get_user(), \"~FG~BB~SKiPhone Login~FW\")\n                code = 1\n            else:\n                logging.user(form.get_user(), \"~FG~BBLogin~FW\")\n                return HttpResponseRedirect(reverse('index'))\n        else:\n            message = form.errors.items()[0][1][0]\n\n    if request.POST.get('api'):\n        return HttpResponse(json.encode(dict(code=code, message=message)), mimetype='application/json')\n    else:\n        return index(request)", "documentation": "Login to the user and return JSON .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_3": {"code": "def signup(request):\n    if request.method == \"POST\":\n        form = SignupForm(prefix='signup', data=request.POST)\n        if form.is_valid():\n            new_user = form.save()\n            login_user(request, new_user)\n            logging.user(new_user, \"~FG~SB~BBNEW SIGNUP: ~FW%s\" % new_user.email)\n            if not new_user.is_active:\n                url = \"https://%s%s\" % (Site.objects.get_current().domain,\n                                         reverse('stripe-form'))\n                return HttpResponseRedirect(url)", "documentation": "Handle signup process .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_4": {"code": "def logout(request):\n    logging.user(request, \"~FG~BBLogout~FW\")\n    logout_user(request)", "documentation": "Logout of user logged - in", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_5": {"code": "def autologin(request, username, secret):\n    next = request.GET.get('next', '')", "documentation": "Autologin using next param in GET .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_6": {"code": "def load_feeds(request):\n    user             = get_user(request)\n    feeds            = {}\n    include_favicons = request.REQUEST.get('include_favicons', False)\n    flat             = request.REQUEST.get('flat', False)\n    update_counts    = request.REQUEST.get('update_counts', False)\n    version          = int(request.REQUEST.get('v', 1))", "documentation": "Loads user feeds from request", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_7": {"code": "def load_feed_favicons(request):\n    user = get_user(request)\n    feed_ids = request.REQUEST.getlist('feed_ids')", "documentation": "Load favicons for feeds", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_8": {"code": "def load_feeds_flat(request):\n    user = request.user\n    include_favicons = is_true(request.REQUEST.get('include_favicons', False))\n    update_counts    = is_true(request.REQUEST.get('update_counts', True))", "documentation": "Loads flatfeeds from the request", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_9": {"code": "def refresh_feeds(request):\n    user = get_user(request)\n    feed_ids = request.REQUEST.getlist('feed_id')\n    check_fetch_status = request.REQUEST.get('check_fetch_status')\n    favicons_fetching = request.REQUEST.getlist('favicons_fetching')\n    social_feed_ids = [feed_id for feed_id in feed_ids if 'social:' in feed_id]\n    feed_ids = list(set(feed_ids) - set(social_feed_ids))", "documentation": "Refreshes the feeds submitted by the user .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_10": {"code": "def interactions_count(request):\n    user = get_user(request)\n\n    interactions_count = MInteraction.user_unread_count(user.pk)\n\n    return {\n        'interactions_count': interactions_count,\n    }", "documentation": "get the number of unread interactions the user has read the interactions", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_11": {"code": "def feed_unread_count(request):\n    user = request.user\n    feed_ids = request.REQUEST.getlist('feed_id')\n    force = request.REQUEST.get('force', False)\n    social_feed_ids = [feed_id for feed_id in feed_ids if 'social:' in feed_id]\n    feed_ids = list(set(feed_ids) - set(social_feed_ids))", "documentation": "Returns the number of unread feeds for a given user", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_12": {"code": "def refresh_feed(request, feed_id):\n    user = get_user(request)\n    feed = get_object_or_404(Feed, pk=feed_id)", "documentation": "Refresh a feed by ID .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_13": {"code": "def load_single_feed(request, feed_id):\n    start                   = time.time()\n    user                    = get_user(request)\n    # offset                  = int(request.REQUEST.get('offset', 0))\n    # limit                   = int(request.REQUEST.get('limit', 6))\n    limit                   = 6\n    page                    = int(request.REQUEST.get('page', 1))\n    offset                  = limit * (page-1)\n    order                   = request.REQUEST.get('order', 'newest')\n    read_filter             = request.REQUEST.get('read_filter', 'all')\n    query                   = request.REQUEST.get('query')\n    include_story_content   = is_true(request.REQUEST.get('include_story_content', True))\n    include_hidden          = is_true(request.REQUEST.get('include_hidden', False))\n    message                 = None\n    user_search             = None\n\n    dupe_feed_id = None\n    user_profiles = []\n    now = localtime_for_timezone(datetime.datetime.now(), user.profile.timezone)\n    if not feed_id: raise Http404\n\n    feed_address = request.REQUEST.get('feed_address')\n    feed = Feed.get_by_id(feed_id, feed_address=feed_address)\n    if not feed:\n        raise Http404", "documentation": "Loads a single feed by feed_id", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_14": {"code": "def load_feed_page(request, feed_id):\n    if not feed_id:\n        raise Http404", "documentation": "Load feed page with given id .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_15": {"code": "def load_starred_stories(request):\n    user         = get_user(request)\n    offset       = int(request.REQUEST.get('offset', 0))\n    limit        = int(request.REQUEST.get('limit', 10))\n    page         = int(request.REQUEST.get('page', 0))\n    query        = request.REQUEST.get('query')\n    order        = request.REQUEST.get('order', 'newest')\n    tag          = request.REQUEST.get('tag')\n    story_hashes = request.REQUEST.getlist('h')[:100]\n    version      = int(request.REQUEST.get('v', 1))\n    now          = localtime_for_timezone(datetime.datetime.now(), user.profile.timezone)\n    message      = None\n    order_by     = '-' if order == \"newest\" else \"\"\n    if page: offset = limit * (page - 1)", "documentation": "Load starred stories", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_16": {"code": "def starred_story_hashes(request):\n    user               = get_user(request)\n    include_timestamps = is_true(request.REQUEST.get('include_timestamps', False))", "documentation": "Returns a list of hash hashes of starred stories for the user .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_17": {"code": "def starred_stories_rss_feed(request, user_id, secret_token, tag_slug):\n    try:\n        user = User.objects.get(pk=user_id)\n    except User.DoesNotExist:\n        raise Http404", "documentation": "starred stories RSS Feed with meta information", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_18": {"code": "def load_read_stories(request):\n    user   = get_user(request)\n    offset = int(request.REQUEST.get('offset', 0))\n    limit  = int(request.REQUEST.get('limit', 10))\n    page   = int(request.REQUEST.get('page', 0))\n    order  = request.REQUEST.get('order', 'newest')\n    query  = request.REQUEST.get('query')\n    now    = localtime_for_timezone(datetime.datetime.now(), user.profile.timezone)\n    message = None\n    if page: offset = limit * (page - 1)", "documentation": "Loads read stories", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_19": {"code": "def load_river_stories__redis(request):\n    limit             = 12\n    start             = time.time()\n    user              = get_user(request)\n    message           = None\n    feed_ids          = [int(feed_id) for feed_id in request.REQUEST.getlist('feeds') if feed_id]\n    if not feed_ids:\n        feed_ids      = [int(feed_id) for feed_id in request.REQUEST.getlist('f') if feed_id]\n    story_hashes      = request.REQUEST.getlist('h')[:100]\n    original_feed_ids = list(feed_ids)\n    page              = int(request.REQUEST.get('page', 1))\n    order             = request.REQUEST.get('order', 'newest')\n    read_filter       = request.REQUEST.get('read_filter', 'unread')\n    query             = request.REQUEST.get('query')\n    include_hidden    = is_true(request.REQUEST.get('include_hidden', False))\n    now               = localtime_for_timezone(datetime.datetime.now(), user.profile.timezone)\n    usersubs          = []\n    code              = 1\n    user_search       = None\n    offset = (page-1) * limit\n    limit = page * limit\n    story_date_order = \"%sstory_date\" % ('' if order == 'oldest' else '-')", "documentation": "Returns a list of the latest story for each given feed ID", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_20": {"code": "def unread_story_hashes__old(request):\n    user              = get_user(request)\n    feed_ids          = [int(feed_id) for feed_id in request.REQUEST.getlist('feed_id') if feed_id]\n    include_timestamps = is_true(request.REQUEST.get('include_timestamps', False))\n    usersubs = {}", "documentation": "Unreads the old story hashes from the given feed", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_21": {"code": "def unread_story_hashes(request):\n    user               = get_user(request)\n    feed_ids           = [int(feed_id) for feed_id in request.REQUEST.getlist('feed_id') if feed_id]\n    include_timestamps = is_true(request.REQUEST.get('include_timestamps', False))\n    order              = request.REQUEST.get('order', 'newest')\n    read_filter        = request.REQUEST.get('read_filter', 'unread')", "documentation": "Reads unread story hashes from a given feed", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_22": {"code": "def mark_all_as_read(request):\n    code = 1\n    try:\n        days = int(request.REQUEST.get('days', 0))\n    except ValueError:\n        return dict(code=-1, message=\"Days parameter must be an integer, not: %s\" %\n                    request.REQUEST.get('days'))\n    read_date = datetime.datetime.utcnow() - datetime.timedelta(days=days)", "documentation": "Mark all as read .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_23": {"code": "def mark_story_as_read(request):\n    story_ids = request.REQUEST.getlist('story_id')\n    try:\n        feed_id = int(get_argument_or_404(request, 'feed_id'))\n    except ValueError:\n        return dict(code=-1, errors=[\"You must pass a valid feed_id: %s\" %\n                                     request.REQUEST.get('feed_id')])", "documentation": "Marks a story as read in a given feed", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_24": {"code": "def mark_story_hashes_as_read(request):\n    r = redis.Redis(connection_pool=settings.REDIS_PUBSUB_POOL)\n    story_hashes = request.REQUEST.getlist('story_hash')", "documentation": "Marks given story hashes as read .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_25": {"code": "def mark_feed_stories_as_read(request):\n    r = redis.Redis(connection_pool=settings.REDIS_PUBSUB_POOL)\n    feeds_stories = request.REQUEST.get('feeds_stories', \"{}\")\n    feeds_stories = json.decode(feeds_stories)\n    data = {\n        'code': -1,\n        'message': 'Nothing was marked as read'\n    }", "documentation": "Marks all feeds_stories as read in the request body", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_26": {"code": "def mark_social_stories_as_read(request):\n    code = 1\n    errors = []\n    data = {}\n    r = redis.Redis(connection_pool=settings.REDIS_PUBSUB_POOL)\n    users_feeds_stories = request.REQUEST.get('users_feeds_stories', \"{}\")\n    users_feeds_stories = json.decode(users_feeds_stories)\n\n    for social_user_id, feeds in users_feeds_stories.items():\n        for feed_id, story_ids in feeds.items():\n            feed_id = int(feed_id)\n            try:\n                socialsub = MSocialSubscription.objects.get(user_id=request.user.pk, \n                                                            subscription_user_id=social_user_id)\n                data = socialsub.mark_story_ids_as_read(story_ids, feed_id, request=request)\n            except OperationError, e:\n                code = -1\n                errors.append(\"Already read story: %s\" % e)\n            except MSocialSubscription.DoesNotExist:\n                MSocialSubscription.mark_unsub_story_ids_as_read(request.user.pk, social_user_id,\n                                                                 story_ids, feed_id,\n                                                                 request=request)\n            except Feed.DoesNotExist:\n                duplicate_feed = DuplicateFeed.objects.filter(duplicate_feed_id=feed_id)\n                if duplicate_feed:\n                    try:\n                        socialsub = MSocialSubscription.objects.get(user_id=request.user.pk,\n                                                                    subscription_user_id=social_user_id)\n                        data = socialsub.mark_story_ids_as_read(story_ids, duplicate_feed[0].feed.pk, request=request)\n                    except (UserSubscription.DoesNotExist, Feed.DoesNotExist):\n                        code = -1\n                        errors.append(\"No feed exists for feed_id %d.\" % feed_id)\n                else:\n                    continue\n            r.publish(request.user.username, 'feed:%s' % feed_id)\n        r.publish(request.user.username, 'social:%s' % social_user_id)\n\n    data.update(code=code, errors=errors)\n    return data", "documentation": "mark stories as read for a social user", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_27": {"code": "def mark_story_as_unread(request):\n    story_id = request.REQUEST.get('story_id', None)\n    feed_id = int(request.REQUEST.get('feed_id', 0))", "documentation": "Marks a story as unread .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_28": {"code": "def mark_story_hash_as_unread(request):\n    r = redis.Redis(connection_pool=settings.REDIS_PUBSUB_POOL)\n    story_hash = request.REQUEST.get('story_hash')\n    feed_id, _ = MStory.split_story_hash(story_hash)\n    story, _ = MStory.find_story(feed_id, story_hash)\n    if not story:\n        data = dict(code=-1, message=\"That story has been removed from the feed, no need to mark it unread.\")\n        return data        \n    message = RUserStory.story_can_be_marked_read_by_user(story, request.user)\n    if message:\n        data = dict(code=-1, message=message)\n        return data", "documentation": "mark a story as unread by the user", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_29": {"code": "def mark_feed_as_read(request):\n    r = redis.Redis(connection_pool=settings.REDIS_PUBSUB_POOL)\n    feed_ids = request.REQUEST.getlist('feed_id')\n    cutoff_timestamp = int(request.REQUEST.get('cutoff_timestamp', 0))\n    direction = request.REQUEST.get('direction', 'older')\n    multiple = len(feed_ids) > 1\n    code = 1\n    errors = []\n    cutoff_date = datetime.datetime.fromtimestamp(cutoff_timestamp) if cutoff_timestamp else None", "documentation": "Marks a single feed as read based on the request parameters", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_30": {"code": "def _parse_user_info(user):\n    return {\n        'user_info': {\n            'is_anonymous': json.encode(user.is_anonymous()),\n            'is_authenticated': json.encode(user.is_authenticated()),\n            'username': json.encode(user.username if user.is_authenticated() else 'Anonymous')\n        }\n    }", "documentation": "Parses the user info from the request .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_31": {"code": "def add_url(request):\n    code = 0\n    url = request.POST['url']\n    folder = request.POST.get('folder', '')\n    new_folder = request.POST.get('new_folder')\n    auto_active = is_true(request.POST.get('auto_active', 1))\n    skip_fetch = is_true(request.POST.get('skip_fetch', False))\n    feed = None", "documentation": "Add a URL to a folder .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_32": {"code": "def add_folder(request):\n    folder = request.POST['folder']\n    parent_folder = request.POST.get('parent_folder', '')\n    folders = None\n    logging.user(request, \"~FRAdding Folder: ~SB%s (in %s)\" % (folder, parent_folder))", "documentation": "Add a folder to the folder list", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_33": {"code": "def delete_feed(request):\n    feed_id = int(request.POST['feed_id'])\n    in_folder = request.POST.get('in_folder', None)\n    if not in_folder or in_folder == ' ':\n        in_folder = \"\"", "documentation": "Delete a feed by POSTing the feed_id in the POST variable feed_id . The", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_34": {"code": "def delete_feed_by_url(request):\n    message = \"\"\n    code = 0\n    url = request.POST['url']\n    in_folder = request.POST.get('in_folder', '')\n    if in_folder == ' ':\n        in_folder = \"\"", "documentation": "Delete a feed by URL", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_35": {"code": "def delete_folder(request):\n    folder_to_delete = request.POST.get('folder_name') or request.POST.get('folder_to_delete')\n    in_folder = request.POST.get('in_folder', None)\n    feed_ids_in_folder = [int(f) for f in request.REQUEST.getlist('feed_id') if f]\n\n    request.user.profile.send_opml_export_email(reason=\"You have deleted an entire folder of feeds, so here's a backup just in case.\")", "documentation": "Deletes a folder and all its contents", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_36": {"code": "def delete_feeds_by_folder(request):\n    feeds_by_folder = json.decode(request.POST['feeds_by_folder'])\n\n    request.user.profile.send_opml_export_email(reason=\"You have deleted a number of feeds at once, so here's a backup just in case.\")", "documentation": "Deletes a list of feeds from a folder", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_37": {"code": "def rename_feed(request):\n    feed = get_object_or_404(Feed, pk=int(request.POST['feed_id']))\n    user_sub = UserSubscription.objects.get(user=request.user, feed=feed)\n    feed_title = request.POST['feed_title']", "documentation": "Rename a feed", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_38": {"code": "def rename_folder(request):\n    folder_to_rename = request.POST.get('folder_name') or request.POST.get('folder_to_rename')\n    new_folder_name = request.POST['new_folder_name']\n    in_folder = request.POST.get('in_folder', '')\n    code = 0", "documentation": "Rename a folder and return the response", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_39": {"code": "def move_feed_to_folders(request):\n    feed_id = int(request.POST['feed_id'])\n    in_folders = request.POST.getlist('in_folders', '')\n    to_folders = request.POST.getlist('to_folders', '')\n\n    user_sub_folders = get_object_or_404(UserSubscriptionFolders, user=request.user)\n    user_sub_folders = user_sub_folders.move_feed_to_folders(feed_id, in_folders=in_folders,\n                                                             to_folders=to_folders)", "documentation": "Moves a feed to a different folder .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_40": {"code": "def move_feed_to_folder(request):\n    feed_id = int(request.POST['feed_id'])\n    in_folder = request.POST.get('in_folder', '')\n    to_folder = request.POST.get('to_folder', '')\n\n    user_sub_folders = get_object_or_404(UserSubscriptionFolders, user=request.user)\n    user_sub_folders = user_sub_folders.move_feed_to_folder(feed_id, in_folder=in_folder,\n                                                            to_folder=to_folder)", "documentation": "Moves the given feed to a folder", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_41": {"code": "def move_folder_to_folder(request):\n    folder_name = request.POST['folder_name']\n    in_folder = request.POST.get('in_folder', '')\n    to_folder = request.POST.get('to_folder', '')", "documentation": "Moves a folder to another folder", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_42": {"code": "def move_feeds_by_folder_to_folder(request):\n    feeds_by_folder = json.decode(request.POST['feeds_by_folder'])\n    to_folder = request.POST['to_folder']\n    new_folder = request.POST.get('new_folder', None)\n\n    request.user.profile.send_opml_export_email(reason=\"You have moved a number of feeds at once, so here's a backup just in case.\")", "documentation": "Move feeds by folder to folder", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_43": {"code": "def add_feature(request):\n    if not request.user.is_staff:\n        return HttpResponseForbidden()\n\n    code = -1    \n    form = FeatureForm(request.POST)", "documentation": "Add a feature to a staff feature .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_44": {"code": "def load_features(request):\n    user = get_user(request)\n    page = max(int(request.REQUEST.get('page', 0)), 0)\n    logging.user(request, \"~FBBrowse features: ~SBPage #%s\" % (page+1))\n    features = Feature.objects.all()[page*3:(page+1)*3+1].values()\n    features = [{\n        'description': f['description'], \n        'date': localtime_for_timezone(f['date'], user.profile.timezone).strftime(\"%b %d, %Y\")\n    } for f in features]\n    return features", "documentation": "Load all features .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_45": {"code": "def save_feed_order(request):\n    folders = request.POST.get('folders')\n    if folders:\n        # Test that folders can be JSON decoded\n        folders_list = json.decode(folders)\n        assert folders_list is not None\n        logging.user(request, \"~FBFeed re-ordering: ~SB%s folders/feeds\" % (len(folders_list)))\n        user_sub_folders = UserSubscriptionFolders.objects.get(user=request.user)\n        user_sub_folders.folders = folders\n        user_sub_folders.save()", "documentation": "Re - order the feeds", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_46": {"code": "def feeds_trainer(request):\n    classifiers = []\n    feed_id = request.REQUEST.get('feed_id')\n    user = get_user(request)\n    usersubs = UserSubscription.objects.filter(user=user, active=True)", "documentation": "Returns a list of FeedClassifiers for the given feed_id", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_47": {"code": "def save_feed_chooser(request):\n    is_premium = request.user.profile.is_premium\n    approved_feeds = [int(feed_id) for feed_id in request.POST.getlist('approved_feeds') if feed_id]\n    if not is_premium:\n        approved_feeds = approved_feeds[:64]\n    activated = 0\n    usersubs = UserSubscription.objects.filter(user=request.user)", "documentation": "Saves a choice of feeds to the database . The user will be activated if the user", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_48": {"code": "def retrain_all_sites(request):\n    for sub in UserSubscription.objects.filter(user=request.user):\n        sub.is_trained = False\n        sub.save()", "documentation": "Set is_trained to False for all sites that are not trained", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_49": {"code": "def activate_premium_account(request):\n    try:\n        usersubs = UserSubscription.objects.select_related('feed').filter(user=request.user)\n        for sub in usersubs:\n            sub.active = True\n            sub.save()\n            if sub.feed.premium_subscribers <= 0:\n                sub.feed.count_subscribers()\n                sub.feed.schedule_feed_fetch_immediately()\n    except Exception, e:\n        subject = \"Premium activation failed\"\n        message = \"%s -- %s\\n\\n%s\" % (request.user, usersubs, e)\n        mail_admins(subject, message, fail_silently=True)", "documentation": "Activate premium account for the authenticated user", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_50": {"code": "def login_as(request):\n    if not request.user.is_staff:\n        logging.user(request, \"~SKNON-STAFF LOGGING IN AS ANOTHER USER!\")\n        assert False\n        return HttpResponseForbidden()\n    username = request.GET['user']\n    user = get_object_or_404(User, username__iexact=username)\n    user.backend = settings.AUTHENTICATION_BACKENDS[0]\n    login_user(request, user)\n    return HttpResponseRedirect(reverse('index'))", "documentation": "Logs the user in as the specified user .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_51": {"code": "def iframe_buster(request):\n    logging.user(request, \"~FB~SBiFrame bust!\")\n    return HttpResponse(status=204)", "documentation": "iframe_buster is a Django view that will cause the browser to load all frames manually .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_52": {"code": "def mark_story_as_starred(request):\n    return _mark_story_as_starred(request)", "documentation": "mark the given story as starred by the authenticated user .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_53": {"code": "def mark_story_hash_as_starred(request):\n    return _mark_story_as_starred(request)", "documentation": "Used in star listings to indicate a story hash was starred .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_54": {"code": "def _mark_story_as_starred(request):\n    code       = 1\n    feed_id    = int(request.REQUEST.get('feed_id', 0))\n    story_id   = request.REQUEST.get('story_id', None)\n    story_hash = request.REQUEST.get('story_hash', None)\n    user_tags  = request.REQUEST.getlist('user_tags')\n    message    = \"\"\n    if story_hash:\n        story, _   = MStory.find_story(story_hash=story_hash)\n        feed_id = story and story.story_feed_id\n    else:\n        story, _   = MStory.find_story(story_feed_id=feed_id, story_id=story_id)", "documentation": "mark a story as starred", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_55": {"code": "def mark_story_as_unstarred(request):\n    return _mark_story_as_unstarred(request)", "documentation": "mark the given story as unstarred", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_56": {"code": "def mark_story_hash_as_unstarred(request):\n    return _mark_story_as_unstarred(request)", "documentation": "mark the given hash as unstarred", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "slava-sh/NewsBlur_apps/reader/views.py_57": {"code": "def send_story_email(request):\n    code       = 1\n    message    = 'OK'\n    story_id   = request.POST['story_id']\n    feed_id    = request.POST['feed_id']\n    to_addresses = request.POST.get('to', '').replace(',', ' ').replace('  ', ' ').strip().split(' ')\n    from_name  = request.POST['from_name']\n    from_email = request.POST['from_email']\n    email_cc   = is_true(request.POST.get('email_cc', 'true'))\n    comments   = request.POST['comments']\n    comments   = comments[:2048] # Separated due to PyLint\n    from_address = 'share@newsblur.com'\n    share_user_profile = MSocialProfile.get_user(request.user.pk)\n\n    if not to_addresses:\n        code = -1\n        message = 'Please provide at least one email address.'\n    elif not all(email_re.match(to_address) for to_address in to_addresses if to_addresses):\n        code = -1\n        message = 'You need to send the email to a valid email address.'\n    elif not email_re.match(from_email):\n        code = -1\n        message = 'You need to provide your email address.'\n    elif not from_name:\n        code = -1\n        message = 'You need to provide your name.'\n    else:\n        story, _ = MStory.find_story(feed_id, story_id)\n        story   = Feed.format_story(story, feed_id, text=True)\n        feed    = Feed.get_by_id(story['story_feed_id'])\n        params  = {\n            \"to_addresses\": to_addresses,\n            \"from_name\": from_name,\n            \"from_email\": from_email,\n            \"email_cc\": email_cc,\n            \"comments\": comments,\n            \"from_address\": from_address,\n            \"story\": story,\n            \"feed\": feed,\n            \"share_user_profile\": share_user_profile,\n        }\n        text    = render_to_string('mail/email_story.txt', params)\n        html    = render_to_string('mail/email_story.xhtml', params)\n        subject = '%s' % (story['story_title'])\n        cc      = None\n        if email_cc:\n            cc = ['%s <%s>' % (from_name, from_email)]\n        subject = subject.replace('\\n', ' ')\n        msg     = EmailMultiAlternatives(subject, text, \n                                         from_email='NewsBlur <%s>' % from_address,\n                                         to=to_addresses, \n                                         cc=cc,\n                                         headers={'Reply-To': '%s <%s>' % (from_name, from_email)})\n        msg.attach_alternative(html, \"text/html\")\n        try:\n            msg.send()\n        except boto.ses.connection.ResponseError, e:\n            code = -1\n            message = \"Email error: %s\" % str(e)\n        logging.user(request, '~BMSharing story by email to %s recipient%s: ~FY~SB%s~SN~BM~FY/~SB%s' % \n                              (len(to_addresses), '' if len(to_addresses) == 1 else 's', \n                               story['story_title'][:50], feed and feed.feed_title[:50]))", "documentation": "Sends an email to the recipient list of the story .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "kif/freesas_freesas/average.py_0": {"code": "def __init__(self, inputfiles):\n        \"\"\"\n        :param inputfiles: list of pdb files needed for averaging\n        \"\"\"\n        self.inputs = inputfiles\n        self.size = []\n        self.nbknots = None\n        self.radius = None\n        self.coordknots = []", "documentation": "A class method for averaging PDB results from a list of input PDB files .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "kif/freesas_freesas/average.py_1": {"code": "def spatial_extent(self):\n        \"\"\"\n        Calculate the maximal extent of input models", "documentation": "The maximum extent of the input models .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "kif/freesas_freesas/average.py_2": {"code": "def calc_radius(self, nbknots=None):\n        \"\"\"\n        Calculate the radius of each point of a hexagonal close-packed grid, \n        knowing the total volume and the number of knots in this grid.\n\n        :param nbknots: number of knots wanted for the grid\n        :return radius: the radius of each knot of the grid\n        \"\"\"\n        if len(self.size)==0:\n            self.spatial_extent()\n        nbknots = nbknots if nbknots is not None else 5000\n        size = self.size\n        dx = size[0] - size[3]\n        dy = size[1] - size[4]\n        dz = size[2] - size[5]\n        volume = dx * dy * dz\n\n        density = numpy.pi / (3*2**0.5)\n        radius = ((3 /( 4 * numpy.pi)) * density * volume / nbknots)**(1.0/3)\n        self.radius = radius\n\n        return radius", "documentation": "Calculate the radius of each point of a hexagonal grid .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "kif/freesas_freesas/average.py_3": {"code": "def __init__(self, inputfiles, grid):\n        \"\"\"\n        :param inputfiles: list of pdb files of aligned models\n        :param grid: 2d-array coordinates of each point of a grid, fourth column full of zeros\n        \"\"\"\n        self.inputfiles = inputfiles\n        self.models = []\n        self.header = []\n        self.radius = None\n        self.atoms = []\n        self.grid = grid", "documentation": "A class to store the model information for later use .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "kif/freesas_freesas/average.py_4": {"code": "def read_files(self, reference=None):\n        \"\"\"\n        Read all the pdb file in the inputfiles list, creating SASModels.\n        The SASModels created are save in a list, the reference model is the first model in the list.\n\n        :param reference: position of the reference model file in the inputfiles list\n        \"\"\"\n        ref = reference if reference is not None else 0\n        inputfiles = self.inputfiles\n\n        models = []\n        models.append(SASModel(inputfiles[ref]))\n        for i in range(len(inputfiles)):\n            if i==ref:\n                continue\n            else:\n                models.append(SASModel(inputfiles[i]))\n        self.models = models\n\n        return models", "documentation": "Read all the pdb file in the inputfiles list creating SASModels . The SASModels", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "kif/freesas_freesas/average.py_5": {"code": "def assign_occupancy(self):\n        \"\"\"\n        For each point of the grid, total occupancy and contribution factor are computed and saved.\n        The grid is then ordered with decreasing value of occupancy.\n        The fourth column of the array correspond to the occupancy of the point and the fifth to \n        the contribution for this point.\n\n        :return sortedgrid: 2d-array, coordinates of each point of the grid\n        \"\"\"\n        grid = self.grid\n        nbknots = grid.shape[0]\n        grid = numpy.append(grid, numpy.zeros((nbknots, 1), dtype=\"float\"), axis=1)\n\n        for i in range(nbknots):\n            occ, contrib = self.calc_occupancy(grid[i, 0:3])\n            grid[i, 3] = occ\n            grid[i, 4] = contrib\n\n        order = numpy.argsort(grid, axis=0)[:, -2]\n        sortedgrid = numpy.empty_like(grid)\n        for i in range(nbknots):\n            sortedgrid[nbknots - i - 1, :] = grid[order[i], :]\n\n        return sortedgrid", "documentation": "Assigns the occupancy to each point of the grid", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "swcarpentry/amy_amy/workshops/management/commands/instructors_activity.py_0": {"code": "def add_arguments(self, parser):\n        parser.add_argument(\n            '--send-out-for-real', action='store_true', default=False,\n            help='Send information to the instructors.',\n        )\n        parser.add_argument(\n            '--no-may-contact-only', action='store_true', default=False,\n            help='Include instructors not willing to be contacted.',\n        )\n        parser.add_argument(\n            '--django-mailing', action='store_true', default=False,\n            help='Use Django mailing system. This requires some environmental '\n                 'variables to be set, see `settings.py`.',\n        )\n        parser.add_argument(\n            '-s', '--sender', action='store',\n            default='workshops@carpentries.org',\n            help='E-mail used in \"from:\" field.',\n        )", "documentation": "Add the arguments for the script", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "swcarpentry/amy_amy/workshops/management/commands/instructors_activity.py_1": {"code": "def fetch_activity(self, may_contact_only=True):\n        roles = Role.objects.filter(name__in=['instructor', 'helper'])\n        instructor_badges = Badge.objects.instructor_badges()\n\n        instructors = Person.objects.filter(badges__in=instructor_badges)\n        instructors = instructors.exclude(email__isnull=True)\n        if may_contact_only:\n            instructors = instructors.exclude(may_contact=False)\n\n        # let's get some things faster\n        instructors = instructors.select_related('airport') \\\n                                 .prefetch_related('task_set', 'lessons',\n                                                   'award_set', 'badges')\n\n        # don't repeat the records\n        instructors = instructors.distinct()\n\n        result = []\n        for person in instructors:\n            tasks = person.task_set.filter(role__in=roles) \\\n                                   .select_related('event', 'role')\n            record = {\n                'person': person,\n                'lessons': person.lessons.all(),\n                'instructor_awards': person.award_set.filter(\n                    badge__in=person.badges.instructor_badges()\n                ),\n                'tasks': zip(tasks,\n                             self.foreign_tasks(tasks, person, roles)),\n            }\n            result.append(record)\n\n        return result", "documentation": "Fetches all instructors and their related activities .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "swcarpentry/amy_amy/workshops/management/commands/instructors_activity.py_2": {"code": "def subject(self, record):\n        # in future we can vary the subject depending on the record details\n        return 'Updating your Software Carpentry information'", "documentation": "Populate the subject line", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "swcarpentry/amy_amy/workshops/management/commands/instructors_activity.py_3": {"code": "def send_message(self, subject, message, sender, recipient, for_real=False,\n                     django_mailing=False):\n        if for_real:\n            if django_mailing:\n                send_mail(subject, message, sender, [recipient])\n\n            else:\n                command = 'mail -s \"{subject}\" -r {sender} {recipient}'.format(\n                    subject=subject,\n                    sender=sender,\n                    recipient=recipient,\n                )\n\n                writer = os.popen(command, 'w')\n                writer.write(message)\n                writer.close()\n\n        if self.verbosity >= 2:\n            # write only a header\n            self.stdout.write('-' * 40 + '\\n')\n            self.stdout.write('To: {}\\n'.format(recipient))\n            self.stdout.write('Subject: {}\\n'.format(subject))\n            self.stdout.write('From: {}\\n'.format(sender))\n        if self.verbosity >= 3:\n            # write whole message out\n            self.stdout.write(message + '\\n')", "documentation": "Sends a message via mail .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_0": {"code": "def build_delete_request(\n    scope: str,\n    policy_assignment_name: str,\n    **kwargs: Any", "documentation": "build DELETE request", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_1": {"code": "def build_create_request(\n    scope: str,\n    policy_assignment_name: str,\n    *,\n    json: JSONType = None,\n    content: Any = None,\n    **kwargs: Any", "documentation": "build create request", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_2": {"code": "def build_get_request(\n    scope: str,\n    policy_assignment_name: str,\n    **kwargs: Any", "documentation": "build GET request", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_3": {"code": "def build_list_for_resource_group_request(\n    resource_group_name: str,\n    subscription_id: str,\n    *,\n    filter: Optional[str] = None,\n    **kwargs: Any", "documentation": "Builds a list for a resource group and subscription .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_4": {"code": "def build_list_for_resource_request(\n    resource_group_name: str,\n    resource_provider_namespace: str,\n    parent_resource_path: str,\n    resource_type: str,\n    resource_name: str,\n    subscription_id: str,\n    *,\n    filter: Optional[str] = None,\n    **kwargs: Any", "documentation": "Builds a list for a resource type and a resource name with the given parameters", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_5": {"code": "def build_list_request(\n    subscription_id: str,\n    *,\n    filter: Optional[str] = None,\n    **kwargs: Any", "documentation": "Builds a list request from the given kwargs .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_6": {"code": "def build_delete_by_id_request(\n    policy_assignment_id: str,\n    **kwargs: Any", "documentation": "Build a delete by id request .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_7": {"code": "def build_create_by_id_request(\n    policy_assignment_id: str,\n    *,\n    json: JSONType = None,\n    content: Any = None,\n    **kwargs: Any", "documentation": "Builds a POST request for the create by Id endpoint", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_8": {"code": "def build_get_by_id_request(\n    policy_assignment_id: str,\n    **kwargs: Any", "documentation": "Builds a get_by_id_request object .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_9": {"code": "def __init__(self, client, config, serializer, deserializer):\n        self._client = client\n        self._serialize = serializer\n        self._deserialize = deserializer\n        self._config = config", "documentation": "Initializes the serializer and deserializer for a client instance .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_10": {"code": "def delete(\n        self,\n        scope: str,\n        policy_assignment_name: str,\n        **kwargs: Any", "documentation": "Deletes the assignment with the given scope and policy assignment name .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_11": {"code": "def create(\n        self,\n        scope: str,\n        policy_assignment_name: str,\n        parameters: \"_models.PolicyAssignment\",\n        **kwargs: Any", "documentation": "Creates a new policy assignment with the given scope and name .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_12": {"code": "def get(\n        self,\n        scope: str,\n        policy_assignment_name: str,\n        **kwargs: Any", "documentation": "Get the policy assignment assignment .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_13": {"code": "def list_for_resource_group(\n        self,\n        resource_group_name: str,\n        filter: Optional[str] = None,\n        **kwargs: Any", "documentation": "Get all resources for a resource group .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_14": {"code": "def prepare_request(next_link=None):\n            if not next_link:", "documentation": "Prepares the next request to be sent .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_15": {"code": "def extract_data(pipeline_response):\n            deserialized = self._deserialize(\"PolicyAssignmentListResult\", pipeline_response)\n            list_of_elem = deserialized.value\n            if cls:\n                list_of_elem = cls(list_of_elem)\n            return deserialized.next_link or None, iter(list_of_elem)", "documentation": "Extract data from a PipelineResponse .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_16": {"code": "def list_for_resource(\n        self,\n        resource_group_name: str,\n        resource_provider_namespace: str,\n        parent_resource_path: str,\n        resource_type: str,\n        resource_name: str,\n        filter: Optional[str] = None,\n        **kwargs: Any", "documentation": "List all instances of resources for the given resource .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_17": {"code": "def prepare_request(next_link=None):\n            if not next_link:", "documentation": "Prepares the next request to be sent .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_18": {"code": "def extract_data(pipeline_response):\n            deserialized = self._deserialize(\"PolicyAssignmentListResult\", pipeline_response)\n            list_of_elem = deserialized.value\n            if cls:\n                list_of_elem = cls(list_of_elem)\n            return deserialized.next_link or None, iter(list_of_elem)", "documentation": "Extract data from a PipelineResponse .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_19": {"code": "def list(\n        self,\n        filter: Optional[str] = None,\n        **kwargs: Any", "documentation": "Lists all entities in the database .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_20": {"code": "def prepare_request(next_link=None):\n            if not next_link:", "documentation": "Prepares the next request to be sent .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_21": {"code": "def extract_data(pipeline_response):\n            deserialized = self._deserialize(\"PolicyAssignmentListResult\", pipeline_response)\n            list_of_elem = deserialized.value\n            if cls:\n                list_of_elem = cls(list_of_elem)\n            return deserialized.next_link or None, iter(list_of_elem)", "documentation": "Extract data from a PipelineResponse .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_22": {"code": "def delete_by_id(\n        self,\n        policy_assignment_id: str,\n        **kwargs: Any", "documentation": "Delete the policy assignment by id .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_23": {"code": "def create_by_id(\n        self,\n        policy_assignment_id: str,\n        parameters: \"_models.PolicyAssignment\",\n        **kwargs: Any", "documentation": "Create a new PolicyAssignment by the given ID . The creation includes the resource identifier of the policy", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "Azure/azure-sdk-for-python_sdk/resources/azure-mgmt-resource/azure/mgmt/resource/policy/v2016_12_01/operations/_policy_assignments_operations.py_24": {"code": "def get_by_id(\n        self,\n        policy_assignment_id: str,\n        **kwargs: Any", "documentation": "Get a policy assignment by its identifier .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "loretoparisi/docker_theano/rsc15/preprocess.py_0": {"code": "def __init__(self, filename=\"Default.log\"):\n        self.terminal = sys.stdout\n        self.log = open(filename, \"a\")", "documentation": "Initialization of the logger", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "loretoparisi/docker_theano/rsc15/preprocess.py_1": {"code": "def flush(self):\n        pass", "documentation": "Implement the flush method to allow the underlying database to be flushed", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_0": {"code": "def __init__ (self, url=None, scheduler='default', session=None) :\n\n        Attributes.__init__ (self)", "documentation": "Initialize a new instance of the attributes class", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_1": {"code": "def add_pilot (self, pid) :\n        \"\"\"\n        add (Compute or Data)-Pilot(s) to the pool\n        \"\"\"\n\n        raise Exception (\"%s.add_pilot() is not implemented\" % self.__class__.__name__)", "documentation": "add a pilot to the pool", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_2": {"code": "def list_pilots (self, ptype=ANY) :\n        \"\"\"\n        List IDs of data and/or compute pilots\n        \"\"\"\n\n        raise Exception (\"%s.list_pilots() is not implemented\" % self.__class__.__name__)", "documentation": "List IDs of data and/or compute pilots", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_3": {"code": "def remove_pilot (self, pid, drain=False) :\n        \"\"\"\n        Remove pilot(s) (does not cancel the pilot(s), but removes all units\n        from the pilot(s).\n\n        `drain` determines what happens to the units which are managed by the\n        removed pilot(s).  If `True`, the pilot removal is delayed until all\n        units reach a final state.  If `False` (the default), then `RUNNING`\n        units will be canceled, and `PENDING` units will be re-assinged to the\n        unit managers for re-scheduling to other pilots.\n        \"\"\"\n\n        raise Exception (\"%s.remove_pilot() is not implemented\" % self.__class__.__name__)", "documentation": "Remove pilot ( s ) does not cancel the pilot but removes all units from the pilot", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_4": {"code": "def submit_unit (self, description) :\n        \"\"\"\n        Instantiate and return (Compute or Data)-Unit object(s)\n        \"\"\"\n\n        raise Exception (\"%s.submit_unit() is not implemented\" % self.__class__.__name__)", "documentation": "Instantiate and return ( Compute or Data ) - Unit object", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_5": {"code": "def list_units (self, utype=ANY) :\n        \"\"\"\n        List IDs of data and/or compute units\n        \"\"\"\n\n        raise Exception (\"%s.list_units() is not implemented\" % self.__class__.__name__)", "documentation": "List IDs of data and/or compute units", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_6": {"code": "def get_unit (self, uids) :\n        \"\"\"\n        Reconnect to and return (Compute or Data)-Unit object(s)\n        \"\"\"\n\n        raise Exception (\"%s.get_unit() is not implemented\" % self.__class__.__name__)", "documentation": "Reconnect to and return ( Compute or Data ) - Unit object", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_7": {"code": "def wait_unit (self, uids, state=[DONE, FAILED, CANCELED], timeout=-1.0) :\n        \"\"\"\n        Wait for given unit(s) to enter given state\n        \"\"\"\n\n        raise Exception (\"%s.wait_unit() is not implemented\" % self.__class__.__name__)", "documentation": "Wait for given unit to enter given state", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "JensTimmerman/radical.pilot_docs/architecture/api_draft/unit_manager.py_8": {"code": "def cancel_units (self, uids) :\n        \"\"\"\n        Cancel given unit(s)\n        \"\"\"\n\n        raise Exception (\"%s.cancel_unit() is not implemented\" % self.__class__.__name__)", "documentation": "Cancel given units", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_0": {"code": "def __missing__(self, key):\n        try:\n            return super().__missing__(key)\n        except KeyError:\n            return NotImplemented", "documentation": "Override missing to catch KeyErrors .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_1": {"code": "def __init__(self, shortname, loader):\n        # Not preloaded\n        # loaders must produce dictionaries (or an appropriate iterable)\n        # with the required keys.\n        # The reason for this is that code for certain servers need not be loaded\n        # if it's not going to be used at all\n        # It also prevents import loop collisions.\n        global __ServerImplementationDict\n        self.__data = ServerImplementationDict(loader)\n        self.__shortname = shortname", "documentation": "Load the server implementation dictionary for a given shortname", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_2": {"code": "def shortname(self):", "documentation": "Short name of the field", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_3": {"code": "def __str__(self):\n        return str(self.__shortname)", "documentation": "__)", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_4": {"code": "def name(self):\n        return self.__data['str_name']", "documentation": "Returns the name of the team .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_5": {"code": "def internal_shortname(self):\n        return self.__data['str_shortname']", "documentation": "Returns the internal name of the actor .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_6": {"code": "def beta(self):\n        return self.__data['bool_tester']", "documentation": "Return the beta tester .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_7": {"code": "def Auth(self): # I really don't know how to call this.\n        return self.__data['cls_auth']", "documentation": "Returns the authentication mechanism of this server .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_8": {"code": "def auth_fields(self):\n        return self.__data['list_authkeys']", "documentation": "Authentication fields of user .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "juanchodepisa/sbtk_SBTK_League_Helper/src/interfacing/servers.py_9": {"code": "def Player(self):\n        return self.__data['cls_player']", "documentation": "Returns the Player class .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "arnavd96/Cinemiezer_myvenv/lib/python3.4/site-packages/music21/ext/jsonpickle/__init__.py_0": {"code": "def __init__(self, name):\n            self.name = name", "documentation": "Shortcut to set name of the field", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "arnavd96/Cinemiezer_myvenv/lib/python3.4/site-packages/music21/ext/jsonpickle/__init__.py_1": {"code": "def encode(value,\n           unpicklable=True,\n           make_refs=True,\n           keys=False,\n           max_depth=None,\n           backend=None,\n           warn=False,\n           max_iter=None):\n    \"\"\"Return a JSON formatted representation of value, a Python object.\n\n    :param unpicklable: If set to False then the output will not contain the\n        information necessary to turn the JSON data back into Python objects,\n        but a simpler JSON stream is produced.\n    :param max_depth: If set to a non-negative integer then jsonpickle will\n        not recurse deeper than 'max_depth' steps into the object.  Anything\n        deeper than 'max_depth' is represented using a Python repr() of the\n        object.\n    :param make_refs: If set to False jsonpickle's referencing support is\n        disabled.  Objects that are id()-identical won't be preserved across\n        encode()/decode(), but the resulting JSON stream will be conceptually\n        simpler.  jsonpickle detects cyclical objects and will break the cycle\n        by calling repr() instead of recursing when make_refs is set False.\n    :param keys: If set to True then jsonpickle will encode non-string\n        dictionary keys instead of coercing them into strings via `repr()`.\n    :param warn: If set to True then jsonpickle will warn when it\n        returns None for an object which it cannot pickle\n        (e.g. file descriptors).\n    :param max_iter: If set to a non-negative integer then jsonpickle will\n        consume at most `max_iter` items when pickling iterators.\n\n    >>> encode('my string')\n    '\"my string\"'\n    >>> encode(36)\n    '36'\n\n    >>> encode({'foo': True})\n    '{\"foo\": true}'\n\n    >>> encode({'foo': True}, max_depth=0)\n    '\"{\\\\'foo\\\\': True}\"'\n\n    >>> encode({'foo': True}, max_depth=1)\n    '{\"foo\": \"True\"}'\n\n\n    \"\"\"\n    if backend is None:\n        backend = json\n    return pickler.encode(value,\n                          backend=backend,\n                          unpicklable=unpicklable,\n                          make_refs=make_refs,\n                          keys=keys,\n                          max_depth=max_depth,\n                          warn=warn)", "documentation": "JSONpickle and pickle a Python object .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570692.0}}, "marcus-nystrom/share-gaze_sync_clocks/test_clock_resolution.py_0": {"code": "def main():", "documentation": "pass", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "ValorNaram/isl_inputchangers/002.py_0": {"code": "def normalizeEnter(src):\n\t#Deletes all user defined for readability reason existing line breaks that are issues for the HTML output\n\tfor elem in blocklevel:\n\t\twhile src.find(\"\\r<\" + elem) > -1:\n\t\t\tsrc = src.replace(\"\\r<\" + elem, \"<\" + elem)\n\t\twhile src.find(\"</\" + elem + \">\\r\") > -1:\n\t\t\tsrc = src.replace(\"</\" + elem + \">\\r\", \"</\" + elem + \">\")\n\t\twhile src.find(\">\\r\") > -1:\n\t\t\tsrc = src.replace(\">\\r\", \">\") #It is really needed, it created some other bugs?!\n\t\twhile src.find(\"\\r</\") > -1:\n\t\t\tsrc = src.replace(\"\\r</\", \"</\") ##It is really needed, it created some other bugs?!\n\treturn src", "documentation": "This removes all user defined lines that are not relevant for the HTML enter context", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrmrwat/pylsner_pylsner/gui.py_0": {"code": "def __init__(self):\n        super(Window, self).__init__(skip_pager_hint=True,\n                                     skip_taskbar_hint=True,\n                                    )\n        self.set_title('Pylsner')\n\n        screen = self.get_screen()\n        self.width = screen.get_width()\n        self.height = screen.get_height()\n        self.set_size_request(self.width, self.height)\n        self.set_position(Gtk.WindowPosition.CENTER)\n        rgba = screen.get_rgba_visual()\n        self.set_visual(rgba)\n        self.override_background_color(Gtk.StateFlags.NORMAL,\n                                       Gdk.RGBA(0, 0, 0, 0),\n                                      )\n\n        self.set_wmclass('pylsner', 'pylsner')\n        self.set_type_hint(Gdk.WindowTypeHint.DOCK)\n        self.stick()\n        self.set_keep_below(True)\n\n        drawing_area = Gtk.DrawingArea()\n        drawing_area.connect('draw', self.redraw)\n        self.refresh_cnt = 0\n        self.add(drawing_area)\n\n        self.connect('destroy', lambda q: Gtk.main_quit())\n\n        self.widgets = []\n\n        self.show_all()", "documentation": "Create a Pylsner window", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrmrwat/pylsner_pylsner/gui.py_1": {"code": "def redraw(self, _, ctx):\n        ctx.set_antialias(cairo.ANTIALIAS_SUBPIXEL)\n        for wid in self.widgets:\n            wid.redraw(ctx)", "documentation": "Redraws all widgets in the hierarchy", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrmrwat/pylsner_pylsner/gui.py_2": {"code": "def __init__(self,\n                 name='default',\n                 metric={'plugin': 'time'},\n                 indicator={'plugin': 'arc'},\n                 fill={'plugin': 'rgba_255'},\n                ):\n        self.name = name\n        MetricPlugin = plugin.load_plugin('metrics', metric['plugin'])\n        self.metric = MetricPlugin(**metric)\n        IndicatorPlugin = plugin.load_plugin('indicators', indicator['plugin'])\n        self.indicator = IndicatorPlugin(**indicator)\n        FillPlugin = plugin.load_plugin('fills', fill['plugin'])\n        self.fill = FillPlugin(**fill)", "documentation": "Loads plugins and sets up the plugin objects", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrwsr/monotone_test/test_monotone.py_0": {"code": "def errno_value():\n    \"\"\"\n    A particular errno.\n    \"\"\"\n    return errno.EINVAL", "documentation": ", None\"", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrwsr/monotone_test/test_monotone.py_1": {"code": "def strerror(errno_value):\n    \"\"\"\n    The string representation of a particular errno\n    \"\"\"\n    return \"[Errno {}] Invalid argument\".format(errno_value)", "documentation": "Return a string representation of a particular errno", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrwsr/monotone_test/test_monotone.py_2": {"code": "def apply_failing_clock_call(monkeypatch):\n    \"\"\"\n    Return a callable that patches in a failing system call fake that\n    fails and return a list of calls to that fake.\n    \"\"\"\n\n    def _apply_failing_clock_call(name, errno_value):\n        calls = []\n\n        def _failing_clock_call(clock_id, timespec):\n            calls.append((clock_id, timespec))\n            monkeypatch.setattr(_api.ffi, \"errno\", errno.EINVAL)\n            return -1\n\n        monkeypatch.setattr(_api, name, _failing_clock_call)\n\n        return calls\n\n    return _apply_failing_clock_call", "documentation": "monkeypatches the _api module to patch in a failing system call fake that fails with an", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrwsr/monotone_test/test_monotone.py_3": {"code": "def apply_timespec(monkeypatch):\n    \"\"\"\n    Return a callable that patches in a fake over the specified clock\n    call that sets the specified resolution and returns a list of\n    calls to that fake.\n    \"\"\"\n\n    def _apply_timespec(name, goal_timespec):\n        calls = []\n\n        def _fake_clock_call(clock_id, timespec):\n            calls.append((clock_id, timespec))\n            timespec[0] = goal_timespec[0]\n            return 0\n\n        monkeypatch.setattr(_api, name, _fake_clock_call)\n\n        return calls\n\n    return _apply_timespec", "documentation": "Decorator that patches a fake API object to the specified method .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrwsr/monotone_test/test_monotone.py_4": {"code": "def test_init(self):\n        \"\"\"\n        The initializer updates the instance's C{__dict__} with its\n        keyword arguments.\n        \"\"\"\n        namespace = _api._SimpleNamespace(x=1)\n        assert namespace.x == 1", "documentation": "Test the initialization of the class with keyword arguments", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrwsr/monotone_test/test_monotone.py_5": {"code": "def test_eq(self):\n        \"\"\"\n        Two instances with equal C{__dict__}s are equal.\n        \"\"\"\n        assert _api._SimpleNamespace(a=1) == _api._SimpleNamespace(a=1)", "documentation": "Make sure that two namespaces with equal dicts are equal", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrwsr/monotone_test/test_monotone.py_6": {"code": "def test_non_monotonic(self):\n        \"\"\"\n        L{get_clock_info} only knows about the monotonic clock.\n        \"\"\"\n        with pytest.raises(ValueError):\n            get_clock_info(\"not monotonic\")", "documentation": "Test that get_clock_info raises a ValueError if get_clock_info fails .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrwsr/monotone_test/test_monotone.py_7": {"code": "def test_info(self, clock_getres_spec, apply_timespec):\n        \"\"\"\n        The reported info always includes a nanosecond resolution when\n        C{clock_getres} indicates nanosecond resolution.\n        \"\"\"\n        calls = apply_timespec(\n            \"_clock_getres\",\n            _bindings.ffi.new(\"struct timespec *\", clock_getres_spec),\n        )\n\n        expected_info = _api._SimpleNamespace(\n            adjustable=False,\n            implementation=\"clock_gettime(MONOTONIC)\",\n            monotonic=True,\n            resolution=None,    # checked separately\n        )\n\n        if clock_getres_spec['tv_nsec']:\n            expected_resolution = 1e-09\n        else:\n            expected_resolution = 1.0\n\n        info = get_clock_info(\"monotonic\")\n        resolution, info.resolution = info.resolution, None\n\n        assert info == expected_info\n        assert resolution - expected_resolution == pytest.approx(0.0)\n\n        assert len(calls) == 1\n        assert calls[0][0] == _bindings.lib.CLOCK_MONOTONIC", "documentation": "Report reported info always includes a nanosecond resolution when C{clock_getres}} indicates nano", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrwsr/monotone_test/test_monotone.py_8": {"code": "def test_non_monotonic(self):\n        \"\"\"\n        L{get_clock_info} only knows about the monotonic clock.\n        \"\"\"\n        with pytest.raises(ValueError):\n            get_clock_info(\"not monotonic\")", "documentation": "Test that get_clock_info raises a ValueError if get_clock_info fails .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrwsr/monotone_test/test_monotone.py_9": {"code": "def test_monotonic_fails_posix(apply_failing_clock_call,\n                               errno_value,\n                               strerror):\n    \"\"\"\n    A failure in C{clock_gettime} results in an L{OSError} that\n    presents the failure's errno.\n    \"\"\"\n    calls = apply_failing_clock_call('_clock_gettime', errno_value)\n\n    with pytest.raises(OSError) as exc:\n        monotonic()\n\n    assert len(calls) == 1\n    assert calls[0][0] == _bindings.lib.CLOCK_MONOTONIC\n\n    assert str(exc.value) == strerror", "documentation": "Attempt to call the libclock_gettime function on POSIX systems .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "mrwsr/monotone_test/test_monotone.py_10": {"code": "def test_clock(clock_gettime_spec, apply_timespec):\n    \"\"\"\n    For any given time resolution, the monotonic time equals the\n    sum of the seconds and nanoseconds.\n    \"\"\"\n    clock_gettime_calls = apply_timespec(\n        '_clock_gettime',\n        _bindings.ffi.new(\"struct timespec *\", clock_gettime_spec),\n    )\n\n    # we a float, representing the current seconds plus the\n    # nanoseconds (offset by a billion) iff the resolution is accurate\n    # to the nanosecond.\n    expected = float(clock_gettime_spec['tv_sec']) + (\n        clock_gettime_spec['tv_nsec'] * 1e-09)\n\n    result = monotonic()\n\n    assert result - expected == pytest.approx(0.0)\n\n    assert clock_gettime_calls[0][0] == _bindings.lib.CLOCK_MONOTONIC", "documentation": "A function that tests that the library can get the current clock value", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_0": {"code": "def format_fans(fans):\n    return format_line(prefix='fans'.rjust(RJUST), values=fans)", "documentation": "Format fans for logging output", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_1": {"code": "def format_pwms(pwms):\n    return format_line(prefix='pwms'.rjust(RJUST), values=pwms)", "documentation": "Format pwms line .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_2": {"code": "def format_names(names):\n    return format_line(prefix='names'.rjust(RJUST), values=names)", "documentation": "Format a names line .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_3": {"code": "def format_temps(temps):\n    return format_line(prefix='temps'.rjust(RJUST), values=temps)", "documentation": "Format a list of temps", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_4": {"code": "def format_limits(limits):\n    return format_line(prefix='limits'.rjust(RJUST), values=limits)", "documentation": "Format limits line .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_5": {"code": "def format_headrooms(headrooms):\n    return format_line(prefix='headrooms'.rjust(RJUST), values=headrooms)", "documentation": "Format list of rooms .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "Bengt/AL-FanControl_python/fancontrol/ui/cli_util.py_6": {"code": "def format_differences(differences):\n    return format_line(prefix='differences'.rjust(RJUST), values=differences)", "documentation": "Format a list of difference values .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "toranb/django-bower-registry_api/migrations/0001_initial.py_0": {"code": "def forwards(self, orm):\n        # Adding model 'Package'\n        db.create_table(u'api_package', (\n            (u'id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),\n            ('name', self.gf('django.db.models.fields.CharField')(unique=True, max_length=500, db_index=True)),\n            ('url', self.gf('django.db.models.fields.CharField')(unique=True, max_length=500)),\n            ('created_at', self.gf('django.db.models.fields.DateField')(auto_now_add=True, blank=True)),\n        ))\n        db.send_create_signal(u'api', ['Package'])\n\n        # Adding unique constraint on 'Package', fields ['name', 'url']\n        db.create_unique(u'api_package', ['name', 'url'])", "documentation": "Adding model 'ApiPackage' to the database .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "smarkets/smk_python_sdk_smarkets/statsd.py_0": {"code": "def __init__(self, host='localhost', port=8125, enabled=True, prefix=''):\n        self.addr = None\n        self.enabled = enabled\n        if enabled:\n            self.set_address(host, port)\n        self.prefix = prefix\n        self.udp_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)", "documentation": "Initializes a UDP connection .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "smarkets/smk_python_sdk_smarkets/statsd.py_1": {"code": "def timed(self, stat, sample_rate=1):\n        log.debug('Entering timed context for %r' % (stat,))\n        start = time.time()\n        yield\n        duration = int((time.time() - start) * 1000)\n        log.debug('Exiting timed context for %r' % (stat,))\n        self.timing(stat, duration, sample_rate)", "documentation": "Context manager to time out a context and add a timing event .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "smarkets/smk_python_sdk_smarkets/statsd.py_2": {"code": "def increment(self, stats, sample_rate=1):\n        \"\"\"\n        Increments one or more stats counters\n        \"\"\"\n        self.update_stats(stats, 1, sample_rate)", "documentation": "Increments one or more stats counters", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "smarkets/smk_python_sdk_smarkets/statsd.py_3": {"code": "def update_stats(self, stats, delta=1, sampleRate=1):\n        \"\"\"\n        Updates one or more stats counters by arbitrary amounts\n        \"\"\"\n        if not self.enabled or self.addr is None:\n            return\n\n        if type(stats) is not list:\n            stats = [stats]\n        data = {}\n        for stat in stats:\n            data[\"%s%s\" % (self.prefix, stat)] = \"%s|c\" % delta\n\n        self.send(data, sampleRate)", "documentation": "Updates one or more stats counters by arbitrary amounts", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "PythonProgramming/Support-Vector-Machines---Basics-and-Fundamental-Investing-Project_p10.py_0": {"code": "def Key_Stats(gather=\"Total Debt/Equity (mrq)\"):\n  statspath = path+'/_KeyStats'\n  stock_list = [x[0] for x in os.walk(statspath)]\n  df = pd.DataFrame(\n    columns = [\n      'Date',\n      'Unix',\n      'Ticker',\n      'DE Ratio',\n      'Price',\n      'stock_p_change',\n      'SP500',\n      'sp500_p_change',\n      'Difference',\n      'Status'\n    ]\n  )\n\n  sp500_df = pd.DataFrame.from_csv(\"YAHOO-INDEX_GSPC.csv\")\n\n  ticker_list = []\n\n  for each_dir in stock_list[1:25]:\n    each_file = os.listdir(each_dir)\n\n    # ticker = each_dir.split(\"\\\\\")[1] # Windows only\n    # ticker = each_dir.split(\"/\")[1] # this didn't work so do this:\n    ticker = os.path.basename(os.path.normpath(each_dir))\n    # print(ticker) # uncomment to verify\n    ticker_list.append(ticker)\n\n    starting_stock_value = False\n    starting_sp500_value = False", "documentation": "Get stock and SP500 difference and difference in each day from each stock", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "KeserOner/puissance4_bestaplayer.py_0": {"code": "def __init__(self, fichier, player):\n        self.fichier = fichier\n        self.grille = self.getFirstGrid()\n        self.best_hit = 0\n        self.players = player", "documentation": "Initialization of the class", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "KeserOner/puissance4_bestaplayer.py_1": {"code": "def updateGrid(self):\n        \"\"\"\n        Implements function to update the grid to alter n-1\n        round values\n\n        \"\"\"\n        with open(self.fichier, 'r') as fi:\n            for line in fi.readlines():\n                i = 0\n                for car in line:\n                    j = 0\n                    if car != '\\n':\n                        self.grille[i][j] = car\n                        j += 1\n                    i += 1", "documentation": "Updates the grid to alter n - round values", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "KeserOner/puissance4_bestaplayer.py_2": {"code": "def checkLines(self, player, inARow):\n        \"\"\"\n        Implements function to check the current lines setup to evaluate best combinaison.\n\n        :param player: check for your numbers (your player number) or those of your opponent.\n        :param inARow: how many tokens in a row (3 or 2).\n        :return: true or false\n\n        \"\"\"\n        count = 0\n        flag = False\n        for line_number, line in enumerate(self.grille):\n            count = 0\n            for car_pos, car in enumerate(line[:len(line) - 1]):\n                if int(car) == player and not flag:\n                    count = 1\n                    flag = True\n                elif int(car) == player and flag:\n                    count += 1\n                    if count == inARow:\n                        if car_pos - inARow >= 0 and self.canPlayLine(line_number, car_pos - inARow):\n                            return True, car_pos - inARow\n                        if car_pos + 1 <= 6 and self.canPlayLine(line_number, car_pos + 1):\n                            return True, car_pos + 1\n                else:\n                    count = 0\n        return False, 0", "documentation": "Check if the current lines setup can be played", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "KeserOner/puissance4_bestaplayer.py_3": {"code": "def changeColumnInLines(self):\n        \"\"\"\n        Implements function to transform columns in lines to make tests eaiser.\n        :return: a reverse matrice\n        \"\"\"\n        column = []\n        for x in xrange(7):\n            col = ''\n            for y in xrange(6):\n                col += self.grille[y][x]\n            column.append(col)\n        return column", "documentation": "Implements function to transform columns in lines to make tests eaiser .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "KeserOner/puissance4_bestaplayer.py_4": {"code": "def checkDiagonalLeftToRight(self, player, inARow):\n        \"\"\"\n        Implements function to check the current diagonal to evaluate best combinaison.\n\n        :param player: check for your numbers or opponent ones.\n        :param inARow:  how many tokens in a row (3 or 2).\n        :return:\n        \"\"\"\n\n        x = 3\n        flag = False\n        while x < 6:\n            count = 0\n            x_int = x\n            y_int = 0\n            while x_int >= 0:\n                if int(self.grille[x_int][y_int]) == player and not flag:\n                    count = 1\n                    flag = True\n                elif int(self.grille[x_int][y_int]) == player and flag:\n                    count += 1\n                    if count == inARow and y_int + 1 <= 6 and x_int - 1 >= 0 and self.grille[x_int][y_int + 1] != '0':\n                        return True, y_int + 1\n                else:\n                    count = 0\n                    flag = False\n                x_int -= 1\n                y_int += 1\n            x += 1\n\n        y = 1\n        flag = False\n        while y <= 3:\n            count = 0\n            x_int = 5\n            y_int = y\n            while y_int <= 6 and x_int >= 0:\n                if int(self.grille[x_int][y_int]) == player and not flag:\n                    count = 1\n                    flag = True\n                elif int(self.grille[x_int][y_int]) == player and flag:\n                    count += 1\n                    if count == inARow and y_int + 1 <= 6 and x_int - 1 >= 0 and self.grille[x_int][y + 1] != '0':\n                        return True, y_int + 1\n                else:\n                    count = 0\n                    flage = False\n                x_int -= 1\n                y_int += 1\n            y += 1\n\n        return False, 0", "documentation": "Checks the current diagonal of the game to see if the player is right .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "KeserOner/puissance4_bestaplayer.py_5": {"code": "def checkDiagonals(self, player, inARow):\n        \"\"\"\n        Calls two diagonal functional.\n        :return: an int, representing the column where to play or 0 and False if there is no pattern search.\n        \"\"\"\n        check = self.checkDiagonalLeftToRight(player, inARow)\n        if check[0]:\n            return check\n        else:\n            return self.checkDiagonalRightToLeft(player, inARow)", "documentation": "Calls two diagonal functional .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "KeserOner/puissance4_bestaplayer.py_6": {"code": "def findFirstColumnEmpty(self):\n        \"\"\"\n        Implements function to get the first column where a slot remain.\n        :return: the column\n        \"\"\"\n        for col in xrange(7):\n            if self.grille[0][col] == '0':\n                return col\n        return -1", "documentation": "Returns the first column where a slot is empty", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "hfaran/slack-export-viewer_slackviewer/user.py_0": {"code": "def __init__(self, raw_data):\n        self._raw = raw_data", "documentation": "Set the raw data from the response", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "hfaran/slack-export-viewer_slackviewer/user.py_1": {"code": "def display_name(self):\n        \"\"\"\n        Find the most appropriate display name for a user: look for a \"display_name\", then\n        a \"real_name\", and finally fall back to the always-present \"name\".\n        \"\"\"\n        for k in self._NAME_KEYS:\n            if self._raw.get(k):\n                return self._raw[k]\n            if \"profile\" in self._raw and self._raw[\"profile\"].get(k):\n                return self._raw[\"profile\"][k]\n        return self._raw[\"name\"]", "documentation": "The most appropriate display name for a user .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "hfaran/slack-export-viewer_slackviewer/user.py_2": {"code": "def email(self):\n        \"\"\"\n        Shortcut property for finding the e-mail address or bot URL.\n        \"\"\"\n        if \"profile\" in self._raw:\n            email = self._raw[\"profile\"].get(\"email\")\n        elif \"bot_url\" in self._raw:\n            email = self._raw[\"bot_url\"]\n        else:\n            email = None\n        if not email:\n            logging.debug(\"No email found for %s\", self._raw.get(\"name\"))\n        return email", "documentation": "Shortcut property for finding the e - mail address or bot URL .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570693.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_0": {"code": "def setUp(self):\n    self.schedule = schedule_parser.Schedule()\n    self.schedule.Parse(SCHEDULE_PATH)", "documentation": "Sets up the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_1": {"code": "def setUp(self):\n    self.schedule = schedule_parser.Schedule()\n    self.schedule.Parse(SCHEDULE_PATH)", "documentation": "Sets up the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_2": {"code": "def setUp(self):\n    self.schedule = schedule_parser.Schedule()\n    self.schedule.Parse(SCHEDULE_PATH)", "documentation": "Sets up the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_3": {"code": "def setUp(self):\n    self.schedule = schedule_parser.Schedule()\n    self.schedule.Parse(SCHEDULE_PATH)", "documentation": "Sets up the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_4": {"code": "def setUp(self):\n    self.schedule = schedule_parser.Schedule()\n    self.schedule.Parse(SCHEDULE_PATH)", "documentation": "Sets up the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_5": {"code": "def setUp(self):\n    self.schedule = schedule_parser.Schedule()\n    self.schedule.Parse(SCHEDULE_PATH)", "documentation": "Sets up the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_6": {"code": "def setUp(self):\n    self.schedule = schedule_parser.Schedule()\n    self.schedule.Parse(SCHEDULE_PATH)", "documentation": "Sets up the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_7": {"code": "def setUp(self):\n    self.schedule = schedule_parser.Schedule()\n    self.schedule.Parse(SCHEDULE_PATH)", "documentation": "Sets up the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_8": {"code": "def setUp(self):\n    self.schedule = schedule_parser.Schedule()\n    self.schedule.Parse(SCHEDULE_PATH)", "documentation": "Sets up the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_9": {"code": "def setUp(self):\n    self.schedule = schedule_parser.Schedule()\n    self.schedule.Parse(SCHEDULE_PATH)", "documentation": "Sets up the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_10": {"code": "def setUp(self):\n    self.schedule = schedule_parser.Schedule()\n    self.schedule.Parse(SCHEDULE_PATH)", "documentation": "Sets up the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_11": {"code": "def setUp(self):\n    self.schedule = schedule_parser.Schedule()\n    self.schedule.Parse(SCHEDULE_PATH)", "documentation": "Sets up the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_12": {"code": "def setUp(self):\n    self.schedule = schedule_parser.Schedule()\n    self.schedule.Parse(SCHEDULE_PATH)", "documentation": "Sets up the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "xairy/mipt-schedule-parser_msp/test/schedule_tests.py_13": {"code": "def suite():\n  loader = unittest.TestLoader()\n  suite = unittest.TestSuite()\n  suite.addTest(WeekdayRangeTest())\n  suite.addTest(DepartmentCountTest())\n  suite.addTest(DepartmentRangeTest())\n  suite.addTest(DepartmentsRowTest())\n  suite.addTest(HoursColumnTest())\n  suite.addTest(HoursRangesTest())\n  suite.addTest(GroupCountTest())\n  suite.addTest(GroupListTest())\n  suite.addTest(GroupRangeTest())\n  suite.addTest(WeekdayByRowTest())\n  suite.addTest(PairByRowTest())\n  suite.addTest(DepartmentByColumnTest())\n  suite.addTest(GroupByColumnTest())\n  return suite", "documentation": "Returns a test suite", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "vervacity/ggr-project_scripts/data_qc/summarize_chipseq_qc.py_0": {"code": "def get_num_lines_gz(filename):\n    num_lines = 0\n    with gzip.open(filename, \"r\") as fp:\n        for line in fp:\n            num_lines += 1", "documentation": "Count the number of lines in a gzipped file", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "vervacity/ggr-project_scripts/data_qc/summarize_chipseq_qc.py_1": {"code": "def main():\n    \"\"\"get stats from PAS-seq", "documentation": "s\"\"", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ajitghz582/PythonLearning_DAY_1_ASSIGNMENTS/1_name_phone_number.py_0": {"code": "def Handler() :\n\twhile (1) :\n\t\tchoice = eval(input(\"Enter :\\t 1 - to search student name \\n \\t 2 - to insert new student record \\n \\t 0 - to quit\\n\"))\n\t\tprint(choice)\n\t\tif (choice == 1) :\n\t\t\tif (student_phoneNumber_name) :\n\t\t\t\tphone_number = input(\"Enter student's phone number : \")\n\t\t\t\tname = SearchRecord(phone_number)\n\t\t\t\tif (name) :\n\t\t\t\t\tprint(\"name : \" + name )\n\t\t\t\telse :\n\t\t\t\t\tprint(str(phone_number) + \"Does not exist in record\" + str(name))\n\t\t\telse :\n\t\t\t\tprint(\"Record is empty \")\n\t\telif (choice == 2) :\n\t\t\tphone_number = input(\"Enter student's phone number : \")\n\t\t\tname = input(\"Enter student's name : \") #best example to understand input() and raw_input()\n\t\t\tInsertRecord(phone_number, name)\n\t\telif (choice == 0) :\n\t\t\tbreak\n\t\telse:\n\t\t\tprint(\"Enter correct choice\")", "documentation": "user input handler for inserting new student name into a database", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ajitghz582/PythonLearning_DAY_1_ASSIGNMENTS/1_name_phone_number.py_1": {"code": "def InsertRecord(x, y):\n\tstudent_phoneNumber_name[x] = y\n\treturn;", "documentation": "Insert a record into the list", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ajitghz582/PythonLearning_DAY_1_ASSIGNMENTS/1_name_phone_number.py_2": {"code": "def SearchRecord(x):\n\tprint(x)\n\tif (x in student_phoneNumber_name) :\n\t\treturn student_phoneNumber_name[x]", "documentation": "Search a record in student phone number list", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_0": {"code": "def setUp(self):\n        super(BaseSystemTest, self).setUp()\n        # Clear out any pre-existing tables\n        for tablename in self.dynamo.list_tables():\n            self.dynamo.delete_table(tablename)", "documentation": "Deletes all tables created by DynamoDB", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_1": {"code": "def tearDown(self):\n        super(TestMisc, self).tearDown()\n        self.dynamo.default_return_capacity = False", "documentation": "Tear down the test .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_2": {"code": "def test_connection_region(self):\n        \"\"\"Connection can access name of connected region\"\"\"\n        self.assertTrue(isinstance(self.dynamo.region, str))", "documentation": "Connection can access name of connected region", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_3": {"code": "def test_connect_to_region_creds(self):\n        \"\"\"Can connect to a dynamo region with credentials\"\"\"\n        conn = DynamoDBConnection.connect(\n            \"us-west-1\", access_key=\"abc\", secret_key=\"12345\"\n        )\n        self.assertIsNotNone(conn.host)", "documentation": "Can connect to a dynamo region with credentials", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_4": {"code": "def test_retry_on_throughput_error(self, time):\n        \"\"\"Throughput exceptions trigger a retry of the request\"\"\"\n\n        def call(*_, **__):\n            \"\"\"Dummy service call\"\"\"\n            response = {\n                \"ResponseMetadata\": {\n                    \"HTTPStatusCode\": 400,\n                },\n                \"Error\": {\n                    \"Code\": \"ProvisionedThroughputExceededException\",\n                    \"Message\": \"Does not matter\",\n                },\n            }\n            raise ClientError(response, \"list_tables\")\n\n        with patch.object(self.dynamo, \"client\") as client:\n            client.list_tables.side_effect = call\n            with self.assertRaises(ThroughputException):\n                self.dynamo.call(\"list_tables\")\n        self.assertEqual(len(time.sleep.mock_calls), self.dynamo.request_retries - 1)\n        self.assertTrue(time.sleep.called)", "documentation": "Test that requests with ProvisionedThroughputExceededException are not retried if the request retries", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_5": {"code": "def test_magic_table_props(self):\n        \"\"\"Table can look up properties on response object\"\"\"\n        hash_key = DynamoKey(\"id\")\n        self.dynamo.create_table(\"foobar\", hash_key=hash_key)\n        ret = self.dynamo.describe_table(\"foobar\")\n        assert ret is not None\n        self.assertEqual(ret.item_count, ret[\"ItemCount\"])\n        with self.assertRaises(KeyError):\n            self.assertIsNotNone(ret[\"Missing\"])", "documentation": "Table can look up properties on response object", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_6": {"code": "def test_describe_during_delete(self):\n        \"\"\"Describing a table during a delete operation should not crash\"\"\"\n        response = {\n            \"ItemCount\": 0,\n            \"ProvisionedThroughput\": {\n                \"NumberOfDecreasesToday\": 0,\n                \"ReadCapacityUnits\": 5,\n                \"WriteCapacityUnits\": 5,\n            },\n            \"TableName\": \"myTableName\",\n            \"TableSizeBytes\": 0,\n            \"TableStatus\": \"DELETING\",\n        }\n        table = Table.from_response(response)\n        self.assertEqual(table.status, \"DELETING\")", "documentation": "Describing a table during a delete operation should not crash", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_7": {"code": "def test_re_raise_passthrough(self):\n        \"\"\"DynamoDBError can re-raise itself if missing original exception\"\"\"\n        err = DynamoDBError(400, Code=\"ErrCode\", Message=\"Ouch\", args={})\n        caught = False\n        try:\n            err.re_raise()\n        except DynamoDBError as e:\n            caught = True\n            self.assertEqual(err, e)\n        self.assertTrue(caught)", "documentation": "DynamoDBError can re - raise itself if missing original exception", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_8": {"code": "def test_default_return_capacity(self):\n        \"\"\"When default_return_capacity=True, always return capacity\"\"\"\n        self.dynamo.default_return_capacity = True\n        with patch.object(self.dynamo, \"call\") as call:\n            call().get.return_value = None\n            rs = self.dynamo.scan(\"foobar\")\n            list(rs)\n        call.assert_called_with(\n            \"scan\",\n            TableName=\"foobar\",\n            ReturnConsumedCapacity=\"INDEXES\",\n            ConsistentRead=False,\n        )", "documentation": "When default_return_capacity=True always return capacity", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_9": {"code": "def test_limit_complete(self):\n        \"\"\"A limit with item_capacity = 0 is 'complete'\"\"\"\n        limit = Limit(item_limit=0)\n        self.assertTrue(limit.complete)", "documentation": "Tests that a limit with item_capacity = 0 is complete", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_10": {"code": "def test_wait_delete_table(self):\n        \"\"\"Delete table shall wait for the table to go offline.\"\"\"\n        tablename = \"foobar_wait\"\n        hash_key = DynamoKey(\"id\")\n        self.dynamo.create_table(tablename, hash_key=hash_key, wait=True)\n        result = self.dynamo.delete_table(tablename, wait=True)\n        self.assertTrue(result)", "documentation": "Delete table shall wait for the table to go offline .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_11": {"code": "def make_table(self):\n        \"\"\"Convenience method for making a table\"\"\"\n        hash_key = DynamoKey(\"id\")\n        self.dynamo.create_table(\"foobar\", hash_key=hash_key)", "documentation": "Convenience method for making a table", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_12": {"code": "def test_int(self):\n        \"\"\"Store and retrieve an int\"\"\"\n        self.make_table()\n        self.dynamo.put_item(\"foobar\", {\"id\": \"a\", \"num\": 1})\n        item = list(self.dynamo.scan(\"foobar\"))[0]\n        self.assertEqual(item[\"num\"], 1)", "documentation": "Store and retrieve an int", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_13": {"code": "def test_decimal(self):\n        \"\"\"Store and retrieve a Decimal\"\"\"\n        self.make_table()\n        self.dynamo.put_item(\"foobar\", {\"id\": \"a\", \"num\": Decimal(\"1.1\")})\n        item = list(self.dynamo.scan(\"foobar\"))[0]\n        self.assertEqual(item[\"num\"], Decimal(\"1.1\"))", "documentation": "Store and retrieve a Decimal", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_14": {"code": "def test_binary_bytes(self):\n        \"\"\"Store and retrieve bytes as a binary\"\"\"\n        self.make_table()\n        data = {\"a\": 1, \"b\": 2}\n        self.dynamo.put_item(\"foobar\", {\"id\": \"a\", \"data\": Binary(dumps(data))})\n        item = list(self.dynamo.scan(\"foobar\"))[0]\n        self.assertEqual(loads(item[\"data\"].value), data)", "documentation": "Store and retrieve bytes as a binary", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_15": {"code": "def test_number_set(self):\n        \"\"\"Store and retrieve a number set\"\"\"\n        self.make_table()\n        item = {\n            \"id\": \"a\",\n            \"datas\": set([1, 2, 3]),\n        }\n        self.dynamo.put_item(\"foobar\", item)\n        ret = list(self.dynamo.scan(\"foobar\"))[0]\n        self.assertEqual(ret, item)", "documentation": "Store and retrieve a number set", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_16": {"code": "def test_binary_equal(self):\n        \"\"\"Binary should eq other Binaries and also raw bytestrings\"\"\"\n        self.assertEqual(Binary(\"a\"), Binary(\"a\"))\n        self.assertEqual(Binary(\"a\"), b\"a\")\n        self.assertFalse(Binary(\"a\") != Binary(\"a\"))", "documentation": "Binary should eq other Binaries and also also raw bytestrings", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_17": {"code": "def test_binary_converts_unicode(self):\n        \"\"\"Binary will convert unicode to bytes\"\"\"\n        b = Binary(\"a\")\n        self.assertTrue(isinstance(b.value, bytes))", "documentation": "Binary will convert unicode to bytes", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_18": {"code": "def test_bool(self):\n        \"\"\"Store and retrieve a boolean\"\"\"\n        self.make_table()\n        self.dynamo.put_item(\"foobar\", {\"id\": \"abc\", \"b\": True})\n        item = list(self.dynamo.scan(\"foobar\"))[0]\n        self.assertEqual(item[\"b\"], True)\n        self.assertTrue(isinstance(item[\"b\"], bool))", "documentation": "Store and retrieve a boolean", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_19": {"code": "def test_dict(self):\n        \"\"\"Store and retrieve a dict\"\"\"\n        self.make_table()\n        data = {\n            \"i\": 1,\n            \"s\": \"abc\",\n            \"n\": None,\n            \"l\": [\"a\", 1, True],\n            \"b\": False,\n        }\n        self.dynamo.put_item(\"foobar\", {\"id\": \"abc\", \"d\": data})\n        item = list(self.dynamo.scan(\"foobar\"))[0]\n        self.assertEqual(item[\"d\"], data)", "documentation": "Store and retrieve a dict", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_20": {"code": "def test_nested_list(self):\n        \"\"\"Store and retrieve a nested list\"\"\"\n        self.make_table()\n        data = [\n            1,\n            [\n                True,\n                None,\n                \"abc\",\n            ],\n        ]\n        self.dynamo.put_item(\"foobar\", {\"id\": \"abc\", \"l\": data})\n        item = list(self.dynamo.scan(\"foobar\"))[0]\n        self.assertEqual(item[\"l\"], data)", "documentation": "Store and retrieve a nested list", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_21": {"code": "def test_register_encoder(self):\n        \"\"\"Can register a custom encoder\"\"\"\n        from datetime import datetime\n\n        dynamizer = Dynamizer()\n        dynamizer.register_encoder(datetime, lambda d, v: (STRING, v.isoformat()))\n        now = datetime.utcnow()\n        self.assertEqual(dynamizer.raw_encode(now), (STRING, now.isoformat()))", "documentation": "Can register a custom encoder", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_22": {"code": "def test_add_dicts_base_case(self):\n        \"\"\"add_dict where one argument is None returns the other\"\"\"\n        f = object()\n        self.assertEqual(add_dicts(f, None), f)\n        self.assertEqual(add_dicts(None, f), f)", "documentation": "add_dict where one argument is None returns the other", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_23": {"code": "def test_count_repr(self):\n        \"\"\"Count repr\"\"\"\n        count = Count(0, 0)\n        self.assertEqual(repr(count), \"Count(0)\")", "documentation": "Test repr of Count with 0 as the default value", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_24": {"code": "def test_count_subtraction(self):\n        \"\"\"Count subtraction\"\"\"\n        count = Count(4, 2)\n        self.assertEqual(count - 2, 2)", "documentation": "Count subtraction", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_25": {"code": "def test_count_division(self):\n        \"\"\"Count division\"\"\"\n        count = Count(4, 2)\n        self.assertEqual(count / 2, 2)", "documentation": "Count division of a number", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_26": {"code": "def test_count_add_capacity(self):\n        \"\"\"Count addition with consumed_capacity\"\"\"\n        count = Count(4, 2, Capacity(3, 0))\n        count2 = Count(5, 3, Capacity(2, 0))\n        ret = count + count2\n        self.assertEqual(ret, 9)\n        self.assertEqual(ret.scanned_count, 5)\n        self.assertEqual(ret.consumed_capacity.read, 5)", "documentation": "Count addition with consumed_capacity", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_27": {"code": "def test_capacity_format(self):\n        \"\"\"String formatting for Capacity\"\"\"\n        c = Capacity(1, 3)\n        self.assertEqual(str(c), \"R:1.0 W:3.0\")\n        c = Capacity(0, 0)\n        self.assertEqual(str(c), \"0\")", "documentation": "String formatting for Capacity", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_28": {"code": "def test_consumed_capacity_equality(self):\n        \"\"\"ConsumedCapacity addition and equality\"\"\"\n        cap = ConsumedCapacity(\n            \"foobar\",\n            Capacity(0, 10),\n            Capacity(0, 2),\n            {\n                \"l-index\": Capacity(0, 4),\n            },\n            {\n                \"g-index\": Capacity(0, 3),\n            },\n        )\n        c2 = ConsumedCapacity(\n            \"foobar\",\n            Capacity(0, 10),\n            Capacity(0, 2),\n            {\n                \"l-index\": Capacity(0, 4),\n                \"l-index2\": Capacity(0, 7),\n            },\n        )\n\n        self.assertNotEqual(cap, c2)\n        c3 = ConsumedCapacity(\n            \"foobar\",\n            Capacity(0, 10),\n            Capacity(0, 2),\n            {\n                \"l-index\": Capacity(0, 4),\n            },\n            {\n                \"g-index\": Capacity(0, 3),\n            },\n        )\n        self.assertIn(cap, set([c3]))\n        combined = cap + c2\n        self.assertEqual(\n            cap + c2,\n            ConsumedCapacity(\n                \"foobar\",\n                Capacity(0, 20),\n                Capacity(0, 4),\n                {\n                    \"l-index\": Capacity(0, 8),\n                    \"l-index2\": Capacity(0, 7),\n                },\n                {\n                    \"g-index\": Capacity(0, 3),\n                },\n            ),\n        )\n        self.assertIn(str(Capacity(0, 3)), str(combined))", "documentation": "ConsumedCapacity addition and equality", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_29": {"code": "def test_always_continue_query(self):\n        \"\"\"Regression test.\n        If result has no items but does have LastEvaluatedKey, keep querying.\n        \"\"\"\n        conn = MagicMock()\n        conn.dynamizer.decode_keys.side_effect = lambda x: x\n        items = [\"a\", \"b\"]\n        results = [\n            {\"Items\": [], \"LastEvaluatedKey\": {\"foo\": 1, \"bar\": 2}},\n            {\"Items\": [], \"LastEvaluatedKey\": {\"foo\": 1, \"bar\": 2}},\n            {\"Items\": items},\n        ]\n        conn.call.side_effect = lambda *_, **__: results.pop(0)\n        rs = ResultSet(conn, Limit())\n        results = list(rs)\n        self.assertEqual(results, items)", "documentation": "Regression test. If result has no items but does have LastEvaluatedKey keep querying", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_30": {"code": "def tearDown(self):\n        super(TestHooks, self).tearDown()\n        for hooks in self.dynamo._hooks.values():\n            while hooks:\n                hooks.pop()", "documentation": "Removes the last mocked hook from the list of registered hooks", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_31": {"code": "def throw(**_):\n            \"\"\"Throw an exception to terminate the request\"\"\"\n            raise Exception()", "documentation": "Display a message to terminate the request", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "stevearc/dynamo3_tests/__init__.py_32": {"code": "def test_postcall(self):\n        \"\"\"postcall hooks are called after API call\"\"\"\n        hash_key = DynamoKey(\"id\")\n        self.dynamo.create_table(\"foobar\", hash_key=hash_key)\n        calls = []\n\n        def hook(*args):\n            \"\"\"Log the call into a list\"\"\"\n            calls.append(args)\n\n        self.dynamo.subscribe(\"postcall\", hook)\n        self.dynamo.describe_table(\"foobar\")\n        self.assertEqual(len(calls), 1)\n        args = calls[0]\n        self.assertEqual(len(args), 4)\n        conn, command, kwargs, response = args\n        self.assertEqual(conn, self.dynamo)\n        self.assertEqual(command, \"describe_table\")\n        self.assertEqual(kwargs[\"TableName\"], \"foobar\")\n        self.assertEqual(response[\"Table\"][\"TableName\"], \"foobar\")", "documentation": "postcall hooks are called after API call", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "zdvresearch/fast15-paper-extras_ecfs_user_sessions/src/analyze_user_sessions.py_0": {"code": "def __init__(self, user_id):\n        self.user_id = user_id\n        self.from_ts = 0\n        self.till_ts = 0\n        self.get_requests = 0\n        self.reget_requests = 0\n        self.put_requests = 0\n        self.get_bytes = 0\n        self.put_bytes = 0\n        self.rename_requests = 0\n        self.del_requests = 0\n        self.get_dirs = 0\n        self.put_dirs = 0\n        self.put_files_per_dir = 0.0\n        self.get_files_per_dir = 0.0\n        self.window_seconds = 0\n\n        self.file_cnt_gets = Counter()\n        self.file_cnt_puts = Counter()\n        self.dir_cnt_gets = Counter()\n        self.dir_cnt_puts = Counter()\n\n        self.num_ops = 0\n        self.last_ts = 0", "documentation": "User initiated file transfer", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "zdvresearch/fast15-paper-extras_ecfs_user_sessions/src/analyze_user_sessions.py_1": {"code": "def finish(self):\n        self.get_dirs = len(self.dir_cnt_gets)\n        if self.get_dirs > 0:\n            self.get_files_per_dir = float(self.get_requests) / self.get_dirs\n\n        self.put_dirs = len(self.dir_cnt_puts)\n        if self.put_dirs > 0:\n            self.put_files_per_dir = float(self.put_requests) / self.put_dirs\n\n        \"\"\"\n        set reget_counter\n        :param counter: contains [ 1, 1, 5] counts of objects. value > 1 is a re-retrieval.\n        :return:\n        \"\"\"\n        for c in self.file_cnt_gets.values():\n            if c > 1:\n                self.reget_requests += (c - 1)\n\n        # self.announce()\n\n        return \";\".join([str(x) for x in [\n        self.user_id,\n        self.from_ts,\n        self.till_ts,\n        self.till_ts - self.from_ts,\n        self.get_requests,\n        self.reget_requests,\n        self.put_requests,\n        self.get_bytes,\n        self.put_bytes,\n        self.rename_requests,\n        self.del_requests,\n        self.get_dirs,\n        self.put_dirs,\n        self.put_files_per_dir,\n        self.get_files_per_dir,\n        self.window_seconds\n        ]]\n        )", "documentation": "get stats and send to server", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "zdvresearch/fast15-paper-extras_ecfs_user_sessions/src/analyze_user_sessions.py_2": {"code": "def find_clusters(atimes):\n    foo = Counter()\n    bar = dict()\n    for i in xrange(120, 3660, 10):\n        clusters = get_clusters(atimes, i)\n        cs = len(clusters)\n        foo[cs] += 1\n\n        # note first occurance of this cluster size.\n        if cs not in bar:\n            bar[cs] = i\n        # print(len(atimes), i, cs)\n\n    return bar[foo.most_common()[0][0]]", "documentation": "Find the cluster of sizes in each cluster .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "zdvresearch/fast15-paper-extras_ecfs_user_sessions/src/analyze_user_sessions.py_3": {"code": "def analyze_user_session(user_session_file, out_pipeline, target_file_name):\n    with open(user_session_file, 'r') as sf:\n        ops = list()\n        atimes = list()\n\n        for line in sf:\n            op = Operation()\n            op.init(line.strip())\n            ops.append(op)\n            atimes.append(op.ts)\n\n        ops.sort(key=operator.attrgetter('ts'))\n        atimes.sort()\n        window_seconds = find_clusters(atimes)\n\n        session_counter = 1\n\n        uf = os.path.basename(user_session_file)\n        user_id = uf[:uf.find(\".user_session.csv\")]\n\n        session = UserSession(user_id)\n        session.window_seconds = window_seconds\n\n        for op in ops:\n            if session.from_ts == 0:\n                    session.from_ts = op.ts\n                    session.till_ts = op.ts + op.execution_time\n\n            if (session.till_ts + window_seconds) < op.ts:\n                # this session is over, so archive it.\n                out_pipeline.write_to(target_file_name, session.finish())\n                del session\n                session = UserSession(user_id)\n                session.window_seconds = window_seconds\n                session_counter += 1", "documentation": "Analyze the user session file and find clusters of operations and clusters of users .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_0": {"code": "def _debug(m):\n        print >> sys.stderr, m", "documentation": "Print debug message to stderr", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_1": {"code": "def apply_method(self, r, **attr):\n        \"\"\"\n            Apply CRUD methods\n\n            @param r: the S3Request\n            @param attr: dictionary of parameters for the method handler\n\n            @returns: output object to send to the view\n\n            Known means of communicating with this module:\n\n            It expects a URL of the form: /prefix/name/import\n\n            It will interpret the http requests as follows:\n\n            GET     will trigger the upload\n            POST    will trigger either commits or display the import details\n            DELETE  will trigger deletes\n\n            It will accept one of the following control vars:\n            item:   to specify a single item in the import job\n            job:    to specify a job\n            It should not receive both so job takes precedent over item\n\n            For CSV imports, the calling controller can add extra fields\n            to the upload form to add columns to each row in the CSV. To add\n            the extra fields, pass a named parameter \"csv_extra_fields\" to the\n            s3_rest_controller call (or the S3Request call, respectively):\n\n            s3_rest_controller(module, resourcename,\n                               csv_extra_fields=[\n                                    dict(label=\"ColumnLabelInTheCSV\",\n                                         field=field_instance)\n                               ])\n\n            The Field instance \"field\" will be added to the upload form, and\n            the user input will be added to each row of the CSV under the\n            label as specified. If the \"field\" validator has options, the\n            input value will be translated into the option representation,\n            otherwise the value will be used as-is.\n\n            Note that the \"label\" in the dict is the column label in the CSV,\n            whereas the field label for the form is to be set in the Field\n            instance passed as \"field\".\n\n            You can add any arbitrary number of csv_extra_fields to the list.\n\n            Additionally, you may want to allow the user to choose whether\n            the import shall first remove all existing data in the target\n            table. To do so, pass a label for the \"replace_option\" to the\n            request:\n\n            s3_rest_controller(module, resourcename,\n                               replace_option=T(\"Remove existing data before import\"))\n\n            This will add the respective checkbox to the upload form.\n\n            You may also want to provide a link to download a CSV template from\n            the upload form. To do that, add the resource name to the request\n            attributes:\n\n            s3_rest_controller(module, resourcename,\n                               csv_template=\"<resourcename>\")\n\n            This will provide a link to:\n                - static/formats/s3csv/<controller>/<resourcename>.csv\n            at the top of the upload form.\n\n        \"\"\"\n\n        _debug(\"S3Importer.apply_method(%s)\" % r)\n\n        # Messages\n        T = current.T\n        messages = self.messages = Messages(T)\n        messages.download_template = \"Download Template\"\n        messages.invalid_file_format = \"Invalid File Format\"\n        messages.unsupported_file_type = \"Unsupported file type of %s\"\n        messages.stylesheet_not_found = \"No Stylesheet %s could be found to manage the import file.\"\n        messages.no_file = \"No file submitted\"\n        messages.file_open_error = \"Unable to open the file %s\"\n        messages.file_not_found = \"The file to upload is missing\"\n        messages.no_records_to_import = \"No records to import\"\n        messages.no_job_to_delete = \"No job to delete, maybe it has already been deleted.\"\n        messages.title_job_read = \"Details of the selected import job\"\n        messages.title_job_list = \"List of import items\"\n        messages.file_uploaded = \"Import file uploaded\"\n        messages.upload_submit_btn = \"Upload Data File\"\n        messages.open_btn = \"Open\"\n        messages.view_btn = \"View\"\n        messages.delete_btn = \"Delete\"\n        messages.item_show_details = \"Display Details\"\n        messages.job_total_records = \"Total records in the Import Job\"\n        messages.job_records_selected = \"Records selected\"\n        messages.job_deleted = \"Import job deleted\"\n        messages.job_completed = \"Job run on %s. With result of (%s)\"\n        messages.import_file = \"Import File\"\n        messages.import_file_comment = \"Upload a file formatted according to the Template.\"\n        messages.user_name = \"User Name\"\n        messages.commit_total_records_imported = \"%s records imported\"\n        messages.commit_total_records_ignored = \"%s records ignored\"\n        messages.commit_total_errors = \"%s records in error\"\n\n        try:\n            self.uploadTitle = current.response.s3.crud_strings[self.tablename].title_upload\n        except:\n            self.uploadTitle = T(\"Upload a %s import file\" % r.function)\n\n        # @todo: correct to switch this off for the whole session?\n        current.session.s3.ocr_enabled = False\n\n        # Reset all errors/warnings\n        self.error = None\n        self.warning = None\n\n        # CSV upload configuration\n        if \"csv_stylesheet\" in attr:\n            self.csv_stylesheet = attr[\"csv_stylesheet\"]\n        else:\n            self.csv_stylesheet = None\n        self.csv_extra_fields = None\n        self.csv_extra_data = None\n\n        # Environment\n        self.controller = r.controller\n        self.function = r.function\n\n        # Target table for the data import\n        self.controller_resource = self.resource\n        self.controller_table = self.table\n        self.controller_tablename = self.tablename\n\n        # Table for uploads\n        self.__define_table()\n        self.upload_resource = None\n        self.item_resource = None\n\n        # XSLT Path\n        self.xslt_path = os.path.join(r.folder, r.XSLT_PATH)\n        self.xslt_extension = r.XSLT_EXTENSION\n\n        # Check authorization\n        authorised = self.permit(\"create\", self.upload_tablename) and \\\n                     self.permit(\"create\", self.controller_tablename)\n        if not authorised:\n            if r.method is not None:\n                r.unauthorised()\n            else:\n                return dict(form=None)\n\n        # @todo: clean this up\n        source = None\n        transform = None\n        upload_id = None\n        items = None\n        # @todo get the data from either get_vars or post_vars appropriately\n        #       for post -> commit_items would need to add the uploadID\n        if \"transform\" in r.get_vars:\n            transform = r.get_vars[\"transform\"]\n        if \"filename\" in r.get_vars:\n            source = r.get_vars[\"filename\"]\n        if \"job\" in r.post_vars:\n            upload_id = r.post_vars[\"job\"]\n        elif \"job\" in r.get_vars:\n            upload_id = r.get_vars[\"job\"]\n        items = self._process_item_list(upload_id, r.vars)\n        if \"delete\" in r.get_vars:\n            r.http = \"DELETE\"\n\n        # If we have an upload ID, then get upload and import job\n        self.upload_id = upload_id\n        query = (self.upload_table.id == upload_id)\n        self.upload_job = current.db(query).select(limitby=(0, 1)).first()\n        if self.upload_job:\n            self.job_id = self.upload_job.job_id\n        else:\n            self.job_id = None\n\n        # Now branch off to the appropriate controller function\n        if r.http == \"GET\":\n            if source != None:\n                self.commit(source, transform)\n                output = self.upload(r, **attr)\n            if upload_id != None:\n                output = self.display_job(upload_id)\n            else:\n                output = self.upload(r, **attr)\n        elif r.http == \"POST\":\n            if items != None:\n                output = self.commit_items(upload_id, items)\n            else:\n                output = self.generate_job(r, **attr)\n        elif r.http == \"DELETE\":\n            if upload_id != None:\n                output = self.delete_job(upload_id)\n        else:\n            r.error(405, current.manager.ERROR.BAD_METHOD)\n\n        return output", "documentation": "This method applies all the CRUD methods to the import job .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_2": {"code": "def upload(self, r, **attr):\n        \"\"\"\n            This will display the upload form\n            It will ask for a file to be uploaded or for a job to be selected.\n\n            If a file is uploaded then it will guess at the file type and\n            ask for the transform file to be used. The transform files will\n            be in a dataTable with the module specific files shown first and\n            after those all other known transform files. Once the transform\n            file is selected the import process can be started which will\n            generate an importJob, and a \"POST\" method will occur\n\n            If a job is selected it will have two actions, open and delete.\n            Open will mean that a \"GET\" method will occur, with the job details\n            passed in.\n            Whilst the delete action will trigger a \"DELETE\" method.\n        \"\"\"\n\n        _debug(\"S3Importer.upload()\")\n\n        request = self.request\n\n        form = self._upload_form(r, **attr)\n        output = self._create_upload_dataTable()\n        if request.representation == \"aadata\":\n            return output\n\n        output.update(form=form, title=self.uploadTitle)\n        return output", "documentation": "Uploads a file or a job to Amazon S3", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_3": {"code": "def generate_job(self, r, **attr):\n        \"\"\"\n            Generate an ImportJob from the submitted upload form\n        \"\"\"\n\n        _debug(\"S3Importer.display()\")\n\n        response = current.response\n        s3 = response.s3\n\n        db = current.db\n        table = self.upload_table\n\n        title=self.uploadTitle\n        form = self._upload_form(r, **attr)\n\n        r = self.request\n        r.read_body()\n        sfilename = form.vars.file\n        try:\n            ofilename = r.post_vars[\"file\"].filename\n        except:\n            form.errors.file = self.messages.no_file\n\n        if form.errors:\n            response.flash = \"\"\n            output = self._create_upload_dataTable()\n            output.update(form=form, title=title)\n\n        elif not sfilename or \\\n             ofilename not in r.files or r.files[ofilename] is None:\n            response.flash = \"\"\n            response.error = self.messages.file_not_found\n            output = self._create_upload_dataTable()\n            output.update(form=form, title=title)\n\n        else:\n            output = dict()\n            query = (table.file == sfilename)\n            db(query).update(controller=self.controller,\n                             function=self.function,\n                             filename=ofilename,\n                             user_id=current.session.auth.user.id)\n            # must commit here to separate this transaction from\n            # the trial import phase which will be rolled back.\n            db.commit()\n\n            extension = ofilename.rsplit(\".\", 1).pop()\n            if extension not in (\"csv\", \"xls\"):\n                response.flash = None\n                response.error = self.messages.invalid_file_format\n                return self.upload(r, **attr)\n\n            upload_file = r.files[ofilename]\n            if extension == \"xls\":\n                if \"xls_parser\" in s3:\n                    upload_file.seek(0)\n                    upload_file = s3.xls_parser(upload_file.read())\n                    extension = \"csv\"\n\n            if upload_file is None:\n                response.flash = None\n                response.error = self.messages.file_not_found\n                return self.upload(r, **attr)\n            else:\n                upload_file.seek(0)\n\n            row = db(query).select(table.id, limitby=(0, 1)).first()\n            upload_id = row.id\n            if \"single_pass\" in r.vars:\n                single_pass = r.vars[\"single_pass\"]\n            else:\n                single_pass = None\n            self._generate_import_job(upload_id,\n                                      upload_file,\n                                      extension,\n                                      commit_job = single_pass)\n            if upload_id is None:\n                row = db(query).update(status = 2) # in error\n                if self.error != None:\n                    response.error = self.error\n                if self.warning != None:\n                    response.warning = self.warning\n                response.flash = \"\"\n                return self.upload(r, **attr)\n            else:\n                if single_pass:\n                    current.session.flash = self.messages.file_uploaded\n                    # For a single pass retain the vars from the original URL\n                    next_URL = URL(r=self.request,\n                                   f=self.function,\n                                   args=[\"import\"],\n                                   vars=current.request.get_vars\n                                  )\n                    redirect(next_URL)\n                s3.dataTable_vars = {\"job\" : upload_id}\n                return self.display_job(upload_id)\n        return output", "documentation": "Generate an ImportJob from the upload form", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_4": {"code": "def display_job(self, upload_id):\n        \"\"\"\n            @todo: docstring?\n        \"\"\"\n\n        _debug(\"S3Importer.display_job()\")\n\n        request = self.request\n        response = current.response\n\n        db = current.db\n        table = self.upload_table\n        job_id = self.job_id\n        output = dict()\n        if job_id == None:\n            # redirect to the start page (removes all vars)\n            query = (table.id == upload_id)\n            row = db(query).update(status = 2) # in error\n            current.session.warning = self.messages.no_records_to_import\n            redirect(URL(r=request, f=self.function, args=[\"import\"]))\n\n        # Get the status of the upload job\n        query = (table.id == upload_id)\n        row = db(query).select(table.status,\n                               table.modified_on,\n                               table.summary_added,\n                               table.summary_error,\n                               table.summary_ignored,\n                               limitby=(0, 1)).first()\n        status = row.status\n        # completed display details\n        if status == 3: # Completed\n            # @todo currently this is an unnecessary server call,\n            #       change for completed records to be a display details\n            #       and thus avoid the round trip.\n            #       but keep this code to protect against hand-crafted URLs\n            #       (and the 'go back' syndrome on the browser)\n            result = (row.summary_added,\n                      row.summary_error,\n                      row.summary_ignored,\n                     )\n            self._display_completed_job(result, row.modified_on)\n            redirect(URL(r=request, f=self.function, args=[\"import\"]))\n        # otherwise display import items\n        response.view = self._view(request, \"list.html\")\n\n        output = self._create_import_item_dataTable(upload_id, job_id)\n        if request.representation == \"aadata\":\n            return output\n\n        if response.s3.error_report:\n            error_report = \"Errors|\" + \"|\".join(response.s3.error_report)\n            error_tip = A(\"All Errors\",\n                          _class=\"errortip\",\n                          _title=error_report)\n        else:\n            # @todo: restore the error tree from all items?\n            error_tip = \"\"\n\n        rowcount = len(self._get_all_items(upload_id))\n        rheader = DIV(TABLE(\n            TR(\n                TH(\"%s: \" % self.messages.job_total_records),\n                TD(rowcount, _id=\"totalAvaliable\"),\n                TH(\"%s: \" % self.messages.job_records_selected),\n                TD(0, _id=\"totalSelected\"),\n                TH(error_tip)\n              ),\n        ))\n\n        output[\"title\"] = self.messages.title_job_read\n        output[\"rheader\"] = rheader\n        output[\"subtitle\"] = self.messages.title_job_list\n\n        return output", "documentation": "Display the details of the job .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_5": {"code": "def commit(self, source, transform):\n        \"\"\"\n            @todo: docstring?\n        \"\"\"\n\n        _debug(\"S3Importer.commit(%s, %s)\" % (source, transform))\n\n        db = current.db\n        session = current.session\n        request = self.request\n\n        try:\n            openFile = open(source, \"r\")\n        except:\n            session.error = self.messages.file_open_error % source\n            redirect(URL(r=request, f=self.function))\n\n        # @todo: manage different file formats\n        # @todo: find file format from request.extension\n        fileFormat = \"csv\"\n\n        # insert data in the table and get the ID\n        try:\n            user = session.auth.user.id\n        except:\n            user = None\n\n        upload_id = self.upload_table.insert(controller=self.controller,\n                                             function=self.function,\n                                             filename = source,\n                                             user_id = user,\n                                             status = 1)\n        db.commit()\n\n        # create the import job\n        result = self._generate_import_job(upload_id,\n                                           openFile,\n                                           fileFormat,\n                                           stylesheet=transform\n                                          )\n        if result == None:\n            if self.error != None:\n                if session.error == None:\n                    session.error = self.error\n                else:\n                    session.error += self.error\n            if self.warning != None:\n                if session.warning == None:\n                    session.warning = self.warning\n                else:\n                    session.warning += self.warning\n        else:\n            items = self._get_all_items(upload_id, True)\n            # commit the import job\n            self._commit_import_job(upload_id, items)\n            result = self._update_upload_job(upload_id)\n\n            # get the results and display\n            msg = \"%s : %s %s %s\" % (source,\n                                     self.messages.commit_total_records_imported,\n                                     self.messages.commit_total_errors,\n                                     self.messages.commit_total_records_ignored)\n            msg = msg % result\n\n            if session.flash == None:\n                session.flash = msg\n            else:\n                session.flash += msg\n\n        # @todo: return the upload_id?", "documentation": "Commit import to S3", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_6": {"code": "def commit_items(self, upload_id, items):\n        \"\"\"\n            @todo: docstring?\n        \"\"\"\n\n        _debug(\"S3Importer.commit_items(%s, %s)\" % (upload_id, items))\n        # Save the import items\n        self._commit_import_job(upload_id, items)\n        # Update the upload table\n        # change the status to completed\n        # record the summary details\n        # delete the upload file\n        result = self._update_upload_job(upload_id)\n        # redirect to the start page (removes all vars)\n        self._display_completed_job(result)\n        redirect(URL(r=self.request, f=self.function, args=[\"import\"]))", "documentation": "Commit import items for a given upload_id", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_7": {"code": "def delete_job(self, upload_id):\n        \"\"\"\n            Delete an uploaded file and the corresponding import job\n\n            @param upload_id: the upload ID\n        \"\"\"\n\n        _debug(\"S3Importer.delete_job(%s)\" % (upload_id))\n\n        db = current.db\n\n        request = self.request\n        resource = request.resource # use self.resource?\n        response = current.response\n\n        # Get the import job ID\n        job_id = self.job_id\n\n        # Delete the import job (if any)\n        if job_id:\n            result = resource.import_xml(None,\n                                         id = None,\n                                         tree = None,\n                                         job_id = job_id,\n                                         delete_job = True)\n        # @todo: check result\n\n        # now delete the upload entry\n        query = (self.upload_table.id == upload_id)\n        count = db(query).delete()\n        # @todo: check that the record has been deleted\n\n        # Now commit the changes\n        db.commit()\n\n        result = count\n\n        # return to the main import screen\n        # @todo: check result properly\n        if result == False:\n            response.warning = self.messages.no_job_to_delete\n        else:\n            response.flash = self.messages.job_deleted\n\n        # redirect to the start page (remove all vars)\n        self.next = self.request.url(vars=dict())\n        return", "documentation": "Delete an uploaded file and the corresponding import job", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_8": {"code": "def _upload_form(self, r, **attr):\n        \"\"\"\n            Create and process the upload form, including csv_extra_fields\n        \"\"\"\n\n        EXTRA_FIELDS = \"csv_extra_fields\"\n        TEMPLATE = \"csv_template\"\n        REPLACE_OPTION = \"replace_option\"\n\n        session = current.session\n        response = current.response\n        s3 = response.s3\n        request = self.request\n        table = self.upload_table\n\n        formstyle = s3.crud.formstyle\n        response.view = self._view(request, \"list_create.html\")\n\n        if REPLACE_OPTION in attr:\n            replace_option = attr[REPLACE_OPTION]\n            if replace_option is not None:\n                table.replace_option.readable = True\n                table.replace_option.writable = True\n                table.replace_option.label = replace_option\n\n        fields = [f for f in table if f.readable or f.writable and not f.compute]\n        if EXTRA_FIELDS in attr:\n            extra_fields = attr[EXTRA_FIELDS]\n            if extra_fields is not None:\n                fields.extend([f[\"field\"] for f in extra_fields if \"field\" in f])\n            self.csv_extra_fields = extra_fields\n        labels, required = s3_mark_required(fields)\n        if required:\n            s3.has_required = True\n\n        form = SQLFORM.factory(table_name=self.UPLOAD_TABLE_NAME,\n                               labels=labels,\n                               formstyle=formstyle,\n                               upload = os.path.join(request.folder, \"uploads\", \"imports\"),\n                               separator = \"\",\n                               message=self.messages.file_uploaded,\n                               *fields)\n\n        args = [\"s3csv\"]\n        template = attr.get(TEMPLATE, True)\n        if template is True:\n            args.extend([self.controller, \"%s.csv\" % self.function])\n        elif isinstance(template, basestring):\n            args.extend([self.controller, \"%s.csv\" % template])\n        elif isinstance(template, (tuple, list)):\n            args.extend(template[:-1])\n            args.append(\"%s.csv\" % template[-1])\n        else:\n            template = None\n        if template is not None:\n            url = URL(r=request, c=\"static\", f=\"formats\", args=args)\n            try:\n                # only add the download link if the template can be opened\n                open(\"%s/../%s\" % (r.folder, url))\n                form[0][0].insert(0, TR(TD(A(self.messages.download_template,\n                                             _href=url)),\n                                        _id=\"template__row\"))\n            except:\n                pass\n\n        if form.accepts(r.post_vars, session,\n                        formname=\"upload_form\"):\n            upload_id = table.insert(**table._filter_fields(form.vars))\n            if self.csv_extra_fields:\n                self.csv_extra_data = Storage()\n                for f in self.csv_extra_fields:\n                    label = f.get(\"label\", None)\n                    if not label:\n                        continue\n                    field = f.get(\"field\", None)\n                    value = f.get(\"value\", None)\n                    if field:\n                        if field.name in form.vars:\n                            data = form.vars[field.name]\n                        else:\n                            data = field.default\n                        value = data\n                        requires = field.requires\n                        if not isinstance(requires, (list, tuple)):\n                            requires = [requires]\n                        if requires:\n                            requires = requires[0]\n                            if isinstance(requires, IS_EMPTY_OR):\n                                requires = requires.other\n                            try:\n                                options = requires.options()\n                            except:\n                                pass\n                            else:\n                                for k, v in options:\n                                    if k == str(data):\n                                        value = v\n                    elif value is None:\n                        continue\n                    self.csv_extra_data[label] = value\n        s3.no_formats = True\n        return form", "documentation": "Create and process the upload form", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_9": {"code": "def _create_upload_dataTable(self):\n        \"\"\"\n            List of previous Import jobs\n        \"\"\"\n\n        db = current.db\n        request = self.request\n        controller = self.controller\n        function = self.function\n        s3 = current.response.s3\n\n        table = self.upload_table\n        s3.filter = (table.controller == controller) & \\\n                    (table.function == function)\n        fields = [\"id\",\n                  \"filename\",\n                  \"created_on\",\n                  \"user_id\",\n                  \"replace_option\",\n                  \"status\"]\n\n        self._use_upload_table()\n\n        # Hide the list of prior uploads for now\n        #output = self._dataTable(fields, sort_by = [[2,\"desc\"]])\n        output = dict()\n\n        self._use_controller_table()\n\n        if request.representation == \"aadata\":\n            return output\n\n        query = (table.status != 3) # Status of Pending or in-Error\n        rows = db(query).select(table.id)\n        restrictOpen = [str(row.id) for row in rows]\n        query = (table.status == 3) # Status of Completed\n        rows = db(query).select(table.id)\n        restrictView = [str(row.id) for row in rows]\n\n        s3.actions = [\n                    dict(label=str(self.messages.open_btn),\n                         _class=\"action-btn\",\n                         url=URL(r=request,\n                                 c=controller,\n                                 f=function,\n                                 args=[\"import\"],\n                                 vars={\"job\":\"[id]\"}),\n                         restrict = restrictOpen\n\n                         ),\n                    dict(label=str(self.messages.view_btn),\n                         _class=\"action-btn\",\n                         url=URL(r=request,\n                                 c=controller,\n                                 f=function,\n                                 args=[\"import\"],\n                                 vars={\"job\":\"[id]\"}),\n                         restrict = restrictView\n                         ),\n                    dict(label=str(self.messages.delete_btn),\n                         _class=\"delete-btn\",\n                         url=URL(r=request,\n                                 c=controller,\n                                 f=function,\n                                 args=[\"import\"],\n                                 vars={\"job\":\"[id]\",\n                                       \"delete\":\"True\"\n                                      }\n                                )\n                         ),\n                  ]\n        # Display an Error if no job is attached with this record\n        query = (table.status == 1) # Pending\n        rows = db(query).select(table.id)\n        s3.dataTableStyleAlert = [str(row.id) for row in rows]\n        query = (table.status == 2) # in error\n        rows = db(query).select(table.id)\n        s3.dataTableStyleWarning = [str(row.id) for row in rows]\n\n        return output", "documentation": "Creates a dataTable with the uploads for the current controller and function", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_10": {"code": "def _create_import_item_dataTable(self, upload_id, job_id):\n        \"\"\"\n            @todo: docstring?\n        \"\"\"\n\n        s3 = current.response.s3\n\n        represent = {\"element\" : self._item_element_represent}\n        self._use_import_item_table(job_id)\n\n        # Add a filter to the dataTable query\n        s3.filter = (self.table.job_id == job_id) & \\\n                    (self.table.tablename == self.controller_tablename)\n\n        # Get a list of the records that have an error of None\n        query =  (self.table.job_id == job_id) & \\\n                 (self.table.tablename == self.controller_tablename)\n        rows = current.db(query).select(self.table.id, self.table.error)\n        select_list = []\n        error_list = []\n        for row in rows:\n            if row.error:\n                error_list.append(str(row.id))\n            else:\n                select_list.append(\"%s\" % row.id)\n        select_id = \",\".join(select_list)\n\n        output = self._dataTable([\"id\", \"element\", \"error\"],\n                                 sort_by = [[1, \"asc\"]],\n                                 represent=represent)\n\n        self._use_controller_table()\n\n        if self.request.representation == \"aadata\":\n            return output\n\n        # Highlight rows in error in red\n        s3.dataTableStyleWarning = error_list\n\n        s3.dataTableSelectable = True\n        s3.dataTablePostMethod = True\n        table = output[\"items\"]\n        job = INPUT(_type=\"hidden\", _id=\"importUploadID\", _name=\"job\",\n                    _value=\"%s\" % upload_id)\n        mode = INPUT(_type=\"hidden\", _id=\"importMode\", _name=\"mode\",\n                     _value=\"Inclusive\")\n        # only select the rows with no errors\n        selected = INPUT(_type=\"hidden\", _id=\"importSelected\",\n                         _name=\"selected\", _value=\"[%s]\" % select_id)\n        form = FORM(table, job, mode, selected)\n        output[\"items\"] = form\n        s3.dataTableSelectSubmitURL = \"import?job=%s&\" % upload_id\n        s3.actions = [\n                        dict(label= str(self.messages.item_show_details),\n                             _class=\"action-btn\",\n                             _jqclick=\"$('.importItem.'+id).toggle();\",\n                             ),\n                      ]\n        return output", "documentation": "Returns a dataTable with elements for an import job .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_11": {"code": "def _generate_import_job(self,\n                             upload_id,\n                             openFile,\n                             fileFormat,\n                             stylesheet=None,\n                             commit_job=False):\n        \"\"\"\n            This will take a s3_import_upload record and\n            generate the importJob\n\n            @param uploadFilename: The name of the uploaded file\n\n            @todo: complete parameter descriptions\n        \"\"\"\n\n        _debug(\"S3Importer._generate_import_job(%s, %s, %s, %s)\" % (upload_id,\n                                                                openFile,\n                                                                fileFormat,\n                                                                stylesheet\n                                                               )\n              )\n\n        db = current.db\n        request = self.request\n        resource = request.resource\n\n        # ---------------------------------------------------------------------\n        # CSV\n        if fileFormat == \"csv\" or fileFormat == \"comma-separated-values\":\n\n            fmt = \"csv\"\n            src = openFile\n\n        # ---------------------------------------------------------------------\n        # XML\n        # @todo: implement\n        #elif fileFormat == \"xml\":\n\n        # ---------------------------------------------------------------------\n        # S3JSON\n        # @todo: implement\n        #elif fileFormat == \"s3json\":\n\n        # ---------------------------------------------------------------------\n        # PDF\n        # @todo: implement\n        #elif fileFormat == \"pdf\":\n\n        # ---------------------------------------------------------------------\n        # Unsupported Format\n        else:\n            msg = self.messages.unsupported_file_type % fileFormat\n            self.error = msg\n            _debug(msg)\n            return None\n\n        # Get the stylesheet\n        if stylesheet == None:\n            stylesheet = self._get_stylesheet()\n        if stylesheet == None:\n            return None\n\n        # before calling import tree ensure the db.table is the controller_table\n        self.table = self.controller_table\n        self.tablename = self.controller_tablename\n\n        # Pass stylesheet arguments\n        args = Storage()\n        mode = request.get_vars.get(\"xsltmode\", None)\n        if mode is not None:\n            args.update(mode=mode)\n\n        # Generate the import job\n        resource.import_xml(src,\n                            format=fmt,\n                            extra_data=self.csv_extra_data,\n                            stylesheet=stylesheet,\n                            ignore_errors = True,\n                            commit_job = commit_job,\n                            **args)\n\n        job = resource.job\n        if job is None:\n            if resource.error:\n                # Error\n                self.error = resource.error\n                return None\n            else:\n                # Nothing to import\n                self.warning = self.messages.no_records_to_import\n                return None\n        else:\n            # Job created\n            job_id = job.job_id\n            errors = current.xml.collect_errors(job)\n            if errors:\n                current.response.s3.error_report = errors\n            query = (self.upload_table.id == upload_id)\n            result = db(query).update(job_id=job_id)\n            # @todo: add check that result == 1, if not we are in error\n            # Now commit the changes\n            db.commit()\n\n        self.job_id = job_id\n        return True", "documentation": "This will take a s3_import_upload record and generate a importJob", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_12": {"code": "def _get_stylesheet(self, file_format=\"csv\"):\n        \"\"\"\n            Get the stylesheet for transformation of the import\n\n            @param file_format: the import source file format\n        \"\"\"\n\n        if file_format == \"csv\":\n            xslt_path = os.path.join(self.xslt_path, \"s3csv\")\n        else:\n            xslt_path = os.path.join(self.xslt_path, file_format, \"import.xsl\")\n            return xslt_path\n\n        # Use the \"csv_stylesheet\" parameter to override the CSV stylesheet subpath\n        # and filename, e.g.\n        #       s3_rest_controller(module, resourcename,\n        #                          csv_stylesheet=(\"inv\", \"inv_item.xsl\"))\n        if self.csv_stylesheet:\n            if isinstance(self.csv_stylesheet, (tuple, list)):\n                stylesheet = os.path.join(xslt_path,\n                                          *self.csv_stylesheet)\n            else:\n                stylesheet = os.path.join(xslt_path,\n                                          self.controller,\n                                          self.csv_stylesheet)\n        else:\n            xslt_filename = \"%s.%s\" % (self.function, self.xslt_extension)\n            stylesheet = os.path.join(xslt_path,\n                                      self.controller,\n                                      xslt_filename)\n\n        if os.path.exists(stylesheet) is False:\n            msg = self.messages.stylesheet_not_found % stylesheet\n            self.error = msg\n            _debug(msg)\n            return None\n\n        return stylesheet", "documentation": "Get the stylesheet for transformation of the import", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_13": {"code": "def _commit_import_job(self, upload_id, items):\n        \"\"\"\n            This will save all of the selected import items\n\n            @todo: parameter descriptions?\n        \"\"\"\n\n        _debug(\"S3Importer._commit_import_job(%s, %s)\" % (upload_id, items))\n\n        db = current.db\n        resource = self.request.resource\n\n        # Load the items from the s3_import_item table\n        self.importDetails = dict()\n\n        table = self.upload_table\n        query = (table.id == upload_id)\n        row = db(query).select(table.job_id,\n                               table.replace_option,\n                               limitby=(0, 1)).first()\n        if row is None:\n            return False\n        else:\n            job_id = row.job_id\n            current.response.s3.import_replace = row.replace_option\n\n        itemTable = S3ImportJob.define_item_table()\n\n        if itemTable != None:\n            #****************************************************************\n            # EXPERIMENTAL\n            # This doesn't delete related items\n            # but import_tree will tidy it up later\n            #****************************************************************\n            # get all the items selected for import\n            rows = self._get_all_items(upload_id, as_string=True)\n\n            # loop through each row and delete the items not required\n            self._store_import_details(job_id, \"preDelete\")\n            for id in rows:\n                if str(id) not in items:\n                    # @todo: replace with a helper method from the API\n                    _debug(\"Deleting item.id = %s\" % id)\n                    query = (itemTable.id == id)\n                    db(query).delete()\n\n            #****************************************************************\n            # EXPERIMENTAL\n            #****************************************************************\n\n            # set up the table we will import data into\n            self.table = self.controller_table\n            self.tablename = self.controller_tablename\n\n            self._store_import_details(job_id, \"preImportTree\")\n\n            # Now commit the remaining items\n            msg = resource.import_xml(None,\n                                      job_id = job_id,\n                                      ignore_errors = True)\n            return resource.error is None", "documentation": "Commits the import job by deleting the selected items and importing them into the controller table .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_14": {"code": "def _store_import_details(self, job_id, key):\n        \"\"\"\n            This will store the details from an importJob\n\n            @todo: parameter descriptions?\n        \"\"\"\n\n        _debug(\"S3Importer._store_import_details(%s, %s)\" % (job_id, key))\n\n        itemTable = S3ImportJob.define_item_table()\n\n        query = (itemTable.job_id == job_id)  & \\\n                (itemTable.tablename == self.controller_tablename)\n        rows = current.db(query).select(itemTable.data, itemTable.error)\n        items = [dict(data=row.data, error=row.error) for row in rows]\n\n        self.importDetails[key] = items", "documentation": "Store import details from an importJob", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_15": {"code": "def _update_upload_job(self, upload_id):\n        \"\"\"\n            This will record the results from the import, and change the\n            status of the upload job\n\n            @todo: parameter descriptions?\n            @todo: report errors in referenced records, too\n        \"\"\"\n\n        _debug(\"S3Importer._update_upload_job(%s)\" % (upload_id))\n\n        request = self.request\n        resource = request.resource\n        db = current.db\n\n        totalPreDelete = len(self.importDetails[\"preDelete\"])\n        totalPreImport = len(self.importDetails[\"preImportTree\"])\n        totalIgnored = totalPreDelete - totalPreImport\n\n        if resource.error_tree is None:\n            totalErrors = 0\n        else:\n            totalErrors = len(resource.error_tree.findall(\n                            \"resource[@name='%s']\" % resource.tablename))\n\n        totalRecords = totalPreImport - totalErrors\n        if totalRecords < 0:\n            totalRecords = 0\n\n        query = (self.upload_table.id == upload_id)\n        result = db(query).update(summary_added=totalRecords,\n                                  summary_error=totalErrors,\n                                  summary_ignored = totalIgnored,\n                                  status = 3)\n\n        # Now commit the changes\n        db.commit()\n        return (totalRecords, totalErrors, totalIgnored)", "documentation": "This will record the results from the import and change the status of the upload job", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_16": {"code": "def _display_completed_job(self, totals, timestmp=None):\n        \"\"\"\n            Generate a summary flash message for a completed import job\n\n            @param totals: the job totals as tuple\n                           (total imported, total errors, total ignored)\n            @param timestmp: the timestamp of the completion\n        \"\"\"\n\n        session = current.session\n\n        msg = \"%s - %s - %s\" % \\\n              (self.messages.commit_total_records_imported,\n               self.messages.commit_total_errors,\n               self.messages.commit_total_records_ignored)\n        msg = msg % totals\n\n        if timestmp != None:\n            session.flash = self.messages.job_completed % \\\n                            (self.date_represent(timestmp), msg)\n        elif totals[1] is not 0:\n            session.error = msg\n        elif totals[2] is not 0:\n            session.warning = msg\n        else:\n            session.flash = msg", "documentation": "Generate a summary flash message for a completed import job", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_17": {"code": "def _dataTable(self,\n                   list_fields = [],\n                   sort_by = [[1, \"asc\"]],\n                   represent={},\n                  ):\n        \"\"\"\n            Method to get the data for the dataTable\n            This can be either a raw html representation or\n            and ajax call update\n            Additional data will be cached to limit calls back to the server\n\n            @param list_fields: list of field names\n            @param sort_by: list of sort by columns\n            @param represent: a dict of field callback functions used\n                              to change how the data will be displayed\n\n            @return: a dict()\n               In html representations this will be a table of the data\n               plus the sortby instructions\n               In ajax this will be a json response\n\n               In addition the following values will be made available:\n               totalRecords         Number of records in the filtered data set\n               totalDisplayRecords  Number of records to display\n               start                Start point in the ordered data set\n               limit                Number of records in the ordered set\n               NOTE: limit - totalDisplayRecords = total cached\n        \"\"\"\n\n        # ********************************************************************\n        # Common tasks\n        # ********************************************************************\n        db = current.db\n        session = current.session\n        request = self.request\n        response = current.response\n        resource = self.resource\n        s3 = response.s3\n        representation = request.representation\n        table = self.table\n        tablename = self.tablename\n        vars = request.get_vars\n        output = dict()\n\n        # Check permission to read this table\n        authorised = self.permit(\"read\", tablename)\n        if not authorised:\n            request.unauthorised()\n\n        # List of fields to select from\n        # fields is a list of Field objects\n        # list_field is a string list of field names\n        if list_fields == []:\n            fields = resource.readable_fields()\n        else:\n            fields = [table[f] for f in list_fields if f in table.fields]\n        if not fields:\n            fields = []\n\n        # attach any represent callbacks\n        for f in fields:\n            if f.name in represent:\n                f.represent = represent[f.name]\n\n        # Make sure that we have the table id as the first column\n        if fields[0].name != table.fields[0]:\n            fields.insert(0, table[table.fields[0]])\n\n        list_fields = [f.name for f in fields]\n\n        # Filter\n        if s3.filter is not None:\n            self.resource.add_filter(s3.filter)\n\n        # ********************************************************************\n        # ajax call\n        # ********************************************************************\n        if representation == \"aadata\":\n            start = vars.get(\"iDisplayStart\", None)\n            limit = vars.get(\"iDisplayLength\", None)\n            if limit is not None:\n                try:\n                    start = int(start)\n                    limit = int(limit)\n                except ValueError:\n                    start = None\n                    limit = None # use default\n            else:\n                start = None # use default\n            # Using the sort variables sent from dataTables\n            if vars.iSortingCols:\n                orderby = self.ssp_orderby(resource, list_fields)\n\n            # Echo\n            sEcho = int(vars.sEcho or 0)\n\n            # Get the list\n            items = resource.sqltable(fields=list_fields,\n                                      start=start,\n                                      limit=limit,\n                                      orderby=orderby,\n                                      download_url=self.download_url,\n                                      as_page=True) or []\n            # Ugly hack to change any occurrence of [id] with the true id\n            # Needed because the represent doesn't know the id\n            for i in range(len(items)):\n                id = items[i][0]\n                for j in range(len(items[i])):\n                    new = items[i][j].replace(\"[id]\",id)\n                    items[i][j] = new\n            totalrows = self.resource.count()\n            result = dict(sEcho = sEcho,\n                          iTotalRecords = totalrows,\n                          iTotalDisplayRecords = totalrows,\n                          aaData = items)\n\n            output = jsons(result)\n\n        # ********************************************************************\n        # html 'initial' call\n        # ********************************************************************\n        else: # catch all\n            start = 0\n            limit = 1\n            # Sort by\n            vars[\"iSortingCols\"] = len(sort_by)\n\n            # generate the dataTables.js variables for sorting\n            index = 0\n            for col in sort_by:\n                colName = \"iSortCol_%s\" % str(index)\n                colValue = col[0]\n                dirnName = \"sSortDir_%s\" % str(index)\n                if len(col) > 1:\n                    dirnValue = col[1]\n                else:\n                    dirnValue = \"asc\"\n                vars[colName] = colValue\n                vars[dirnName] = dirnValue\n            # Now using these sort variables generate the order by statement\n            orderby = self.ssp_orderby(resource, list_fields)\n\n            del vars[\"iSortingCols\"]\n            for col in sort_by:\n                del vars[\"iSortCol_%s\" % str(index)]\n                del vars[\"sSortDir_%s\" % str(index)]\n\n            # Get the first row for a quick up load\n            items = resource.sqltable(fields=list_fields,\n                                      start=start,\n                                      limit=1,\n                                      orderby=orderby,\n                                      download_url=self.download_url)\n            totalrows = resource.count()\n            if items:\n                if totalrows:\n                    if s3.dataTable_iDisplayLength:\n                        limit = 2 * s3.dataTable_iDisplayLength\n                    else:\n                        limit = 50\n                # Add a test on the first call here:\n                # Now get the limit rows for ajax style update of table\n                sqltable = resource.sqltable(fields=list_fields,\n                                             start=start,\n                                             limit=limit,\n                                             orderby=orderby,\n                                             download_url=self.download_url,\n                                             as_page=True)\n                aadata = dict(aaData = sqltable or [])\n                # Ugly hack to change any occurrence of [id] with the true id\n                # Needed because the represent doesn't know the id\n                for i in range(len(aadata[\"aaData\"])):\n                    id = aadata[\"aaData\"][i][0]\n                    for j in range(len(aadata[\"aaData\"][i])):\n                        new = aadata[\"aaData\"][i][j].replace(\"[id]\",id)\n                        aadata[\"aaData\"][i][j] = new\n\n                aadata.update(iTotalRecords=totalrows,\n                              iTotalDisplayRecords=totalrows)\n                response.aadata = jsons(aadata)\n                s3.start = 0\n                s3.limit = limit\n            else: # No items in database\n                # s3import tables don't have a delete field but kept for the record\n                if \"deleted\" in table:\n                    available_records = db(table.deleted == False)\n                else:\n                    available_records = db(table.id > 0)\n                # check for any records on an unfiltered table\n                if available_records.select(table.id,\n                                            limitby=(0, 1)).first():\n                    items = self.crud_string(tablename, \"msg_no_match\")\n                else:\n                    items = self.crud_string(tablename, \"msg_list_empty\")\n\n            output.update(items=items, sortby=sort_by)\n            # Value to be added to the dataTable ajax call\n            s3.dataTable_Method = \"import\"\n\n        return output", "documentation": "Method to get the data for the table", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_18": {"code": "def _item_element_represent(self, value):\n        \"\"\"\n            Represent the element in an import item for dataTable display\n\n            @param value: the string containing the element\n        \"\"\"\n\n        T = current.T\n        db = current.db\n\n        value = S3XML.xml_decode(value)\n        try:\n            element = etree.fromstring(value)\n        except:\n            # XMLSyntaxError: return the element as-is\n            return DIV(value)\n\n        tablename = element.get(\"name\")\n        table = current.db[tablename]\n\n        output = DIV()\n        details = TABLE(_class=\"importItem [id]\")\n        header, rows = self._add_item_details(element.findall(\"data\"), table)\n        if header is not None:\n            output.append(header)\n        # Add components, if present\n        components = element.findall(\"resource\")\n        for component in components:\n            ctablename = component.get(\"name\")\n            ctable = db[ctablename]\n            self._add_item_details(component.findall(\"data\"), ctable,\n                                   details=rows, prefix=True)\n        if rows:\n            details.append(TBODY(rows))\n        # Add error messages, if present\n        errors = current.xml.collect_errors(element)\n        if errors:\n            details.append(TFOOT(TR(TH(\"%s:\" % T(\"Errors\")),\n                                   TD(UL([LI(e) for e in errors])))))\n        if rows == [] and components == []:\n            # At this stage we don't have anything to display to see if we can\n            # find something to show. This could be the case when a table being\n            # imported is a resolver for a many to many relationship\n            refdetail = TABLE(_class=\"importItem [id]\")\n            references = element.findall(\"reference\")\n            for reference in references:\n                tuid = reference.get(\"tuid\")\n                resource = reference.get(\"resource\")\n                refdetail.append(TR(TD(resource), TD(tuid)))\n            output.append(refdetail)\n        else:\n            output.append(details)\n        return str(output)", "documentation": "Represent the element in an import item for dataTable display", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_19": {"code": "def _add_item_details(data, table, details=None, prefix=False):\n        \"\"\"\n            Add details of the item element\n\n            @param data: the list of data elements in the item element\n            @param table: the table for the data\n            @param details: the existing details rows list (to append to)\n        \"\"\"\n\n        tablename = table._tablename\n        if details is None:\n            details = []\n        first = None\n        firstString = None\n        header = None\n        for child in data:\n            f = child.get(\"field\", None)\n            if f not in table.fields:\n                continue\n            elif f == \"wkt\":\n                # Skip bulky WKT fields\n                continue\n            field = table[f]\n            ftype = str(field.type)\n            value = child.get(\"value\", None)\n            if not value:\n                value = child.text\n            try:\n                value = S3Importer._decode_data(field, value)\n            except:\n                pass\n            if value:\n                value = S3XML.xml_encode(unicode(value))\n            else:\n                value = \"\"\n            if f != None and value != None:\n                headerText = P(B(\"%s: \" % f), value)\n                if not first:\n                    first = headerText\n                if ftype == \"string\" and not firstString:\n                    firstString = headerText\n                if f == \"name\":\n                    header = headerText\n                if prefix:\n                    details.append(TR(TH(\"%s.%s:\" % (tablename, f)), TD(value)))\n                else:\n                    details.append(TR(TH(\"%s:\" % f), TD(value)))\n        if not header:\n            if firstString:\n                header = firstString\n            else:\n                header = first\n        return (header, details)", "documentation": "Add details of the item element", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_20": {"code": "def _decode_data(field, value):\n        \"\"\"\n            Try to decode string data into their original type\n\n            @param field: the Field instance\n            @param value: the stringified value\n\n            @todo: replace this by ordinary decoder\n        \"\"\"\n\n        if field.type == \"string\" or \\\n            field.type == \"string\" or  \\\n            field.type == \"password\" or \\\n            field.type == \"upload\" or \\\n            field.type == \"text\":\n            return value\n        elif field.type == \"integer\" or field.type == \"id\":\n            return int(value)\n        elif field.type == \"double\" or field.type == \"decimal\":\n            return double(value)\n        elif  field.type == 'boolean':\n            if value and not str(value)[:1].upper() in [\"F\", \"0\"]:\n                return \"T\"\n            else:\n                return \"F\"\n        elif field.type == \"date\":\n            return value # @todo fix this to get a date\n        elif field.type == \"time\":\n            return value # @todo fix this to get a time\n        elif field.type == \"datetime\":\n            return value # @todo fix this to get a datetime\n        else:\n            return value", "documentation": "Try to decode the data into their original type", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_21": {"code": "def date_represent(date_obj):\n        \"\"\"\n            Represent a datetime object as string\n\n            @param date_obj: the datetime object\n\n            @todo: replace by S3DateTime method?\n        \"\"\"\n        return date_obj.strftime(\"%d %B %Y, %I:%M%p\")", "documentation": "Represent a datetime object as a string", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_22": {"code": "def _process_item_list(self, upload_id, vars):\n        \"\"\"\n            Get the list of IDs for the selected items from the \"mode\"\n            and \"selected\" request variables\n\n            @param upload_id: the upload_id\n            @param vars: the request variables\n        \"\"\"\n\n        items = None\n        if \"mode\" in vars:\n            mode = vars[\"mode\"]\n            if \"selected\" in vars:\n                selected = vars[\"selected\"].split(\",\")\n            else:\n                selected = []\n            if mode == \"Inclusive\":\n                items = selected\n            elif mode == \"Exclusive\":\n                all_items = self._get_all_items(upload_id, as_string=True)\n                items = [i for i in all_items if i not in selected]\n        return items", "documentation": "Get the list of IDs for the selected items", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_23": {"code": "def _get_all_items(self, upload_id, as_string=False):\n        \"\"\" Get a list of the record IDs of all import items for\n            the the given upload ID\n\n            @param upload_id: the upload ID\n            @param as_string: represent each ID as string\n        \"\"\"\n\n        item_table = S3ImportJob.define_item_table()\n        upload_table = self.upload_table\n\n        query = (upload_table.id == upload_id) & \\\n                (item_table.job_id == upload_table.job_id) & \\\n                (item_table.tablename == self.controller_tablename)\n\n        rows = current.db(query).select(item_table.id)\n        if as_string:\n            items = [str(row.id) for row in rows]\n        else:\n            items = [row.id for row in rows]\n\n        return items", "documentation": "Get a list of record IDs of all import items for the upload ID", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_24": {"code": "def _use_upload_table(self):\n        \"\"\"\n            Set the resource and the table to being s3_import_upload\n        \"\"\"\n\n        if self.upload_resource == None:\n            from s3resource import S3Resource\n            (prefix, name) = self.UPLOAD_TABLE_NAME.split(\"_\",1)\n            self.upload_resource = S3Resource(prefix, name)\n        self.resource = self.upload_resource\n        self.table = self.upload_table\n        self.tablename = self.upload_tablename", "documentation": "s3_import_upload set the resource and the table to being s3_import_upload", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_25": {"code": "def _use_controller_table(self):\n        \"\"\"\n            Set the resource and the table to be the imported resource\n        \"\"\"\n\n        self.resource = self.controller_resource\n        self.table = self.controller_table\n        self.tablename = self.controller_tablename", "documentation": "Use the resource and table name from the controller table .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_26": {"code": "def _use_import_item_table(self, job_id):\n        \"\"\"\n            Set the resource and the table to being s3_import_item \n        \"\"\"\n\n        if self.item_resource == None:\n            from s3resource import S3Resource\n            (prefix, name) = S3ImportJob.ITEM_TABLE_NAME.split(\"_\",1)\n            self.item_resource = S3Resource(prefix, name)\n        self.resource = self.item_resource\n        self.tablename = S3ImportJob.ITEM_TABLE_NAME\n        self.table = S3ImportJob.define_item_table()", "documentation": "s3_import_item set the resource and the table to be s3_import_item", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_27": {"code": "def __define_table(self):\n        \"\"\" Configures the upload table \"\"\"\n\n        _debug(\"S3Importer.__define_table()\")\n\n        T = current.T\n        db = current.db\n        request = current.request\n\n        self.upload_tablename = self.UPLOAD_TABLE_NAME\n\n        import_upload_status = {\n            1: T(\"Pending\"),\n            2: T(\"In error\"),\n            3: T(\"Completed\"),\n        }\n\n        def user_name_represent(id):\n            # @todo: use s3_present_user?\n\n            rep_str = \"-\"\n            table = db.auth_user\n            query = (table.id == id)\n            row = db(query).select(table.first_name,\n                                   table.last_name,\n                                   limitby=(0, 1)).first()\n            if row:\n                rep_str = \"%s %s\" % (row.first_name, row.last_name)\n            return rep_str\n\n        def status_represent(index):\n            if index == None:\n                return \"Unknown\" # @todo: use messages (internationalize)\n            else:\n                return import_upload_status[index]\n\n        now = request.utcnow\n        table = self.define_upload_table()\n        table.file.upload_folder = os.path.join(request.folder,\n                                                \"uploads\",\n                                                #\"imports\"\n                                                )\n        table.file.comment = DIV(_class=\"tooltip\",\n                                 _title=\"%s|%s\" %\n                                    (self.messages.import_file,\n                                     self.messages.import_file_comment))\n        table.file.label = self.messages.import_file\n        table.status.requires = IS_IN_SET(import_upload_status, zero=None)\n        table.status.represent = status_represent\n        table.user_id.label = self.messages.user_name\n        table.user_id.represent = user_name_represent\n        table.created_on.default = now\n        table.created_on.represent = self.date_represent\n        table.modified_on.default = now\n        table.modified_on.update = now\n        table.modified_on.represent = self.date_represent\n\n        table.replace_option.label = T(\"Replace\")\n\n        self.upload_table = db[self.UPLOAD_TABLE_NAME]", "documentation": "Defines the upload table", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_28": {"code": "def define_upload_table(cls):\n        \"\"\" Defines the upload table \"\"\"\n\n        db = current.db\n        uploadfolder = os.path.join(current.request.folder,\n                                    \"uploads\",\n                                    )\n        if cls.UPLOAD_TABLE_NAME not in db:\n            upload_table = db.define_table(cls.UPLOAD_TABLE_NAME,\n                    Field(\"controller\",\n                          readable=False,\n                          writable=False),\n                    Field(\"function\",\n                          readable=False,\n                          writable=False),\n                    Field(\"file\", \"upload\",\n                          uploadfolder=os.path.join(current.request.folder, \"uploads\", \"imports\"),\n                          autodelete=True),\n                    Field(\"filename\",\n                          readable=False,\n                          writable=False),\n                    Field(\"status\", \"integer\",\n                          default=1,\n                          readable=False,\n                          writable=False),\n                    Field(\"extra_data\",\n                          readable=False,\n                          writable=False),\n                    Field(\"replace_option\", \"boolean\",\n                          default=False,\n                          readable=False,\n                          writable=False),\n                    Field(\"job_id\",\n                          length=128,\n                          readable=False,\n                          writable=False),\n                    Field(\"user_id\", \"integer\",\n                          readable=False,\n                          writable=False),\n                    Field(\"created_on\", \"datetime\",\n                          readable=False,\n                          writable=False),\n                    Field(\"modified_on\", \"datetime\",\n                          readable=False,\n                          writable=False),\n                    Field(\"summary_added\", \"integer\",\n                          readable=False,\n                          writable=False),\n                    Field(\"summary_error\", \"integer\",\n                          readable=False,\n                          writable=False),\n                    Field(\"summary_ignored\", \"integer\",\n                          readable=False,\n                          writable=False),\n                    Field(\"completed_details\", \"text\",\n                          readable=False,\n                          writable=False))\n        else:\n            upload_table = db[cls.UPLOAD_TABLE_NAME]\n\n        return upload_table", "documentation": "Defines the upload table .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_29": {"code": "def __init__(self, job):\n        \"\"\"\n            Constructor\n\n            @param job: the import job this item belongs to\n        \"\"\"\n\n        self.job = job\n        self.ERROR = current.manager.ERROR\n\n        # Locking and error handling\n        self.lock = False\n        self.error = None\n\n        # Identification\n        import uuid\n        self.item_id = uuid.uuid4() # unique ID for this item\n        self.id = None\n        self.uid = None\n\n        # Data elements\n        self.table = None\n        self.tablename = None\n        self.element = None\n        self.data = None\n        self.original = None\n        self.components = []\n        self.references = []\n        self.load_components = []\n        self.load_references = []\n        self.parent = None\n        self.skip = False\n\n        # Conflict handling\n        self.mci = 2\n        self.mtime = datetime.utcnow()\n        self.modified = True\n        self.conflict = False\n\n        # Allowed import methods\n        self.strategy = job.strategy\n        # Update and conflict resolution policies\n        self.update_policy = job.update_policy\n        self.conflict_policy = job.conflict_policy\n\n        # Actual import method\n        self.method = None\n\n        self.onvalidation = None\n        self.onaccept = None\n\n        # Item import status flags\n        self.accepted = None\n        self.permitted = False\n        self.committed = False\n\n        # Writeback hook for circular references:\n        # Items which need a second write to update references\n        self.update = []", "documentation": "Imports the item", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_30": {"code": "def __repr__(self):\n        \"\"\" Helper method for debugging \"\"\"\n\n        _str = \"<S3ImportItem %s {item_id=%s uid=%s id=%s error=%s data=%s}>\" % \\\n               (self.table, self.item_id, self.uid, self.id, self.error, self.data)\n        return _str", "documentation": "Returns a string representation of the object", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_31": {"code": "def parse(self,\n              element,\n              original=None,\n              table=None,\n              tree=None,\n              files=None):\n        \"\"\"\n            Read data from a <resource> element\n\n            @param element: the element\n            @param table: the DB table\n            @param tree: the import tree\n            @param files: uploaded files\n\n            @returns: True if successful, False if not (sets self.error)\n        \"\"\"\n\n        db = current.db\n        xml = current.xml\n        manager = current.manager\n        validate = manager.validate\n        s3db = current.s3db\n\n        self.element = element\n        if table is None:\n            tablename = element.get(xml.ATTRIBUTE.name, None)\n            try:\n                table = s3db[tablename]\n            except:\n                self.error = self.ERROR.BAD_RESOURCE\n                element.set(xml.ATTRIBUTE.error, self.error)\n                return False\n\n        self.table = table\n        self.tablename = table._tablename\n\n        if original is None:\n            original = manager.original(table, element)\n        data = xml.record(table, element,\n                          files=files,\n                          original=original,\n                          validate=validate)\n\n        if data is None:\n            self.error = self.ERROR.VALIDATION_ERROR\n            self.accepted = False\n            if not element.get(xml.ATTRIBUTE.error, False):\n                element.set(xml.ATTRIBUTE.error, str(self.error))\n            return False\n\n        self.data = data\n\n        if original is not None:\n            self.original = original\n            self.id = original[table._id.name]\n            if xml.UID in original:\n                self.uid = original[xml.UID]\n                self.data.update({xml.UID:self.uid})\n        elif xml.UID in data:\n            self.uid = data[xml.UID]\n        if xml.MTIME in data:\n            self.mtime = data[xml.MTIME]\n        if xml.MCI in data:\n            self.mci = data[xml.MCI]\n\n        _debug(\"New item: %s\" % self)\n        return True", "documentation": "Parses the <resource > element and saves its data in the corresponding attribute of the item .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_32": {"code": "def deduplicate(self):\n\n        RESOLVER = \"deduplicate\"\n\n        if self.id:\n            return\n\n        table = self.table\n\n        if table is None:\n            return\n        if self.original is not None:\n            original = self.original\n        else:\n            original = current.manager.original(table, self.data)\n\n        if original is not None:\n            self.original = original\n            self.id = original[table._id.name]\n            UID = current.xml.UID\n            if UID in original:\n                self.uid = original[UID]\n                self.data.update({UID:self.uid})\n            self.method = self.METHOD.UPDATE\n        else:\n            resolve = current.s3db.get_config(self.tablename, RESOLVER)\n            if self.data and resolve:\n                resolve(self)\n\n        return", "documentation": "Deduplicates the current object from the table", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_33": {"code": "def authorize(self):\n        \"\"\"\n            Authorize the import of this item, sets self.permitted\n        \"\"\"\n\n        db = current.db\n        manager = current.manager\n        authorize = manager.permit\n\n        self.permitted = False\n\n        if not self.table:\n            return False\n\n        prefix = self.tablename.split(\"_\", 1)[0]\n        if prefix in manager.PROTECTED:\n            return False\n\n        if not authorize:\n            self.permitted = True\n\n        self.method = self.METHOD.CREATE\n        if self.id:\n\n            if self.data.deleted is True:\n                self.method = self.METHOD.DELETE\n                self.accepted = True\n\n            else:\n                if not self.original:\n                    query = (self.table.id == self.id)\n                    self.original = db(query).select(limitby=(0, 1)).first()\n                if self.original:\n                    self.method = self.METHOD.UPDATE\n\n        if self.method == self.METHOD.CREATE:\n            self.id = 0\n\n        if authorize:\n            self.permitted = authorize(self.method,\n                                       self.tablename,\n                                       record_id=self.id)\n\n        return self.permitted", "documentation": "Authorize the import of the current item", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_34": {"code": "def validate(self):\n        \"\"\"\n            Validate this item (=record onvalidation), sets self.accepted\n        \"\"\"\n\n        if self.accepted is not None:\n            return self.accepted\n        if self.data is None or not self.table:\n            self.accepted = False\n            return False\n\n        form = Storage()\n        form.method = self.method\n        form.vars = self.data\n        if self.id:\n            form.vars.id = self.id\n        form.errors = Storage()\n        tablename = self.tablename\n        key = \"%s_onvalidation\" % self.method\n        s3db = current.s3db\n        onvalidation = s3db.get_config(tablename, key,\n                       s3db.get_config(tablename, \"onvalidation\"))\n        if onvalidation:\n            try:\n                callback(onvalidation, form, tablename=tablename)\n            except:\n                pass # @todo need a better handler here.\n        self.accepted = True\n        if form.errors:\n            error = current.xml.ATTRIBUTE.error\n            for k in form.errors:\n                e = self.element.findall(\"data[@field='%s']\" % k)\n                if not e:\n                    e = self.element.findall(\"reference[@field='%s']\" % k)\n                if not e:\n                    e = self.element\n                    form.errors[k] = \"[%s] %s\" % (k, form.errors[k])\n                else:\n                    e = e[0]\n                e.set(error,\n                      str(form.errors[k]).decode(\"utf-8\"))\n            self.error = self.ERROR.VALIDATION_ERROR\n            self.accepted = False\n        return self.accepted", "documentation": "Method to validate the current item .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_35": {"code": "def commit(self, ignore_errors=False):\n        \"\"\"\n            Commit this item to the database\n\n            @param ignore_errors: skip invalid components\n                                  (still reports errors)\n        \"\"\"\n\n        db = current.db\n        s3db = current.s3db\n        xml = current.xml\n        manager = current.manager\n        table = self.table\n\n        # Check if already committed\n        if self.committed:\n            # already committed\n            return True\n\n        # If the parent item gets skipped, then skip this item as well\n        if self.parent is not None and self.parent.skip:\n            return True\n\n        _debug(\"Committing item %s\" % self)\n\n        # Resolve references\n        self._resolve_references()\n\n        # Validate\n        if not self.validate():\n            _debug(\"Validation error: %s (%s)\" % (self.error, xml.tostring(self.element, pretty_print=True)))\n            self.skip = True\n            return ignore_errors\n\n        elif self.components:\n            for component in self.components:\n                if not component.validate():\n                    if hasattr(component, \"tablename\"):\n                        tn = component.tablename\n                    else:\n                        tn = None\n                    _debug(\"Validation error, component=%s\" % tn)\n                    component.skip = True\n                    # Skip this item on any component validation errors\n                    # unless ignore_errors is True\n                    if ignore_errors:\n                        continue\n                    else:\n                        self.skip = True\n                        return False\n\n        # De-duplicate\n        self.deduplicate()\n\n        # Log this item\n        if manager.log is not None:\n            manager.log(self)\n\n        # Authorize item\n        if not self.authorize():\n            _debug(\"Not authorized - skip\")\n            self.error = manager.ERROR.NOT_PERMITTED\n            self.skip = True\n            return ignore_errors\n\n        _debug(\"Method: %s\" % self.method)\n\n        # Check if import method is allowed in strategy\n        if not isinstance(self.strategy, (list, tuple)):\n            self.strategy = [self.strategy]\n        if self.method not in self.strategy:\n            _debug(\"Method not in strategy - skip\")\n            self.error = manager.ERROR.NOT_PERMITTED\n            self.skip = True\n            return True\n\n        this = self.original\n        if not this and self.id and \\\n           self.method in (self.METHOD.UPDATE, self.METHOD.DELETE):\n            query = (table.id == self.id)\n            this = db(query).select(limitby=(0, 1)).first()\n        this_mtime = None\n        this_mci = 0\n        if this:\n            if xml.MTIME in table.fields:\n                this_mtime = xml.as_utc(this[xml.MTIME])\n            if xml.MCI in table.fields:\n                this_mci = this[xml.MCI]\n        self.mtime = xml.as_utc(self.mtime)\n\n        # Conflict detection\n        this_modified = True\n        self.modified = True\n        self.conflict = False\n        last_sync = xml.as_utc(self.job.last_sync)\n        if last_sync:\n            if this_mtime and this_mtime < last_sync:\n                this_modified = False\n            if self.mtime and self.mtime < last_sync:\n                self.modified = False\n            if self.modified and this_modified:\n                self.conflict = True\n\n        if self.conflict and \\\n           self.method in (self.METHOD.UPDATE, self.METHOD.DELETE):\n            _debug(\"Conflict: %s\" % self)\n            if self.job.onconflict:\n                self.job.onconflict(self)\n\n        if self.data is not None:\n            data = Storage(self.data)\n        else:\n            data = Storage()\n\n        # Update existing record\n        if self.method == self.METHOD.UPDATE:\n\n            if this:\n                if \"deleted\" in this and this.deleted:\n                    policy = self._get_update_policy(None)\n                    if policy == self.POLICY.NEWER and \\\n                       this_mtime and this_mtime > self.mtime or \\\n                       policy == self.POLICY.MASTER and \\\n                       (this_mci == 0 or self.mci != 1):\n                        self.skip = True\n                        return True\n                fields = data.keys()\n                for f in fields:\n                    if f not in this:\n                        continue\n                    if isinstance(this[f], datetime):\n                        if xml.as_utc(data[f]) == xml.as_utc(this[f]):\n                            del data[f]\n                            continue\n                    else:\n                        if data[f] == this[f]:\n                            del data[f]\n                            continue\n                    remove = False\n                    policy = self._get_update_policy(f)\n                    if policy == self.POLICY.THIS:\n                        remove = True\n                    elif policy == self.POLICY.NEWER:\n                        if this_mtime and this_mtime > self.mtime:\n                            remove = True\n                    elif policy == self.POLICY.MASTER:\n                        if this_mci == 0 or self.mci != 1:\n                            remove = True\n                    if remove:\n                        del data[f]\n                        self.data.update({f:this[f]})\n                if \"deleted\" in this and this.deleted:\n                    # Undelete re-imported records:\n                    data.update(deleted=False)\n                    if \"deleted_fk\" in table:\n                        data.update(deleted_fk=\"\")\n                    if \"created_by\" in table:\n                        data.update(created_by=table.created_by.default)\n                    if \"modified_by\" in table:\n                        data.update(modified_by=table.modified_by.default)\n\n            if not self.skip and not self.conflict and \\\n               (len(data) or self.components or self.references):\n                if self.uid and xml.UID in table:\n                    data.update({xml.UID:self.uid})\n                if xml.MTIME in table:\n                    data.update({xml.MTIME: self.mtime})\n                if xml.MCI in data:\n                    # retain local MCI on updates\n                    del data[xml.MCI]\n                query = (table._id == self.id)\n                try:\n                    success = db(query).update(**dict(data))\n                except:\n                    self.error = sys.exc_info()[1]\n                    self.skip = True\n                    return False\n                if success:\n                    self.committed = True\n            else:\n                # Nothing to update\n                self.committed = True\n\n        # Create new record\n        elif self.method == self.METHOD.CREATE:\n\n            # Do not apply field policy to UID and MCI\n            UID = xml.UID\n            if UID in data:\n                del data[UID]\n            MCI = xml.MCI\n            if MCI in data:\n                del data[MCI]\n\n            for f in data:\n                policy = self._get_update_policy(f)\n                if policy == self.POLICY.MASTER and self.mci != 1:\n                    del data[f]\n\n            if len(data) or self.components or self.references:\n\n                # Restore UID and MCI\n                if self.uid and UID in table.fields:\n                    data.update({UID:self.uid})\n                if MCI in table.fields:\n                    data.update({MCI:self.mci})\n\n                # Insert the new record\n                try:\n                    success = table.insert(**dict(data))\n                except:\n                    self.error = sys.exc_info()[1]\n                    self.skip = True\n                    return False\n                if success:\n                    self.id = success\n                    self.committed = True\n\n            else:\n                # Nothing to create\n                self.skip = True\n                return True\n\n        # Delete local record\n        elif self.method == self.METHOD.DELETE:\n\n            if this:\n                if this.deleted:\n                    self.skip = True\n                policy = self._get_update_policy(None)\n                if policy == self.POLICY.THIS:\n                    self.skip = True\n                elif policy == self.POLICY.NEWER and \\\n                     (this_mtime and this_mtime > self.mtime):\n                    self.skip = True\n                elif policy == self.POLICY.MASTER and \\\n                     (this_mci == 0 or self.mci != 1):\n                    self.skip = True\n            else:\n                self.skip = True\n\n            if not self.skip and not self.conflict:\n\n                prefix, name = self.tablename.split(\"_\", 1)\n                resource = manager.define_resource(prefix, name, id=self.id)\n\n                ondelete = s3db.get_config(self.tablename, \"ondelete\")\n                success = resource.delete(ondelete=ondelete,\n                                          cascade=True)\n                if resource.error:\n                    self.error = resource.error\n                    self.skip = True\n                    return ignore_errors\n\n            _debug(\"Success: %s, id=%s %sd\" % (self.tablename, self.id,\n                                               self.skip and \"skippe\" or \\\n                                               self.method))\n            return True\n\n        # Audit + onaccept on successful commits\n        if self.committed:\n            form = Storage()\n            form.method = self.method\n            form.vars = self.data\n            tablename = self.tablename\n            prefix, name = tablename.split(\"_\", 1)\n            if self.id:\n                form.vars.id = self.id\n            if manager.audit is not None:\n                manager.audit(self.method, prefix, name,\n                              form=form,\n                              record=self.id,\n                              representation=\"xml\")\n            s3db.update_super(table, form.vars)\n            if self.method == self.METHOD.CREATE:\n                current.auth.s3_set_record_owner(table, self.id)\n            key = \"%s_onaccept\" % self.method\n            onaccept = s3db.get_config(tablename, key,\n                       s3db.get_config(tablename, \"onaccept\"))\n            if onaccept:\n                callback(onaccept, form, tablename=self.tablename)\n\n        # Update referencing items\n        if self.update and self.id:\n            for u in self.update:\n                item = u.get(\"item\", None)\n                if not item:\n                    continue\n                field = u.get(\"field\", None)\n                if isinstance(field, (list, tuple)):\n                    pkey, fkey = field\n                    query = table.id == self.id\n                    row = db(query).select(table[pkey],\n                                           limitby=(0, 1)).first()\n                    if row:\n                        item._update_reference(fkey, row[pkey])\n                else:\n                    item._update_reference(field, self.id)\n\n        _debug(\"Success: %s, id=%s %sd\" % (self.tablename, self.id,\n                                           self.skip and \"skippe\" or \\\n                                           self.method))\n        return True", "documentation": "Commit this item to the S3 database", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_36": {"code": "def _get_update_policy(self, field):\n        \"\"\"\n            Get the update policy for a field (if the item will\n            update an existing record)\n\n            @param field: the name of the field\n        \"\"\"\n\n        if isinstance(self.update_policy, dict):\n            r = self.update_policy.get(field,\n                self.update_policy.get(\"__default__\", self.POLICY.THIS))\n        else:\n            r = self.update_policy\n        if not r in self.POLICY.values():\n            r = self.POLICY.THIS\n        return r", "documentation": "Get the update policy for a field", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_37": {"code": "def _resolve_references(self):\n        \"\"\"\n            Resolve the references of this item (=look up all foreign\n            keys from other items of the same job). If a foreign key\n            is not yet available, it will be scheduled for later update.\n        \"\"\"\n\n        if not self.table:\n            return\n\n        items = self.job.items\n        for reference in self.references:\n\n            item = None\n            field = reference.field\n            entry = reference.entry\n            if not entry:\n                continue\n\n            # Resolve key tuples\n            if isinstance(field, (list,tuple)):\n                pkey, fkey = field\n            else:\n                pkey, fkey = (\"id\", field)\n\n            # Resolve the key table name\n            ktablename, key, multiple = s3_get_foreign_key(self.table[fkey])\n            if not ktablename:\n                if self.tablename == \"auth_user\" and \\\n                   fkey == \"organisation_id\":\n                    ktablename = \"org_organisation\"\n                else:\n                    continue\n            if entry.tablename:\n                ktablename = entry.tablename\n            try:\n                ktable = current.s3db[ktablename]\n            except:\n                continue\n\n            # Resolve the foreign key (value)\n            fk = entry.id\n            if entry.item_id:\n                item = items[entry.item_id]\n                if item:\n                    fk = item.id\n            if fk and pkey != \"id\":\n                row = current.db(ktable._id == fk).select(ktable[pkey],\n                                                          limitby=(0, 1)).first()\n                if not row:\n                    fk = None\n                    continue\n                else:\n                    fk = row[pkey]\n\n            # Update record data\n            if fk:\n                if multiple:\n                    val = self.data.get(fkey, [])\n                    if fk not in val:\n                        val.append(fk)\n                    self.data[fkey] = val\n                else:\n                    self.data[fkey] = fk\n            else:\n                if fkey in self.data and not multiple:\n                    del self.data[fkey]\n                if item:\n                    item.update.append(dict(item=self, field=fkey))", "documentation": "Resolve the references of this item", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_38": {"code": "def _update_reference(self, field, value):\n        \"\"\"\n            Helper method to update a foreign key in an already written\n            record. Will be called by the referenced item after (and only\n            if) it has been committed. This is only needed if the reference\n            could not be resolved before commit due to circular references.\n\n            @param field: the field name of the foreign key\n            @param value: the value of the foreign key\n        \"\"\"\n\n        if not value or not self.table:\n            return\n        db = current.db\n        if self.id and self.permitted:\n            fieldtype = str(self.table[field].type)\n            if fieldtype.startswith(\"list:reference\"):\n                query = (self.table.id == self.id)\n                record = db(query).select(self.table[field],\n                                          limitby=(0,1)).first()\n                if record:\n                    values = record[field]\n                    if value not in values:\n                        values.append(value)\n                        db(self.table.id == self.id).update(**{field:values})\n            else:\n                db(self.table.id == self.id).update(**{field:value})", "documentation": "Updates the value of a reference field in the already written record", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_39": {"code": "def store(self, item_table=None):\n        \"\"\"\n            Store this item in the DB\n        \"\"\"\n\n        _debug(\"Storing item %s\" % self)\n        if item_table is None:\n            return None\n        db = current.db\n        query = item_table.item_id == self.item_id\n        row = db(query).select(item_table.id, limitby=(0, 1)).first()\n        if row:\n            record_id = row.id\n        else:\n            record_id = None\n        record = Storage(job_id = self.job.job_id,\n                         item_id = self.item_id,\n                         tablename = self.tablename,\n                         record_uid = self.uid,\n                         error = self.error)\n        if self.element is not None:\n            element_str = current.xml.tostring(self.element,\n                                               xml_declaration=False)\n            record.update(element=element_str)\n        if self.data is not None:\n            data = Storage()\n            for f in self.data.keys():\n                table = self.table\n                if f not in table.fields:\n                    continue\n                fieldtype = str(self.table[f].type)\n                if fieldtype == \"id\" or s3_has_foreign_key(self.table[f]):\n                    continue\n                data.update({f:self.data[f]})\n            data_str = cPickle.dumps(data)\n            record.update(data=data_str)\n        ritems = []\n        for reference in self.references:\n            field = reference.field\n            entry = reference.entry\n            store_entry = None\n            if entry:\n                if entry.item_id is not None:\n                    store_entry = dict(field=field,\n                                       item_id=str(entry.item_id))\n                elif entry.uid is not None:\n                    store_entry = dict(field=field,\n                                       tablename=entry.tablename,\n                                       uid=str(entry.uid))\n                if store_entry is not None:\n                    ritems.append(json.dumps(store_entry))\n        if ritems:\n            record.update(ritems=ritems)\n        citems = [c.item_id for c in self.components]\n        if citems:\n            record.update(citems=citems)\n        if self.parent:\n            record.update(parent=self.parent.item_id)\n        if record_id:\n            db(item_table.id == record_id).update(**record)\n        else:\n            record_id = item_table.insert(**record)\n        _debug(\"Record ID=%s\" % record_id)\n        return record_id", "documentation": "Store this item in the DB", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_40": {"code": "def restore(self, row):\n        \"\"\"\n            Restore an item from a item table row. This does not restore\n            the references (since this can not be done before all items\n            are restored), must call job.restore_references() to do that\n\n            @param row: the item table row\n        \"\"\"\n\n        xml = current.xml\n\n        self.item_id = row.item_id\n        self.accepted = None\n        self.permitted = False\n        self.committed = False\n        tablename = row.tablename\n        self.id = None\n        self.uid = row.record_uid\n        if row.data is not None:\n            self.data = cPickle.loads(row.data)\n        else:\n            self.data = Storage()\n        data = self.data\n        if xml.MTIME in data:\n            self.mtime = data[xml.MTIME]\n        if xml.MCI in data:\n            self.mci = data[xml.MCI]\n        UID = xml.UID\n        if UID in data:\n            self.uid = data[UID]\n        self.element = etree.fromstring(row.element)\n        if row.citems:\n            self.load_components = row.citems\n        if row.ritems:\n            self.load_references = [json.loads(ritem) for ritem in row.ritems]\n        self.load_parent = row.parent\n        try:\n            table = current.s3db[tablename]\n        except:\n            self.error = self.ERROR.BAD_RESOURCE\n            return False\n        else:\n            self.table = table\n            self.tablename = tablename\n        original = current.manager.original(table, self.data)\n        if original is not None:\n            self.original = original\n            self.id = original[table._id.name]\n            if UID in original:\n                self.uid = original[UID]\n                self.data.update({UID:self.uid})\n        self.error = row.error\n        if self.error and not self.data:\n            # Validation error\n            return False\n        return True", "documentation": "Restore an item from an item table row .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_41": {"code": "def __init__(self, manager, table,\n                 tree=None,\n                 files=None,\n                 job_id=None,\n                 strategy=None,\n                 update_policy=None,\n                 conflict_policy=None,\n                 last_sync=None,\n                 onconflict=None):\n        \"\"\"\n            Constructor\n\n            @param manager: the S3RequestManager instance performing this job\n            @param tree: the element tree to import\n            @param files: files attached to the import (for upload fields)\n            @param job_id: restore job from database (record ID or job_id)\n            @param strategy: the import strategy\n            @param update_policy: the update policy\n            @param conflict_policy: the conflict resolution policy\n            @param last_sync: the last synchronization time stamp (datetime)\n            @param onconflict: custom conflict resolver function\n        \"\"\"\n\n        self.error = None # the last error\n        self.error_tree = etree.Element(current.xml.TAG.root)\n\n        self.table = table\n        self.tree = tree\n        self.files = files\n        self.directory = Storage()\n\n        self.elements = Storage()\n        self.items = Storage()\n        self.references = []\n\n        self.job_table = None\n        self.item_table = None\n\n        self.count = 0 # total number of records imported\n        self.created = [] # IDs of created records\n        self.updated = [] # IDs of updated records\n        self.deleted = [] # IDs of deleted records\n\n        # Import strategy\n        self.strategy = strategy\n        if self.strategy is None:\n            self.strategy = [S3ImportItem.METHOD.CREATE,\n                             S3ImportItem.METHOD.UPDATE,\n                             S3ImportItem.METHOD.DELETE]\n        if not isinstance(self.strategy, (tuple, list)):\n            self.strategy = [self.strategy]\n\n        # Update policy (default=always update)\n        self.update_policy = update_policy\n        if not self.update_policy:\n            self.update_policy = S3ImportItem.POLICY.OTHER\n        # Conflict resolution policy (default=always update)\n        self.conflict_policy = conflict_policy\n        if not self.conflict_policy:\n            self.conflict_policy = S3ImportItem.POLICY.OTHER\n\n        # Synchronization settings\n        self.mtime = None\n        self.last_sync = last_sync\n        self.onconflict = onconflict\n\n        if job_id:\n            self.__define_tables()\n            jobtable = self.job_table\n            if str(job_id).isdigit():\n                query = jobtable.id == job_id\n            else:\n                query = jobtable.job_id == job_id\n            row = current.db(query).select(limitby=(0, 1)).first()\n            if not row:\n                raise SyntaxError(\"Job record not found\")\n            self.job_id = row.job_id\n            if not self.table:\n                tablename = row.tablename\n                try:\n                    table = current.s3db[tablename]\n                except:\n                    pass\n        else:\n            import uuid\n            self.job_id = uuid.uuid4() # unique ID for this job", "documentation": "Imports a record from Amazon S3 using the given table and tree . If a job_", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_42": {"code": "def add_item(self,\n                 element=None,\n                 original=None,\n                 components=None,\n                 parent=None,\n                 joinby=None):\n        \"\"\"\n            Parse and validate an XML element and add it as new item\n            to the job.\n\n            @param element: the element\n            @param original: the original DB record (if already available,\n                             will otherwise be looked-up by this function)\n            @param components: a dictionary of components (as in S3Resource)\n                               to include in the job (defaults to all\n                               defined components)\n            @param parent: the parent item (if this is a component)\n            @param joinby: the component join key(s) (if this is a component)\n\n            @returns: a unique identifier for the new item, or None if there\n                      was an error. self.error contains the last error, and\n                      self.error_tree an element tree with all failing elements\n                      including error attributes.\n        \"\"\"\n\n        if element in self.elements:\n            # element has already been added to this job\n            return self.elements[element]\n\n        # Parse the main element\n        item = S3ImportItem(self)\n\n        # Update lookup lists\n        item_id = item.item_id\n        self.items[item_id] = item\n        if element is not None:\n            self.elements[element] = item_id\n\n        if not item.parse(element,\n                          original=original,\n                          files=self.files):\n            self.error = item.error\n            item.accepted = False\n            if parent is None:\n                self.error_tree.append(deepcopy(item.element))\n\n        else:\n            # Now parse the components\n            table = item.table\n            components = current.s3db.get_components(table, names=components)\n\n            cnames = Storage()\n            cinfos = Storage()\n            for alias in components:\n                component = components[alias]\n                pkey = component.pkey\n                if component.linktable:\n                    ctable = component.linktable\n                    fkey = component.lkey\n                else:\n                    ctable = component.table\n                    fkey = component.fkey\n                ctablename = ctable._tablename\n                if ctablename in cnames:\n                    cnames[ctablename].append(alias)\n                else:\n                    cnames[ctablename] = [alias]\n                cinfos[(ctablename, alias)] = Storage(component = component,\n                                                      ctable = ctable,\n                                                      pkey = pkey,\n                                                      fkey = fkey,\n                                                      original = None,\n                                                      uid = None)\n            add_item = self.add_item\n            xml = current.xml\n            for celement in xml.components(element, names=cnames.keys()):\n\n                # Get the component tablename\n                ctablename = celement.get(xml.ATTRIBUTE.name, None)\n                if not ctablename:\n                    continue\n\n                # Get the component alias (for disambiguation)\n                calias = celement.get(xml.ATTRIBUTE.alias, None)\n                if calias is None:\n                    if ctablename not in cnames:\n                        continue\n                    aliases = cnames[ctablename]\n                    if len(aliases) == 1:\n                        calias = aliases[0]\n                    else:\n                        # ambiguous components *must* use alias\n                        continue\n                if (ctablename, calias) not in cinfos:\n                    continue\n                else:\n                    cinfo = cinfos[(ctablename, calias)]\n\n                component = cinfo.component\n                original = cinfo.original\n                ctable = cinfo.ctable\n                pkey = cinfo.pkey\n                fkey = cinfo.fkey\n                if not component.multiple:\n                    if cinfo.uid is not None:\n                        continue\n                    if original is None and item.id:\n                        query = (table.id == item.id) & \\\n                                (table[pkey] == ctable[fkey])\n                        original = current.db(query).select(ctable.ALL,\n                                                            limitby=(0, 1)).first()\n                    if original:\n                        cinfo.uid = uid = original.get(xml.UID, None)\n                        celement.set(xml.UID, uid)\n                    cinfo.original = original\n\n                item_id = add_item(element=celement,\n                                   original=original,\n                                   parent=item,\n                                   joinby=(pkey, fkey))\n                if item_id is None:\n                    item.error = self.error\n                    self.error_tree.append(deepcopy(item.element))\n                else:\n                    citem = self.items[item_id]\n                    citem.parent = item\n                    item.components.append(citem)\n\n            # Handle references\n            table = item.table\n            tree = self.tree\n            if tree is not None:\n                fields = [table[f] for f in table.fields]\n                rfields = filter(s3_has_foreign_key, fields)\n                item.references = self.lookahead(element,\n                                                 table=table,\n                                                 fields=rfields,\n                                                 tree=tree,\n                                                 directory=self.directory)\n                for reference in item.references:\n                    entry = reference.entry\n                    if entry and entry.element is not None:\n                        item_id = add_item(element=entry.element)\n                        if item_id:\n                            entry.update(item_id=item_id)\n\n            # Parent reference\n            if parent is not None:\n                entry = Storage(item_id=parent.item_id,\n                                element=parent.element,\n                                tablename=parent.tablename)\n                item.references.append(Storage(field=joinby,\n                                               entry=entry))\n\n        return item.item_id", "documentation": "Parse an XML element add it as a new item in the job and return the item_id", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_43": {"code": "def lookahead(self,\n                  element,\n                  table=None,\n                  fields=None,\n                  tree=None,\n                  directory=None):\n        \"\"\"\n            Find referenced elements in the tree\n\n            @param element: the element\n            @param table: the DB table\n            @param fields: the FK fields in the table\n            @param tree: the import tree\n            @param directory: a dictionary to lookup elements in the tree\n                              (will be filled in by this function)\n        \"\"\"\n\n        db = current.db\n        s3db = current.s3db\n        xml = current.xml\n        import_uid = xml.import_uid\n        ATTRIBUTE = xml.ATTRIBUTE\n        TAG = xml.TAG\n        UID = xml.UID\n        reference_list = []\n\n        root = None\n        if tree is not None:\n            if isinstance(tree, etree._Element):\n                root = tree\n            else:\n                root = tree.getroot()\n        references = element.findall(\"reference\")\n        for reference in references:\n            field = reference.get(ATTRIBUTE.field, None)\n            # Ignore references without valid field-attribute\n            if not field or field not in fields:\n                continue\n            # Find the key table\n            multiple = False\n            fieldtype = str(table[field].type)\n            if fieldtype.startswith(\"reference\"):\n                ktablename = fieldtype[10:]\n            elif fieldtype.startswith(\"list:reference\"):\n                ktablename = fieldtype[15:]\n                multiple = True\n            else:\n                # ignore if the field is not a reference type\n                continue\n            try:\n                ktable = s3db[ktablename]\n            except:\n                # Invalid tablename - skip\n                continue\n            tablename = reference.get(ATTRIBUTE.resource, None)\n            # Ignore references to tables without UID field:\n            if UID not in ktable.fields:\n                continue\n            # Fall back to key table name if tablename is not specified:\n            if not tablename:\n                tablename = ktablename\n            # Super-entity references must use the super-key:\n            if tablename != ktablename:\n                field = (ktable._id.name, field)\n            # Ignore direct references to super-entities:\n            if tablename == ktablename and ktable._id.name != \"id\":\n                continue\n            # Get the foreign key\n            uids = reference.get(UID, None)\n            attr = UID\n            if not uids:\n                uids = reference.get(ATTRIBUTE.tuid, None)\n                attr = ATTRIBUTE.tuid\n            if uids and multiple:\n                uids = json.loads(uids)\n            elif uids:\n                uids = [uids]\n\n            # Find the elements and map to DB records\n            relements = []\n\n            # Create a UID<->ID map\n            id_map = Storage()\n            if attr == UID and uids:\n                _uids = map(import_uid, uids)\n                query = ktable[UID].belongs(_uids)\n                records = db(query).select(ktable.id,\n                                           ktable[UID])\n                id_map = dict([(r[UID], r.id) for r in records])\n\n            if not uids:\n                # Anonymous reference: <resource> inside the element\n                expr = './/%s[@%s=\"%s\"]' % (TAG.resource,\n                                            ATTRIBUTE.name,\n                                            tablename)\n                relements = reference.xpath(expr)\n                if relements and not multiple:\n                    relements = [relements[0]]\n\n            elif root is not None:\n\n                for uid in uids:\n\n                    entry = None\n                    # Entry already in directory?\n                    if directory is not None:\n                        entry = directory.get((tablename, attr, uid), None)\n                    if not entry:\n                        expr = \".//%s[@%s='%s' and @%s='%s']\" % (\n                                    TAG.resource,\n                                    ATTRIBUTE.name,\n                                    tablename,\n                                    attr,\n                                    uid)\n                        e = root.xpath(expr)\n                        if e:\n                            # Element in the source => append to relements\n                            relements.append(e[0])\n                        else:\n                            # No element found, see if original record exists\n                            _uid = import_uid(uid)\n                            if _uid and _uid in id_map:\n                                _id = id_map[_uid]\n                                entry = Storage(tablename=tablename,\n                                                element=None,\n                                                uid=uid,\n                                                id=_id,\n                                                item_id=None)\n                                reference_list.append(Storage(field=field,\n                                                              entry=entry))\n                            else:\n                                continue\n                    else:\n                        reference_list.append(Storage(field=field,\n                                                      entry=entry))\n\n            # Create entries for all newly found elements\n            for relement in relements:\n                uid = relement.get(attr, None)\n                if attr == UID:\n                    _uid = import_uid(uid)\n                    id = _uid and id_map and id_map.get(_uid, None) or None\n                else:\n                    _uid = None\n                    id = None\n                entry = Storage(tablename=tablename,\n                                element=relement,\n                                uid=uid,\n                                id=id,\n                                item_id=None)\n                # Add entry to directory\n                if uid and directory is not None:\n                    directory[(tablename, attr, uid)] = entry\n                # Append the entry to the reference list\n                reference_list.append(Storage(field=field, entry=entry))\n\n        return reference_list", "documentation": "Find referenced elements in the tree .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_44": {"code": "def load_item(self, row):\n        \"\"\"\n            Load an item from the item table (counterpart to add_item\n            when restoring a job from the database)\n        \"\"\"\n\n        item = S3ImportItem(self)\n        if not item.restore(row):\n            self.error = item.error\n            if item.load_parent is None:\n                self.error_tree.append(deepcopy(item.element))\n        # Update lookup lists\n        item_id = item.item_id\n        self.items[item_id] = item\n        return item_id", "documentation": "Load an item from the item table", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_45": {"code": "def resolve(self, item_id, import_list):\n        \"\"\"\n            Resolve the reference list of an item\n\n            @param item_id: the import item UID\n            @param import_list: the ordered list of items (UIDs) to import\n        \"\"\"\n\n        item = self.items[item_id]\n        if item.lock or item.accepted is False:\n            return False\n        references = []\n        for reference in item.references:\n            ritem_id = reference.entry.item_id\n            if ritem_id and ritem_id not in import_list:\n                references.append(ritem_id)\n        for ritem_id in references:\n            item.lock = True\n            if self.resolve(ritem_id, import_list):\n                import_list.append(ritem_id)\n            item.lock = False\n        return True", "documentation": "Resolve the reference list of an import item", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_46": {"code": "def commit(self, ignore_errors=False):\n        \"\"\"\n            Commit the import job to the DB\n\n            @param ignore_errors: skip any items with errors\n                                  (does still report the errors)\n        \"\"\"\n\n        ATTRIBUTE = current.xml.ATTRIBUTE\n\n        # Resolve references\n        import_list = []\n        for item_id in self.items:\n            self.resolve(item_id, import_list)\n            if item_id not in import_list:\n                import_list.append(item_id)\n        # Commit the items\n        items = self.items\n        count = 0\n        mtime = None\n        created = []\n        cappend = created.append\n        updated = []\n        deleted = []\n        tablename = self.table._tablename\n        for item_id in import_list:\n            item = items[item_id]\n            error = None\n            success = item.commit(ignore_errors=ignore_errors)\n            error = item.error\n            if error:\n                self.error = error\n                element = item.element\n                if element is not None:\n                    if not element.get(ATTRIBUTE.error, False):\n                        element.set(ATTRIBUTE.error, str(self.error))\n                    self.error_tree.append(deepcopy(element))\n                if not ignore_errors:\n                    return False\n            elif item.tablename == tablename:\n                count += 1\n                if mtime is None or item.mtime > mtime:\n                    mtime = item.mtime\n                if item.id:\n                    if item.method == item.METHOD.CREATE:\n                        cappend(item.id)\n                    elif item.method == item.METHOD.UPDATE:\n                        updated.append(item.id)\n                    elif item.method == item.METHOD.DELETE:\n                        deleted.append(item.id)\n        self.count = count\n        self.mtime = mtime\n        self.created = created\n        self.updated = updated\n        self.deleted = deleted\n        return True", "documentation": "Commit the import job to the DB", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_47": {"code": "def __define_tables(self):\n        \"\"\"\n            Define the database tables for jobs and items\n        \"\"\"\n\n        self.job_table = self.define_job_table()\n        self.item_table = self.define_item_table()", "documentation": "Define the tables for the items and jobs", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_48": {"code": "def define_job_table(cls):\n\n        db = current.db\n        if cls.JOB_TABLE_NAME not in db:\n            job_table = db.define_table(cls.JOB_TABLE_NAME,\n                                        Field(\"job_id\", length=128,\n                                              unique=True,\n                                              notnull=True),\n                                        Field(\"tablename\"),\n                                        Field(\"timestmp\", \"datetime\",\n                                              default=datetime.utcnow()))\n        else:\n            job_table = db[cls.JOB_TABLE_NAME]\n        return job_table", "documentation": "Defines and returns the job table in the database", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_49": {"code": "def define_item_table(cls):\n\n        db = current.db\n        if cls.ITEM_TABLE_NAME not in db:\n            item_table = db.define_table(cls.ITEM_TABLE_NAME,\n                                        Field(\"item_id\", length=128,\n                                              unique=True,\n                                              notnull=True),\n                                        Field(\"job_id\", length=128),\n                                        Field(\"tablename\", length=128),\n                                        #Field(\"record_id\", \"integer\"),\n                                        Field(\"record_uid\"),\n                                        Field(\"error\", \"text\"),\n                                        Field(\"data\", \"text\"),\n                                        Field(\"element\", \"text\"),\n                                        Field(\"ritems\", \"list:string\"),\n                                        Field(\"citems\", \"list:string\"),\n                                        Field(\"parent\", length=128))\n        else:\n            item_table = db[cls.ITEM_TABLE_NAME]\n        return item_table", "documentation": "Defines and returns the item table if it does not exist yet .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_50": {"code": "def store(self):\n        \"\"\"\n            Store this job and all its items in the job table\n        \"\"\"\n\n        db = current.db\n\n        _debug(\"Storing Job ID=%s\" % self.job_id)\n        self.__define_tables()\n        jobtable = self.job_table\n        query = jobtable.job_id == self.job_id\n        row = db(query).select(jobtable.id, limitby=(0, 1)).first()\n        if row:\n            record_id = row.id\n        else:\n            record_id = None\n        record = Storage(job_id=self.job_id)\n        try:\n            tablename = self.table._tablename\n        except:\n            pass\n        else:\n            record.update(tablename=tablename)\n        for item in self.items.values():\n            item.store(item_table=self.item_table)\n        if record_id:\n            db(jobtable.id == record_id).update(**record)\n        else:\n            record_id = jobtable.insert(**record)\n        _debug(\"Job record ID=%s\" % record_id)\n        return record_id", "documentation": "Store this job and all its items in the job table", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_51": {"code": "def get_tree(self):\n        \"\"\"\n            Reconstruct the element tree of this job\n        \"\"\"\n\n        if self.tree is not None:\n            return tree\n        else:\n            xml = current.xml\n            root = etree.Element(xml.TAG.root)\n            for item in self.items.values():\n                if item.element is not None and not item.parent:\n                    if item.tablename == self.table._tablename or \\\n                       item.element.get(xml.UID, None) or \\\n                       item.element.get(xml.ATTRIBUTE.tuid, None):\n                        root.append(deepcopy(item.element))\n            return etree.ElementTree(root)", "documentation": "Reconstruct the element tree of this job .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_52": {"code": "def delete(self):\n        \"\"\"\n            Delete this job and all its items from the job table\n        \"\"\"\n\n        db = current.db\n\n        _debug(\"Deleting job ID=%s\" % self.job_id)\n        self.__define_tables()\n        item_table = self.item_table\n        query = item_table.job_id == self.job_id\n        db(query).delete()\n        job_table = self.job_table\n        query = job_table.job_id == self.job_id\n        db(query).delete()", "documentation": "Delete this job and all its items from the job table", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ashwyn/eden-message_parser_modules/s3/s3import.py_53": {"code": "def restore_references(self):\n        \"\"\"\n            Restore the job's reference structure after loading items\n            from the item table\n        \"\"\"\n\n        db = current.db\n        UID = current.xml.UID\n\n        for item in self.items.values():\n            for citem_id in item.load_components:\n                if citem_id in self.items:\n                    item.components.append(self.items[citem_id])\n            item.load_components = []\n            for ritem in item.load_references:\n                field = ritem[\"field\"]\n                if \"item_id\" in ritem:\n                    item_id = ritem[\"item_id\"]\n                    if item_id in self.items:\n                        _item = self.items[item_id]\n                        entry = Storage(tablename=_item.tablename,\n                                        element=_item.element,\n                                        uid=_item.uid,\n                                        id=_item.id,\n                                        item_id=item_id)\n                        item.references.append(Storage(field=field,\n                                                       entry=entry))\n                else:\n                    _id = None\n                    uid = ritem.get(\"uid\", None)\n                    tablename = ritem.get(\"tablename\", None)\n                    if tablename and uid:\n                        try:\n                            table = current.s3db[tablename]\n                        except:\n                            continue\n                        if UID not in table.fields:\n                            continue\n                        query = table[UID] == uid\n                        row = db(query).select(table._id,\n                                               limitby=(0, 1)).first()\n                        if row:\n                            _id = row[table._id.name]\n                        else:\n                            continue\n                        entry = Storage(tablename = ritem[\"tablename\"],\n                                        element=None,\n                                        uid = ritem[\"uid\"],\n                                        id = _id,\n                                        item_id = None)\n                        item.references.append(Storage(field=field,\n                                                       entry=entry))\n            item.load_references = []\n            if item.load_parent is not None:\n                item.parent = self.items[item.load_parent]\n                item.load_parent = None", "documentation": "Restore the job s reference structure after loading items from the item table", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "VulcanTechnologies/oauth2lib_oauth2lib/utils.py_0": {"code": "def random_ascii_string(length):\n    random = SystemRandom()\n    return ''.join([random.choice(UNICODE_ASCII_CHARACTERS) for x in range(length)])", "documentation": "Generate a random string of ASCII characters .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "VulcanTechnologies/oauth2lib_oauth2lib/utils.py_1": {"code": "def url_dequery(url):\n    \"\"\"Return a URL with the query component removed.\n\n    :param url: URL to dequery.\n    :type url: str\n    :rtype: str\n    \"\"\"\n    url = urlparse(url)\n    return urlunparse((url.scheme,\n                                url.netloc,\n                                url.path,\n                                url.params,\n                                '',\n                                url.fragment))", "documentation": "Return a URL with the query component removed .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_0": {"code": "def __init__(self, air):\n        DistributedCCharBaseAI.DistributedCCharBaseAI.__init__(self, air, TTLocalizer.Goofy)\n        self.fsm = ClassicFSM.ClassicFSM('DistributedGoofySpeedwayAI', [State.State('Off', self.enterOff, self.exitOff, ['Lonely', 'TransitionToCostume', 'Walk']),\n         State.State('Lonely', self.enterLonely, self.exitLonely, ['Chatty', 'Walk', 'TransitionToCostume']),\n         State.State('Chatty', self.enterChatty, self.exitChatty, ['Lonely', 'Walk', 'TransitionToCostume']),\n         State.State('Walk', self.enterWalk, self.exitWalk, ['Lonely', 'Chatty', 'TransitionToCostume']),\n         State.State('TransitionToCostume', self.enterTransitionToCostume, self.exitTransitionToCostume, ['Off'])], 'Off', 'Off')\n        self.fsm.enterInitialState()\n        self.handleHolidays()", "documentation": "Initialize the AI .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_1": {"code": "def generate(self):\n        DistributedCCharBaseAI.DistributedCCharBaseAI.generate(self)\n        name = self.getName()\n        self.lonelyDoneEvent = self.taskName(name + '-lonely-done')\n        self.lonely = CharStateDatasAI.CharLonelyStateAI(self.lonelyDoneEvent, self)\n        self.chattyDoneEvent = self.taskName(name + '-chatty-done')\n        self.chatty = CharStateDatasAI.CharChattyStateAI(self.chattyDoneEvent, self)\n        self.walkDoneEvent = self.taskName(name + '-walk-done')\n        if self.diffPath == None:\n            self.walk = CharStateDatasAI.CharWalkStateAI(self.walkDoneEvent, self)\n        else:\n            self.walk = CharStateDatasAI.CharWalkStateAI(self.walkDoneEvent, self, self.diffPath)\n        return", "documentation": "generate the AI states", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_2": {"code": "def start(self):\n        self.fsm.request('Lonely')", "documentation": "Starts the FSM by sending a request to the FSM to start the streaming .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_3": {"code": "def enterOff(self):\n        pass", "documentation": "return", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_4": {"code": "def enterLonely(self):\n        self.lonely.enter()\n        self.acceptOnce(self.lonelyDoneEvent, self.__decideNextState)", "documentation": "Called when the actor enters the Lately state . Starts the actor in the background and", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_5": {"code": "def __goForAWalk(self, task):\n        self.notify.debug('going for a walk')\n        self.fsm.request('Walk')\n        return Task.done", "documentation": "Called when a walk is requested", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_6": {"code": "def exitChatty(self):\n        self.ignore(self.chattyDoneEvent)\n        self.chatty.exit()", "documentation": "Exit chatty process", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_7": {"code": "def exitWalk(self):\n        self.ignore(self.walkDoneEvent)\n        self.walk.exit()", "documentation": "Stops the current walk and ignores the walkDoneEvent", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_8": {"code": "def avatarExitNextState(self):\n        if len(self.nearbyAvatars) == 0:\n            if self.fsm.getCurrentState().getName() != 'Walk':\n                self.fsm.request('Lonely')", "documentation": "Exits the next state of the avatar if no nearby avatars are available", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "ksmit799/Toontown-Source_toontown/classicchars/DistributedGoofySpeedwayAI.py_9": {"code": "def getCCLocation(self):\n        if self.diffPath == None:\n            return 1\n        else:\n            return 0\n        return", "documentation": "Returns the CC location of the diff", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "isaac-philip/loolu_common/django/contrib/admin/views/decorators.py_0": {"code": "def _display_login_form(request, error_message=''):\n    request.session.set_test_cookie()\n    return render_to_response('admin/login.html', {\n        'title': _('Log in'),\n        'app_path': request.get_full_path(),\n        'error_message': error_message\n    }, context_instance=template.RequestContext(request))", "documentation": "Display the login form and set the cookie .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "isaac-philip/loolu_common/django/contrib/admin/views/decorators.py_1": {"code": "def _checklogin(request, *args, **kwargs):\n        if request.user.is_authenticated() and request.user.is_staff:\n            # The user is valid. Continue to the admin page.\n            return view_func(request, *args, **kwargs)\n\n        assert hasattr(request, 'session'), \"The Django admin requires session middleware to be installed. Edit your MIDDLEWARE_CLASSES setting to insert 'django.contrib.sessions.middleware.SessionMiddleware'.\"\n\n        # If this isn't already the login page, display it.\n        if LOGIN_FORM_KEY not in request.POST:\n            if request.POST:\n                message = _(\"Please log in again, because your session has expired.\")\n            else:\n                message = \"\"\n            return _display_login_form(request, message)\n\n        # Check that the user accepts cookies.\n        if not request.session.test_cookie_worked():\n            message = _(\"Looks like your browser isn't configured to accept cookies. Please enable cookies, reload this page, and try again.\")\n            return _display_login_form(request, message)\n        else:\n            request.session.delete_test_cookie()\n\n        # Check the password.\n        username = request.POST.get('username', None)\n        password = request.POST.get('password', None)\n        user = authenticate(username=username, password=password)\n        if user is None:\n            message = ERROR_MESSAGE\n            if '@' in username:\n                # Mistakenly entered e-mail address instead of username? Look it up.\n                users = list(User.all().filter('email =', username))\n                if len(users) == 1 and users[0].check_password(password):\n                    message = _(\"Your e-mail address is not your username. Try '%s' instead.\") % users[0].username\n                else:\n                    # Either we cannot find the user, or if more than 1\n                    # we cannot guess which user is the correct one.\n                    message = _(\"Usernames cannot contain the '@' character.\")\n            return _display_login_form(request, message)\n\n        # The user data is correct; log in the user in and continue.\n        else:\n            if user.is_active and user.is_staff:\n                login(request, user)\n                return http.HttpResponseRedirect(request.get_full_path())\n            else:\n                return _display_login_form(request, ERROR_MESSAGE)", "documentation": "Check if the user is logged in and if is staff log them in .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "coders-creed/botathon_src/info/fetch_info.py_0": {"code": "def send_info_handler(bot, update, args):\n\targs = list(parse_args(args))\n\tif len(args) == 0 or \"portfolio\" in [arg.lower() for arg in args] :\n\t\tsend_portfolio_info(bot, update)\n\telse:\n\t\tinfo_companies = get_companies(args)\n\t\tsend_companies_info(bot, update, info_companies)", "documentation": "Send info messages to the update", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "coders-creed/botathon_src/info/fetch_info.py_1": {"code": "def send_portfolio_info(bot, update):\n\tprint \"Userid: %d requested portfolio information\" %(update.message.chat_id)\n\tcontext = {\n\t'positions': Portfolio.instance.positions,\n    'wallet_value': Portfolio.instance.wallet_value,\n\t}\n\thtml_str = engine.render('portfolio_info.pyhtml', context)\n\tbot.sendMessage(parse_mode=\"HTML\", chat_id=update.message.chat_id, text=html_str)", "documentation": "Sends the portfolio info to the bot .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "coders-creed/botathon_src/info/fetch_info.py_2": {"code": "def send_companies_info(bot, update, companies):\n\tprint \"Userid: requested information for following companies %s\" %','.join([c.name for c in companies])\n\n\tfor company in companies:\n\t\tcontext = {\n\t\t'company': company,\n\t\t'current_price': get_current_price(company),\n\t\t'description': wikipedia.summary(company.name.split()[0], sentences=2)\n\t\t}\n\n\t\twiki_page = wikipedia.page(company.name.split()[0])\n\t\thtml_page = urllib2.urlopen(wiki_page.url)\n\t\tsoup = bs.BeautifulSoup(html_page)\n\t\timg_url = 'http:' + soup.find('td', { \"class\" : \"logo\" }).find('img')['src']\n\t\tbot.sendPhoto(chat_id=update.message.chat_id, photo=img_url)\n\n\t\thtml_str = engine.render('company_template.pyhtml', context)\n\t\tbot.sendMessage(parse_mode=\"HTML\", chat_id=update.message.chat_id, text=html_str)\n\n\tsymbols = [c.symbol for c in companies]\n\tif len(symbols) >= 2:\n\t\tsymbol_string = \", \".join(symbols[:-1]) + \" and \" + symbols[-1]\n\telse:\n\t\tsymbol_string = symbols[0]\n\n\tlast_n_days = 10\n\n\tif len(companies) < 4:\n\t\tcreate_graph(companies, last_n_days)\n\t\thistory_text = '''\n\t\t\tHere's the price history for {} for the last {} days\n\t\t'''.format(symbol_string, last_n_days)\n\n\t\tbot.sendMessage(chat_id=update.message.chat_id, text=history_text)\n\t\tbot.sendPhoto(chat_id=update.message.chat_id, photo=open(\"plots/temp.png\",'rb'))", "documentation": "Send company info to the bot .", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "pisskidney/leetcode_medium/16.py_0": {"code": "def bsearch(nums, left, right, res, i, j, target):\n    while left <= right:\n        middle = (left + right) // 2\n        candidate = nums[i] + nums[j] + nums[middle]\n        if res is None or abs(candidate - target) < abs(res - target):\n            res = candidate\n        if candidate == target:\n            return res\n        elif candidate > target:\n            right = middle - 1\n        else:\n            left = middle + 1\n    return res", "documentation": "Binary search for a target number", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}, "pisskidney/leetcode_medium/16.py_1": {"code": "def threeSumClosest(self, nums: List[int], target: int) -> Optional[int]:\n        res = None\n        nums = sorted(nums)\n\n        for i in range(len(nums)):\n            for j in range(i + 1, len(nums)):\n                res = bsearch(nums, j + 1, len(nums) - 1, res, i, j, target)\n        return res", "documentation": "Find the sum of the closest number in a list of numbers", "reputation": {"num_stars": 0, "num_forks": 0, "num_watchers": 0, "num_open_issues": 0, "created_at": 1677570694.0}}}}